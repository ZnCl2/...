{
	"title": "\nTransmitter",
	"description": "\nTransmitter",
	"links": "- [Source code](https://github.com/HelloZeroNet)",
	"next_post_id": 12,
	"demo": false,
	"modified": 1463326296,
	"post": [
		{
			"post_id": 8,
			"title": "HTML，CSS，font-family：中文字体的英文名称 （宋体 微软雅黑）",
			"date_published": 1463245067.751,
			"body": "<font color=Black size=3 face=\"Microsoft YaHei\">\n宋体\tSimSun\n黑体\tSimHei\n微软雅黑\tMicrosoft YaHei\n微软正黑体\tMicrosoft JhengHei\n新宋体\tNSimSun\n新细明体\tPMingLiU\n细明体\tMingLiU\n标楷体\tDFKai-SB\n仿宋\tFangSong\n楷体\tKaiTi\n仿宋_GB2312\tFangSong_GB2312\n楷体_GB2312\tKaiTi_GB2312\n\n宋体：SimSuncss中中文字体（font-family）的英文名称\nMac OS的一些：\n华文细黑：STHeiti Light [STXihei]\n华文黑体：STHeiti\n华文楷体：STKaiti\n华文宋体：STSong\n华文仿宋：STFangsong\n儷黑 Pro：LiHei Pro Medium\n儷宋 Pro：LiSong Pro Light\n標楷體：BiauKai\n蘋果儷中黑：Apple LiGothic Medium\n蘋果儷細宋：Apple LiSung Light\nWindows的一些：\n新細明體：PMingLiU\n細明體：MingLiU\n標楷體：DFKai-SB\n黑体：SimHei\n新宋体：NSimSun\n仿宋：FangSong\n楷体：KaiTi\n仿宋_GB2312：FangSong_GB2312\n楷体_GB2312：KaiTi_GB2312\n微軟正黑體：Microsoft JhengHei\n微软雅黑体：Microsoft YaHei\n装Office会生出来的一些：\n隶书：LiSu\n幼圆：YouYuan\n华文细黑：STXihei\n华文楷体：STKaiti\n华文宋体：STSong\n华文中宋：STZhongsong\n华文仿宋：STFangsong\n方正舒体：FZShuTi\n方正姚体：FZYaoti\n华文彩云：STCaiyun\n华文琥珀：STHupo\n华文隶书：STLiti\n华文行楷：STXingkai\n华文新魏：STXinwei\nWindows 中的中文字体。\n在默认情况下，也就是未自行安装新字体或者 Office 等文字处理软件的情况下，Windows 默认提供下列字体：\nWindows 95/98/98SE 宋体、黑体、楷体_GB2312、仿宋_GB2312\nWindows XP/2000/2003/ME/NT 宋体/新宋体、黑体、楷体_GB2312、仿宋_GB2312 (Windows XP SP3 宋体-PUA)\nWindows Vista/7/2008 宋体/新宋体、黑体、楷体、仿宋、微软雅黑、SimSun-ExtB\n那么每种字体能显示那些汉字呢？\nVista 之前的 Windows 中宋体/新宋体、黑体支持 GBK 1.0 字符集，\n楷体_GB2312、仿宋_GB2312 支持 GB2312-80 字符集。\n（注：Windows 3.X 只能支持 GB2312-80 字符集）\nVista 及之后的 Windows 中宋体/新宋体、黑体、楷体、仿宋、微软雅黑支持 GB18030-2000 字符集，\nSimSun-ExtB 只支持 GB18030-2005 字符集扩展 B 部分。\n下面对字符集进行简单的介绍：\nGB2312-80 < GBK 1.0 < GB18030-2000 < GB18030-2005\nGB2312-80 中的字符数量最少，GB18030-2005 字符数量最多。\nGB2312-80 是最早的版本，字符数比较少；\nGBK 1.0 中的汉字大致与 Unicode 1.1 中的汉字数量相同；\nGB18030-2000 中的汉字大致与 Unicode 3.0 中的汉字数量相同，主要增加了扩展 A 部分；\nGB18030-2005 中的汉字大致与 Unicode 4.1 中的汉字数量相同，主要增加了扩展 B 部分；\n由于 Unicode 5.2 的发布，估计 GB18030 会在近期发布新版本，增加扩展 C 部分。\n需要说明的是在 GB18030 中扩展 B 部分并不是强制标准。\n如果想查看 GB18030 的标准文本，请访问 http://www.gb168.cn 中的强标阅读。\n如果想了解 Unicode 的内容，请访问 http://www.unicode.org。\n现在纠正网上普遍的一个错误：\nGB18030-2000 和 GB18030-2005 都不支持单字节的欧元符号\n与简体中文有关的代吗页如下：\n936 gb2312 简体中文(GB2312)————其实是GBK\n10008 x-mac-chinesesimp 简体中文(Mac)\n20936 x-cp20936 简体中文(GB2312-80)\n50227 x-cp50227 简体中文(ISO-2022)\n51936 EUC-CN 简体中文(EUC)\n52936 hz-gb-2312 简体中文(HZ)\n54936 GB18030 简体中文(GB18030)\n补充：\n使用楷体_GB2312、仿宋_GB2312后，在 Windows 7/Vista/2008 中可能不再显示为对应的字体。\n这是因为 Windows 7/Vista/2008 中有楷体、仿宋，默认情况下没有楷体_GB2312、仿宋_GB2312，字体名称相差“_GB2312”。\n</font>",
			"tag": "ZeroNet"
		},
		{
			"post_id": 7,
			"title": "你所谓的辛苦，在别人眼中不过是安逸的代名词",
			"date_published": 1463241693.168,
			"body": "![image alt](img/XFW3S.jpg)\n<font color=Black size=3 face=\"Microsoft YaHei\">昨天我写了篇文说我已经连上了13天班，然后手贱发到我朋友圈，结果……\n \n别提了，已经不想活了。我没想到一条链接竟激出了朋友圈那么多隐藏得很深的励志狗，这些家伙在我的“连上13天班”这个事实面前，无一不表示了不屑或鄙夷。\n \n先是我的发小A说，13天就让你骄傲了，你到底有没有好好工作过？她说她老公最长时间300天没休息，每天九点前到公司，九点后才回家。\n \nA的老公是程序猿，单位的技术大拿，A说他从来都是主动加班，不，更确切地说是没有休假和工作日之分，除非有特别重要的事情，否则他就去公司，不但没抱怨过辛苦，反倒经常听他说，剩余的工作三年都做不完。因为每次都是主动加班，所以自然也没有加班费。A起初也不太理解老公，问他累不累？结果他老公很淡然地反问：“做自己喜欢的事，怎么会累呢？”\n \n那种平静淡然，是我等庸碌无为一说加班就呼天喊地的人无数想象的，A说，她和老公已经十年没去电影院了，唯一能把老公从工作状态中抽离出来的办法，就是订三张机票，全家人一起去旅游。\n \n是的，这就是工作狂的境界，最让你郁闷的是，人家不觉得辛苦啊，人家觉得工作很快乐很嗨啊。你看，你腆着脸说自己好累好累快撑不下去的时候，人家从来不觉得自己在撑，而是最平凡，最普通的日常。\n \n姐听得毛骨悚然，姐再也不羡慕他们的房子车子一年好几次的旅游了，唉，让臣妾这么努力付出，臣妾做不到啊。\n \n我另一个同学B自己创业，在老家开了家医药超市，她说原本以为给自己打工会轻松些，结果一年至少340天在店里，虽然看店不累，但会觉得很烦，有时特想出去转转看看不同的风景，可是她一出去，就意味要关门或者找人来代看。她基本八点前都会准时开门，晚上最早也要8点回家，忙起来，十点十一点关门的时候都有，有时半夜还会有人敲门买药，她即使睡着了也会第一时间爬起来，不是为了挣那几毛钱，而是大半夜买药，肯定是急症，不能让顾客着急，她问我，你有大冬天夜里爬起来两三次的经历吗？\n \n至此，我已经彻底哑口无言，害臊得憋不出一句话来了。\n \n你看，这世上有多少人付出的比我们想象中要多得多，可怕的不是人家比我们优秀，而是人家明明比我们优秀，还要比我们努力一百倍，认清了这点儿，有时觉得人生简直TM是无望啊。比如A的老公，早就是公司合伙人了，可他从来没有一天懈怠过，爱好就是工作，那简直是传说中的永动机啊。\n \n我找工作时，有个必要的前提，那就是双休，否则钱再多对我都没诱惑力，起点已经决定了基调，找工作时就奔着安逸去的，所以连上13天班，才会成为我的最高记录。我这些朋友，有的给自己打工，有的给别人打工，他们从来没受过“双休”，在他们看来，我经常去看电影、带孩子去公园玩、每天有时间写字读书，生活非常安逸舒适，一点儿也不“屌丝”。\n \n我还有个朋友是这么回复的：我也是才十年没休息而已。\n \n十年而已，而已……\n \n原来，我所谓的特别拼，特别努力，在人家眼中，只不过是在换一种方式炫耀我的舒适与懒惰，只是安逸的代名词。\n \n我记得上次见宋律师时，他提到了他司法考试前那段日子，他说那是他平生最努力最拼的一段日子，心无旁骛，每天醒来第一件事想的就是，要把哪些内容吃透，哪些知识点了解于心，看书看得累了，就去楼下小花园散散步，一边散步，还在一边默默地梳理知识点，晚上睡五六个小时是常有的事。但那时他从来不觉得自己有多么努力多么辛苦，相反只有一个念头，我一定要通过司法考试，于是每天朝这个目标努力，每天过得特充实特兴奋，他说这件事情已经过去十多年了，每每回忆起来，都觉得那段日子像是开了挂一般，每天除了学学学，压根没其他乱七八糟的心思，所以他以高分通过司法考试时，自己并不觉得意外，因为，“那是我应得的”。\n \n这样的日子，其实你也不陌生吧，高考前，我们不都是这样过来的吗？白天就是做题背书，晚上寝室熄灯后，还要打着手电筒在被窝里学习一会儿，每个同学都在拼命读书，你追我赶，你有觉得自己苦自己累吗？至少我当时是没有。\n \n是的，当你全身心地为了一个目标努力时，你就会努力往前跑，一直跑一直跑，直到够到目标为止。非常不幸，我们中的大多数，其实连个目标也没有。上班于我们而言，只是因为这是能养家糊口的最轻松的方式，至于额外付出，对不起，我想在家舒服地刷刷朋友圈或者看看电视。\n \n别再抱怨自己多么努力多么辛苦，真正在努力付出的，才不会觉得自己辛苦，更不会抱怨，相反他会为了离目标更进一点喜不自胜，那种类似孩童的快乐，我们都曾体会过，可惜，我们早把那样的自己抛之身后了。\n \n已经羞得不好意思再屁叨叨下去了。\n \n听完这些励志狗的倾诉，原本打算晚上看电视消遣的我，果断坐到电脑前，默默码下以上文字。\n \n当然啦，人各有志，这也不必强求。他喜欢工作，你喜欢舒服，只要自己满意，都挺好哒。\n</font>"
		},
		{
			"post_id": 6,
			"title": "shell中的 find xargs exec 命令",
			"date_published": 1463211817.821,
			"body": "**1. find**\n`find pathname -options [-print -exec -ok]`\n\n参数\n  pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。\n  -print：  find命令将匹配的文件输出到标准输出。\n  -exec：   find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为'command' {} /; ，注意{}和/；之间的空格。\n  例子：\n      `find . -type f \\( -name '*.[ch]' -o -name '*.cpp' -o -name 'CodeGen.*' \\) -exec ./se-replace.py {} \\+`\n这个例子找出当前目录及其子目录下的文件，从中搜索 .h 或 .c 或 .cpp 或 CodeGen.* 的文件，然后把这些文件传给 se-replae.py 执行。\n  -ok：     和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。\n  find命令选项\n\n  -name：按照文件名查找文件。\n  -perm：按照文件权限来查找文件。\n  -prune：使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。\n  -user： 按照文件属主来查找文件。\n  -group：按照文件所属的组来查找文件。\n  -mtime -n +n：按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+n表示文件更改时间距现在n天以前。Find命令还有-atime和-ctime选项，但它们都和-mtime选项。\n  -nogroup：查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。\n  -nouser：查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。\n  -newer file1 ! file2：查找更改时间比文件file1新但比文件file2旧的文件\n  -type 查找某一类型的文件\n        b - 块设备文件。\n        d - 目录。\n        c - 字符设备文件。\n        p - 管道文件。\n        l - 符号链接文件。\n        f - 普通文件\n        -size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。\n        -depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。\n        -fstype：查找位于某一类型文件系统中的文件，这些文件系统类型通常可以在配置文件/etc/fstab找到，该配置文件中包含了本系统中有关文件系统的信息\n        -mount：在查找文件时不跨越文件系统mount点。\n        -follow：如果find命令遇到符号链接文件，就跟踪至链接所指向的文件。\n        -cpio：对匹配的文件使用cpio命令，将这些文件备份到磁带设备中\n\n例子：\n    当前目录及子目录中查找文件名以一个大写字母开头的文件并输出到标准输出：\n            find . -name \"[A-Z]*\" -print\n\n    在/etc目录中查找文件名以host开头的文件并输出到标准输出：\n            find /etc -name \"host*\" -print\n2. exec\n find命令将所有匹配到的文件一起传递给exec执行。例子如下：\n \n     匹配当前目录下的所有普通文件并ls列出来\n        `find . -type f -exec ls -l {} /;`\n\n     在/logs目录中查找更改时间在5日以前的文件并删除它们\n        `find /logs -type f -mtime +5 -exec rm {} /;`\n    在当前目录中查找所有文件名以.log结尾、更改时间在5日以上的文件，并删除它们，在删除之前给出提示:\n            find . -name \"*.log\" -mtime +5 -ok rm {} /;` \n     (-ok 与 -exec 作用一样，但是以一种更安全的模式执行)\n\n    首先匹配所有文件名为“ passwd*”的文件，然后执行grep命令查看这些文件中是否存在一个sam用户\n            `find /etc -name \"passwd*\" -exec grep \"sam\" {} /;`\n\n    查找当前用户主目录下的所有文件\n            find $HOME -print\n            find ~ -print\n\n    在当前目录中文件属主具有读、写权限，并且文件所属组的用户和其他用户具有读权限的文件\n            find . -type f -perm 644 -exec ls -l {} /;\n\n    查找系统中所有文件长度为0的普通文件，并列出它们的完整路径\n            find / -type f -size 0 -exec ls -l {} /;\n\n    查找系统中所有属于root组的文件\n            find . -group root -exec ls -l {} /;\n\n    查找当前目录中的所有目录并排序\n            find . -type d |sort\n\n3. xargs\n    xargs 与 exec 的作用类似，但是xargs与find 一起使用时，一般配合管道一起使用\n    前面的输出转换为后方指令的参数输入\n    使用exec和xargs可以使用户对所匹配到的文件执行几乎所有的命令。\n\n    查找系统中的每一个普通文件，然后使用xargs命令测试它们分别属于哪类文件\n        find . -type f -print | xargs file\n\n    在整个系统中查找内存信息转储文件(core dump) ，然后把结果保存到/tmp/core.log 文件中\n        find / -name \"core\" -print | xargs echo \"\" >/tmp/core.log\n\n    在当前目录下查找所有用户具有读、写和执行权限的文件，并收回相应的写权限\n        find . -perm -7 -print | xargs chmod o-w\n\n    用grep命令在所有的普通文件中搜索hostname这个词\n        find . -type f -print | xargs grep \"hostname\"\n\n    统计当前目录下所有文件的大小,含子目录,精确到字节\n        find ./ -type f | xargs ls -l | awk 'BEGIN{size=0}{size+=$5};END{print size}'"
		},
		{
			"post_id": 5,
			"title": "ls 列出文件目录（可以含子目录）及文件的完整路径",
			"date_published": 1463211332.849,
			"body": "\n1、列出当前目录的文件、文件夹完整路径\n  ```ls -1 |awk '{print i$0}' i=`pwd`'/'```\n\n2、列出当前目录及子目录的文件、文件夹完整路径\n  ```ls -R |awk '{print i$0}' i=`pwd`'/'```\n\n2b） 列出当前目录及子目录下的文件夹完整路径\n    ```ls -FR | grep /$ | sed \"s:^:`pwd`/:\"``` \n\n3、用find实现，好像运行要慢些\n   ```find / -name \"*.*\" -exec ls {} \\;```\n\n4、递归列出当前目录及子目录名称\n    `ls -FR | grep /$`\n\n5、递归列出当前目录及子目录名称，包括相关属性\n    `ls -lR | grep \"^d\"`\n\n6、只列出当前目录下的子目录\n    用ls只列出子目录\n    `ls -d */`"
		},
		{
			"post_id": 4,
			"title": "ZeroNet on  Raspberry Pi",
			"date_published": 1463156626.93,
			"body": "```sh\nsudo apt-get update\nsudo apt-get install python-pip\nsudo pip install gevent --upgrade\nsudo pip install msgpack-python --upgrade\n\nwget https://github.com/HelloZeroNet/ZeroNet/archive/master.tar.gz\ntar xvpfz master.tar.gz\ncd ZeroNet-master\nStart with python zeronet.py\nOpen http://127.0.0.1:43110/ in your browser\n```"
		},
		{
			"post_id": 3,
			"title": "拼音搜索",
			"date_published": 1463155665,
			"body": "search.py\n```python\n#!/opt/python3/bin/python3\n\n#-------------------------------------------------------------------------------\n# Name:        search phone num\n# Purpose:     search phone num by pinyin\n# Created:     19/01/2014\n# Copyright:   (c) john 2014\n# Licence:     <GPL3>\n#-------------------------------------------------------------------------------\n\nimport read_write as rw\nimport os\nimport sys\n\npinyin_db=r'convert-utf-8.txt'\nphone_db=r'PHONE.txt'\n\ndef is_chinese(uchar):\n    if uchar >= u'\\u4e00' and uchar<=u'\\u9fa5':\n        return True\n    else:\n        return False\ndef has_chinese(ustring):\n    for c in ustring:\n        if(is_chinese(c)):\n            return True\n        else:\n            return False\n        \ndef load_db():\n    pinyin_list=rw.read2memory(pinyin_db).split('\\n')\n    pinyin_dic={}\n    for pinyin in pinyin_list:\n        pinyin_dic[pinyin[0]]=pinyin[1:-1]\n    phone_list=rw.read2memory(phone_db).split('\\n')\n    p2=list()\n    index=0\n    for p in phone_list:\n        for ps in p.split('\\t'):\n            for s in ps:\n                if(is_chinese(s)):\n                    try:\n                        p2[index] += pinyin_dic[s]\n                    except:\n                        p2.append(pinyin_dic[s])\n            try:\n                p2[index] += \" \"\n            except:\n                pass\n\n        index += 1\n    #print(len(p2),len(phone_list))\n    return phone_list,p2\n\ndef search(phone_list,pinyin_list,key):\n    index=0\n    if not has_chinese(key):\n        for pinyin in pinyin_list:\n            if (pinyin.__contains__(key)):\n                print(phone_list[index])\n            index += 1\n    else:\n        for phone in phone_list:\n            if (phone.__contains__(key)):\n                print(phone)\n    pass\n\ndef get_key():\n    if len(sys.argv)>1:\n        try:\n            key_word=sys.argv[1]+\" \"+sys.argv[2]\n        except:\n            key_word=sys.argv[1]\n        sys.argv=sys.argv[0]\n    else:\n        key_word=input(\">> \")\n        tmp=os.system('clear')\n    if (key_word=='exit'):\n        os._exit(0)\n    return key_word\n\ndef main():\n    db=load_db()\n    while(1>0):\n        key=get_key()\n        search(db[0],db[1],key)\n    pass\n\nif __name__ == '__main__':\n    main()\n\n```\n"
		},
		{
			"post_id": 2,
			"title": "QQ Image",
			"date_published": 1463154876.6,
			"body": "```python\nimport os\nimport io\nimport re\nimport gc\nimport hashlib\nimport shutil\nimport time\n\n\ndef get_file_list(root):\n    file_list = [];\n    for root, dirs, files in os.walk(root):\n        for f in files:\n            path=root+os.path.sep+f\n            file_list.append(path)\n    return file_list\n\ndef show_list(a_list):\n    for i in a_list:\n        print(i)\n\ndef write2file(a_list,file_name):\n    f=open(file_name,'w',encoding='utf-8')\n    for line in a_list:\n        f.write(line+'\\n')\n    f.close()\n\ndef write_part2file(file_name,a_list,index_list):\n    f=open(file_name,'w',encoding='utf8')\n    for i in index_list:\n        f.write(a_list[i]+'\\n')\n    f.close()\n\ndef append2file(file_path,a_list):\n    f=open(file_path,'a',encoding='utf8')\n    for i in a_list:\n        f.write(i+'\\n')\n    f.close()\n    print(\"Append %d md5sum to database\"%(len(a_list)))\n\ndef read2memory(file_path):\n    with open(file_path,'r',encoding='utf8') as f:\n        return f.read()\n\ndef read2memory2(file_path):\n    return open(file_path,'r',encoding='utf8').read()\n\ndef get_file_info(file_path):\n    size=os.path.getsize(file_path)\n    modify_time=os.path.getmtime(file_path)\n    last_access_time=os.path.getctime(file_path)\n    creat_time=os.path.getctime(file_path)\n    return [file_path,size,creat_time,modify_time,last_access_time]\n\ndef get_md5sum(file_path):\n    size=os.path.getsize(file_path)\n    if(size<30*1024*1024):\n        with open(file_path.strip(),'rb') as f:\n            data=f.read()\n            md5sum=hashlib.md5(data).hexdigest()\n    else:\n        m = hashlib.md5()\n        file = io.FileIO(file_path,'r')\n        bytes = file.read(8192)\n        while(bytes != b''):\n            m.update(bytes)\n            bytes = file.read(8192)\n            file.close()\n        md5sum=m.hexdigest()\n    return md5sum\n\ndef get_md5sum_list(file_list):\n    md5list=list()\n    len_file_list=len(file_list)\n    index=0\n    for i in file_list:\n        md5sum=get_md5sum(i)\n        md5list.append( md5sum )\n        #print((\"%d\\t%d\\t%.3f/100\")%(len_file_list,index+1,((index+1)*100)/len_file_list))\n        index+=1\n    return md5list\n\ndef get_size_list(file_list):\n    file_size_list=list()\n    for i in file_list:\n        file_size=os.path.getsize(i)\n        file_size_list.append(file_size)\n    return file_size_list\n\ndef get_same_md5_index_from_self(md5list):\n    same_md5_index=list()\n    temp=list()\n    index=0\n    for i in md5list:\n        if temp.__contains__(i):\n            same_md5_index.append(index)\n        else:\n            temp.append(i)\n        index+=1\n    return same_md5_index\n\ndef get_same_md5_index_from_md5database(md5list,database_list):\n    database_set=set(database_list)\n    index=0\n    same_md5_index=list()\n\n    for i in md5list:\n        if database_set.__contains__(i):\n            same_md5_index.append(index)\n        index+=1\n    #database_set.\n\n    #print(len(database_set))\n    print(\"Find %d same files compared with database\"%(len(same_md5_index)))\n    return same_md5_index\n\ndef get_small_file_index_list(size_list):\n    small_size_index=list()\n    index=0\n    for i in size_list:\n        if (i<1024*200):\n            small_size_index.append(index)\n        index+=1\n    return small_size_index\n\ndef del_file(file_list,index):\n    for i in index:\n        try:\n            os.remove(file_list[i])\n        except:\n            print(\"Can't delete %s\"%(i))\n            pass\n        #print(\"del %s,%d\"%(file_list[i],len(file_list)))\n\ndef move_file(source,dest):\n    try:\n        shutil.move(source,dest)\n        #print(source,dest)\n    except:\n        pass\n\ndef move_filelist(a_list,dest_dir):\n    for i in a_list:\n        move_file(i, os.path.join(dest_dir,i.split('\\\\')[-1]))\n\ndef get_filelist_by_size(file_list):\n    list_1024k=list()\n    list_900k=list()\n    list_800k=list()\n    list_700k=list()\n    list_600k=list()\n    list_500k=list()\n    list_400k=list()\n    list_300k=list()\n    list_200k=list()\n    list_100k=list()\n    for i in file_list:\n        size=os.path.getsize(i)\n        if size >1024*1024:\n            list_1024k.append(i)\n        elif size > 900 * 1024:\n            list_900k.append(i)\n        elif size > 800 * 1024:\n            list_800k.append(i)\n        elif size > 700 * 1024:\n            list_700k.append(i)\n        elif size > 600 * 1024:\n            list_600k.append(i)\n        elif size > 500 * 1024:\n            list_500k.append(i)\n        elif size > 400 * 1024:\n            list_400k.append(i)\n        elif size > 300 * 1024:\n            list_300k.append(i)\n        elif size > 200 * 1024:\n            list_200k.append(i)\n        elif size > 100 * 1024:\n            list_100k.append(i)\n    return [list_100k,list_200k,list_300k,list_400k,list_500k,list_600k,list_700k,list_800k,list_900k,list_1024k]\n\ndef get_filelist_by_type(file_list):\n    jpg=r'jpg'\n    gif=r'gif'\n    jpg_list=list()\n    gif_list=list()\n    other_list=list()\n    for i in file_list:\n        if re.search(jpg,i,re.I):\n            jpg_list.append(i)\n        elif re.search(gif,i,re.I):\n            gif_list.append(i)\n        else:\n            other_list.append(i)\n    return [jpg_list,gif_list,other_list]\n\ndef move_by_size(file_list,file_type,root):\n    file_type_root=os.path.join(root,file_type)\n    d_100k=os.path.join(file_type_root,'100k')\n    d_200k=os.path.join(file_type_root,'200k')\n    d_300k=os.path.join(file_type_root,'300k')\n    d_400k=os.path.join(file_type_root,'400k')\n    d_500k=os.path.join(file_type_root,'500k')\n    d_600k=os.path.join(file_type_root,'600k')\n    d_700k=os.path.join(file_type_root,'700k')\n    d_800k=os.path.join(file_type_root,'800k')\n    d_900k=os.path.join(file_type_root,'900k')\n    d_1024k=os.path.join(file_type_root,'1024k')\n    if not os.path.exists(file_type_root):\n        os.mkdir(file_type_root)\n        os.mkdir(d_100k)\n        os.mkdir(d_200k)\n        os.mkdir(d_300k)\n        os.mkdir(d_400k)\n        os.mkdir(d_500k)\n        os.mkdir(d_600k)\n        os.mkdir(d_700k)\n        os.mkdir(d_800k)\n        os.mkdir(d_900k)\n        os.mkdir(d_1024k)\n    #show_list(file_list[0])\n    move_filelist(file_list[0],d_100k)\n    move_filelist(file_list[1],d_200k)\n    move_filelist(file_list[2],d_300k)\n    move_filelist(file_list[3],d_400k)\n    move_filelist(file_list[4],d_500k)\n    move_filelist(file_list[5],d_600k)\n    move_filelist(file_list[6],d_700k)\n    move_filelist(file_list[7],d_800k)\n    move_filelist(file_list[8],d_900k)\n    move_filelist(file_list[9],d_1024k)\n\ndef move(file_list,dest_root):\n    list_by_type=get_filelist_by_type(file_list)\n\n    move_by_size( get_filelist_by_size ( list_by_type[0]),'jpg',dest_root)\n    move_by_size( get_filelist_by_size(list_by_type[1]),'gif',dest_root)\n    move_by_size( get_filelist_by_size(list_by_type[2]),'other',dest_root)\n    print((\"Move %d files to %s\")%(len(file_list),dest_root))\n\ndef remove_same_file_from_source_dir(dir):\n    file_list= get_file_list(dir)\n    md5list=get_md5sum_list(file_list)\n    write2file(md5list,'removed_same_md5.txt')\n    same_md5_index_list=get_same_md5_index_from_self(md5list)\n    del_file(file_list,same_md5_index_list)\n    print(\"Del %d same files from source dir\"%(len(same_md5_index_list)))\n\n    cont=0\n    for i in same_md5_index_list:\n        md5list.pop(i-cont)\n        cont+=1\n    return md5list\n\n\ndef log_same_file(dir,log_path):\n    file_list=get_file_list(dir)\n    md5list=get_md5sum_list(file_list)\n    same_md5_index_list=get_same_md5_index_from_self(md5list)\n    write_part2file(log_path,file_list,same_md5_index_list)\n\n\ndef remove_small_file(dir):\n    file_list= get_file_list(dir)\n    file_size_list=get_size_list(file_list)\n    small_size_index_list=get_small_file_index_list(file_size_list)\n    del_file (file_list,small_size_index_list)\n    print(\"Del %d small files\"%(len(small_size_index_list)))\n\ndef remove_same_file_from_md5database(dir,source_md5sum_list,md5database):\n    same_md5_list=list()\n    oldmd5list=read2memory(md5database).split('\\n')\n    print(\"Read %d Lines from md5database\"%(len(oldmd5list)))\n    file_list = get_file_list(dir)\n    print(\"The source dir have %d files\"%(len(file_list)))\n    same_md5_index_from_database=get_same_md5_index_from_md5database(source_md5sum_list,oldmd5list)\n    print(\"Del %d same files from DIR\"%(len(same_md5_index_from_database)))\n    del_file(file_list,same_md5_index_from_database)\n    for i in same_md5_index_from_database:\n        #print (i)\n        try:\n            same_md5_list.append(source_md5sum_list[i])\n\n        except:\n            #print(i,len(source_md5sum_list))\n            pass\n    difference_list=list(set(source_md5sum_list).difference(set(same_md5_list)))\n    #print(len(source_md5sum_list),len(same_md5_index_from_database),len(same_md5_list),len(difference_list))\n    return difference_list\n\ndef restor_qq_image():\n    dir=r\"C:\\Users\\acc\\Documents\\Tencent Files\\qq_acc\\Image\"\n    #dir=r\"C:\\Users\\acc\\Documents\\Tencent Files\\qq_acc\\Image\\Group\"\n    dest=r\"E:\\QQ_Image\"\n    md5database=r\"E:\\qq_image\\qq_image_md5.txt\"\n\n    remove_small_file(dir)\n    source_md5sum_list=remove_same_file_from_source_dir(dir)\n    dirrerence_source_md5sum_list=remove_same_file_from_md5database(dir,source_md5sum_list,md5database)\n    append2file(md5database,dirrerence_source_md5sum_list)\n    move(get_file_list(dir),dest)\n\ndef main():\n    start_time=time.clock()\n\n    restor_qq_image()\n    end_time=time.clock()\n    print((\"ALL time:%.5f(S)\")%(end_time-start_time))\n    #time.sleep(20)\n    input(\"Press any key to continu ......\")\n    pass\n\nif __name__ == '__main__':\n    main()\n\n```"
		}
	],
	"tag": [
		{
			"value": "Shell",
			"post_id": 6
		},
		{
			"value": "Shell",
			"post_id": 5
		},
		{
			"value": "ZeroNet",
			"post_id": 4
		},
		{
			"value": "Python",
			"post_id": 3
		},
		{
			"value": "Python",
			"post_id": 2
		},
		{
			"value": "ssssssdf",
			"post_id": 8
		},
		{
			"value": "aa",
			"post_id": 7
		}
	]
}