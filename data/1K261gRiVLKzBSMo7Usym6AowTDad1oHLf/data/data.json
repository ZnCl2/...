{
	"title": "Dalec",
	"description": "一个普通的程序员",
	"links": "*   [Github](https://github.com/dalecgu)",
	"next_post_id": 8,
	"demo": false,
	"modified": 1483761901,
	"post": [
		{
			"post_id": 7,
			"title": "无屏幕安装Raspbian",
			"date_published": 1483243400,
			"body": "乘着双十二入手了一个树莓派3，但是买的是空的SD卡，要自己烧录系统到SD卡。按照树莓派[官网](https://raspberrypi.org)的指导很容易就成功了，可是我想用ssh连接到树莓派的时候竟然发现被拒绝了。原来最新的系统默认是关闭ssh的，可是我没有HDMI连接线，根本看不了屏幕，更没办法打开ssh。\n\n* * *\n\n折腾了几个小时，终于在Github上找到了[raspbian-ua-netinst](https://github.com/debian-pi/raspbian-ua-netinst)这个仓库，它提供了一个很小的镜像，这个镜像不仅可以通过网络安装Rapbian，而且提供了配置功能，它默认的配置就打开了ssh，非常方便。\n\n本来只要在它的Release里直接下载镜像然后烧到SD卡里，打开树莓派，等待一段时间就自动安装好了。\n\n* * *\n\n以下仅树莓派3需要\n\n截止到现在，它还没有发布可供树莓派3的版本，所以需要自己使用v1.1.x分支上的代码制作树莓派3用的镜像。\n\n过程其实不难，在[BUILD.md](https://github.com/debian-pi/raspbian-ua-netinst/blob/v1.1.x/BUILD.md)里有很详细的描述，但是它提供的代码仅适用于Debian，在macOS上不能使用。\n\n在Debian上先运行`aptitude install git curl bzip2 zip xz-utils gnupg kpartx dosfstools binutils bc`安装必要的工具，然后依次运行[clean.sh](https://github.com/debian-pi/raspbian-ua-netinst/blob/v1.1.x/clean.sh)、[update.sh](https://github.com/debian-pi/raspbian-ua-netinst/blob/v1.1.x/update.sh)、[build.sh](https://github.com/debian-pi/raspbian-ua-netinst/blob/v1.1.x/build.sh)，之后把生成的zip文件解压到SD卡就可以了。"
		},
		{
			"post_id": 6,
			"title": "Python采集点评网商家信息",
			"date_published": 1480996825,
			"body": "### 前期准备\n\n#### 背景\n\n朋友工作中需要采集点评网某分类下的商家信息，拜托我帮忙。对这类需求其实我是没什么经验的，不过还是答应了下来，就当给自己的小挑战吧。\n\n* * *\n\n#### 数据位置及采集流程\n\n##### 观察网页结构\n\n首先我看了一下点评网的网络结构。\n\n主页url是`http://www.dianping.com/`。\n\n![点评网主页](https://ooo.0o0.ooo/2016/12/05/5845399d86c96.png)\n\n具体某个分类的url形如`http://www.dianping.com/search/category/{city}/{main_category}/{sub_category}`，其中{city}是城市编号，如上海是1、北京是2，{main_category}是主分类，如食品是10，{sub_category}是子分类，如火锅是g110。注意这里的商家列表采用了分页的形式，第二页的url的{sub_category}部分是g110p2，以此类推。\n\n![点评网火锅](https://ooo.0o0.ooo/2016/12/05/584581f786a9a.png)\n\n商家主页的url形如`http://www.dianping.com/shop/{shop_id}`，其中{shop_id}是商家编号。\n\n![点评网商家](https://ooo.0o0.ooo/2016/12/05/584583e3074a7.png)\n\n##### 明确采集目标\n\n需求的商家信息要从商家主页中获得，主要包括商家名称、地址、电话等信息。\n\n##### 设定采集流程\n\n虽然最后需要从商家主页中获得信息，但是要定位到商家主页，需要知道商家的编号，这里的编号还是要从分类的页面中得到。分类的页面包含了商店列表，而且列表中每个商家都链接到了该商家主页，因此一定可以从列表中得到商家主页的url。\n\n总结一下，我构想的采集流程是：\n\n1.  根据需求找到对应的分类页面（手动）\n2.  根据分类上显示的商家数量计算出页数\n3.  访问每个页面得到所有商家主页的url\n4.  访问每个商家主页得到所需信息\n\n#### 网站对信息的保护\n\n商业性质的网站总是会有一些保护信息的措施，尤其是像大众点评这样有大量商家信息的网站，采用一些反爬虫的机制也很正常。写代码前简单的搜索了一下，发现确实存在这样的问题，所以代码中每次请求都随机等待了一会儿。\n\n#### 工具选择\n\n本文选择了python这个采集信息常用的语言，具体版本是3.5.2。\n\n### 编码\n\n#### 请求\n\n使用urllib来发送请求。\n\n```py\nimport urllib.request\nimport io\nimport gzip\nheaders = {\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Encoding': 'gzip, deflate, sdch',\n        'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4,ja;q=0.2',\n        'Cache-Control': 'max-age=0',\n        'Connection': 'keep-alive',\n        'Host': 'www.dianping.com',\n        'Referer': 'http://www.dianping.com/search/category/5/75/g3030',\n        'Upgrade-Insecure-Requests': '1',\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36'\n}\nrequest = urllib.request.Request(url, headers=headers)\nresponse = urllib.request.urlopen(request)\nif response.info().get('Content-Encoding') == 'gzip':\n    buf = io.BytesIO(response.read())\n    f = gzip.GzipFile(fileobj=buf)\n    html = f.read().decode('utf-8')\nelse:\n    html = response.read().decode('utf-8')\n```\n\n##### header\n\n这里如果不加header，点评网会直接返回403，所以header是必须的，具体header的内容可以参考手动请求的时候的header。\n\n##### gzip\n\n这里还根据相应编码方式做了不同处理，因为点评网返回的是gzip压缩后的响应，所以需要解压一下。\n\n#### 解析\n\n用了BeautifulSoup4解析得到的响应。例如以下代码解析出了某分类下商家数量。\n\n```py\nfrom bs4 import BeautifulSoup\nimport re\nsoup = BeautifulSoup(html, 'lxml')\nnum_str = soup.find('span', class_='num').string\nnum = int(re.match(r'\\((\\d+)\\)', num_str).group(1))\nprint('共有{:d}条数据'.format(num))\n```\n\n`BeautifulSoup(html, 'lxml')`这里用了`lxml`来解析，而不是默认的`html parser`，是因为不同的parser得到的结果不一样，而`html parser`在这里解析的结果不正确。\n\n如何知道数据位置，主要依靠观察源代码。正则表达式的内容比较复杂，不详述了。\n\n#### 随机等待\n\n之前提到为了防止请求太多被封IP，每次请求前随机等待一段时间，这里等1~5秒，没有遇到问题。\n\n```py\nimport random\nimport time\ntime.sleep(random.randint(1,5))\n```\n\n#### 保存数据\n\n因为朋友说要把信息保存到excel，所以去搜索了一下有没有可以用的库，发现选择还挺多的，一开始用了`xlwt`，结果发现它不支持xlsx格式，所以就换成了`openpyxl`，使用的方式也很简单。\n\n```py\nimport openpyxl\nwb = openpyxl.Workbook()\nws = wb.active\nfor r in enumerate(result):\n    ws.append(r)\nwb.save('data.xlsx')\n```\n\n这里的`result`是一个二维数组，里面每一行代表一个商家的信息（名称、电话、地址）。\n\n### 采集\n\n正式开始采集了。\n\n在写代码的时候选了一个小的测试集做测试，没想到测试的时候很正常，结果采集的时候才发现代码实在太弱了，遇到了些特殊情况就挂了，有商家没电话的、有多个电话的、甚至有没店名的……主要还是缺少经验，下次注意就不难处理了。\n\n调整后顺利完成了任务。"
		},
		{
			"post_id": 5,
			"title": "DevOps开发环境使用",
			"date_published": 1479225404,
			"body": "### 概述\n\n本文假设：\n\n1.  环境已经搭建完毕，如果没有的话，可以参考[DevOps开发环境搭建](http://127.0.0.1:43110/dalec.bit/?Post:4:DevOps%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88Jenkins+SonarQube+Docker%EF%BC%89)\n2.  已有一个能够在Tomcat中运行的项目\n\n* * *\n\n### Jenkins\n\n#### 创建项目\n\nJenkins中创建项目，类型选择Pipeline。\n\nBuild Triggers选择Build when a change is pushed to Github。Poll SCM可以填`H/5 * * * *`，这样每过5分钟就会检查一下Git仓库，如果有更新就会执行构建。\n\nSCM选择Git，然后填写仓库地址、分支，配置登录信息。\n\n#### Jenkinsfile\n\n在项目根目录建立一个名为Jenkinsfile的文件，用来定义Pipeline。以下是一个例子。\n\n```\nnode {\n    stage('SCM') {\n        git '<REPOSITORY>'\n    }\n    stage('QA') {\n        sh 'sonar-scanner'\n    }\n    stage('build') {\n        def mvnHome = tool 'M3'\n        def javaHome = tool 'jdk1.8'\n        sh \"JAVA_HOME=${javaHome} ${mvnHome}/bin/mvn -B clean package -Dmaven.test.skip=true\"\n    }\n    stage('deploy') {\n        sh \"docker stop demo || true\"\n        sh \"docker rm demo || true\"\n        sh \"docker run --name demo -p 80:8080 -d tomcat:jre8-alpine\"\n        sh \"docker cp target/<WAR> my:/usr/local/tomcat/webapps/\"\n    }\n    stage('result') {\n        archiveArtifacts artifacts: '**/target/*.war', fingerprint: true\n    }\n}\n```\n\n1.  从Git中取出最新代码\n2.  使用SonarQube Scanners检查代码质量\n3.  使用Maven打包，这里用到了Jenkins中定义的全局工具，并且跳过了测试（**不推荐**）\n4.  使用Docker部署\n5.  生成制品\n\n第一步可以换成`checkout scm`。\n\n如果第二步失败，可以尝试`sh '<SONAR_SCANNER_HOME>/bin/sonar-scanner'`。\n\n第四步使用了tomcat的jre8-alpine版本，如果不指明版本，tomcat会使用jre7，此时如果项目使用了1.8的编译器编译，不会报错但不也会正确运行。另外，alpine版本较小。\n\n### SonarQube\n\n首先需要在SonarQube中为用户生成一个token。\n\n然后在项目根目录建立一个名为sonar-project.properties的文件，配置SonarQube Scanner。\n\n例子如下。\n\n```\nsonar.projectKey=group01\nsonar.projectName=Demo\nsonar.projectVersion=1.0-SNAPSHOT\nsonar.sources=src/main/java\nsonar.login=<TOKEN>\n\nsonar.host.url=http://<IP>:9000\nsonar.language=java\n```"
		},
		{
			"post_id": 4,
			"title": "DevOps开发环境搭建（Jenkins+SonarQube+Docker）",
			"date_published": 1479225018,
			"body": "### 云服务器的选择\n\n虽然对阿里云比较熟悉，但是在队友的建议下我们还是选择了ucloud。单核1GB内存的配置已经足够，加上外网IP一个月也就一百出头。\n\n### 操作系统的选择\n\n这里选择的是Ubuntu14.04，当然16.04也完全可以，只是细节上有些区别。\n\n> 由于Docker不支持CentOS6，因此用CentOS的话必须用CentOS7或以上版本。\n\n* * *\n\n### Jenkins\n\n#### 安装\n\nJenkins需要运行在Tomcat中，这里有两种选择。\n可以先装Docker，然后Jenkins运行在Docker容器中。或者手动装个Tomcat。这里选了后者。\n\n从Tomcat官网上下载tar.gz压缩包，这里用的是8.5.6的版本。在服务器端使用`tar x`解压，即可使用。假设解压后目录为<tomcat_home></tomcat_home>\n\n把Jenkins官网上下载的war包放到<tomcat_home>/webapps目录下，运行`sh <TOMCAT_HOME>/bin/startup.sh`即可启动Tomcat并部署Jenkins。</tomcat_home>\n\n默认Tomcat会运行在8080端口，访问`http://<IP>:8080/jenkins`会进入Jenkins配置页面。\n\n> 如果访问不了且日志没问题，可以先考虑防火墙的问题，在云服务器提供商处改。\n\n#### 配置\n\nJenkins初始化时默认会把数据写入`~/.jenkins`目录，假设该目录为<jenkins_home>。\n第一次进入Jenkins的时候需要提供一个管理员密码，该密码可以从日志中获得，或者通过查看`<JENKINS_HOME>/secrets/initialAdminPassword`获得密码。\n插件可以跳过安装，稍后手动安装。</jenkins_home>\n\n##### 插件\n\n在系统管理>管理插件中可以安装插件。\n\n需要安装的插件有：\n\n*   Pipeline\n*   CloudBees Docker Pipeline\n*   Github plugin\n\n插件有可能会因为网络原因下载失败，此时可以去[插件仓库](http://updates.jenkins-ci.org/download/plugins/)手动下载，然后在管理插件页面“高级”Tab页下，上传插件。\n\n##### 全局工具\n\n在系统管理>Global Tool Configuration中可以配置全局工具。\n\n可以配置Maven、JDK等在Pipeline中使用。\n\n### SonarQube\n\n#### MySQL\n\nSonarQube需要数据库的支持，这里用的是MySQL。需要注意的是，直接`sudo apt-get install mysqlserver`在Ubuntu14.04上不可以，因为这个会下载5.5版本的MySQL，SonarQube不支持。应该安装`mysql-server-5.6`。\n\n连接MySQL数据库，创建sonar数据库，以及能够操作该数据库的用户。\n\n#### SonarQube配置\n\n下载SonarQube并解压，根据数据库的配置修改`<SONAR_HOME>/conf/sonar.properties`。\n\n#### SonarQube Scanners配置\n\n下载SonarQube Scanners并解压，根据数据库的配置修改`<SONAR_SCANNER_HOME>/conf/sonar.properties`。将`<SONAR_SCANNER_HOME>/bin`加入到PATH。\n\n#### 运行\n\n`sh <SONAR_HOME>/bin/<OS>/sonar.sh start`启动SonarQube，默认会运行在9000端口。\n\n### Docker\n\nDocker安装相对容易，参考官网说明即可。例如，[Ubuntu安装Docker](https://docs.docker.com/engine/installation/linux/ubuntulinux/)\n\n需要注意的是，为了便于Jenkins使用，需要把当前用户加入docker组，使当前用户可以不需要sudo就能运行Docker。"
		},
		{
			"post_id": 3,
			"title": "HttpClient登录知乎（侧重Wireshark的使用）",
			"date_published": 1479201370,
			"body": "因为课业的关系，最近开始研究网络爬虫相关的知识。\n\n根据《自己动手写网络爬虫》这本书的描述，我下载了HttpClient，结果HttpClient现在已经更新到4.4.1，书中给的代码不能用。网上有一个[例子](http://www.aichengxu.com/view/52657)，是用的HttpClient4.4.1模拟登录知乎，但是文章中关于使用Wireshark工具来抓包讲的不是很清楚，所以平生第一篇博客就献给这个主题吧。\n\n* * *\n\n### 使用Wireshark抓包\n\n用HttpClient登录知乎其实就是提交一个登录用的表单。首先得知道表单里要填写哪些内容，所以要用到Wireshark抓包分析出来。具体过程如下。\n\n1.  打开安装好的Wireshark\n    ![Wireshark初始界面](https://ooo.0o0.ooo/2016/11/14/5829e85113540.png)\n\n2.  左侧选择“以太网”，Start开始抓包\n    ![Wireshark抓包](https://ooo.0o0.ooo/2016/11/14/5829e851406d5.png)\n\n3.  在浏览器打开知乎，并进行一次登陆操作\n    ![登录知乎](https://ooo.0o0.ooo/2016/11/14/5829e850e73e8.png)\n\n4.  在Wireshark中会出现非常多的项，我们可以在左上角Filter里填写http来筛选出通过http协议的数据包，如图中筛选出的第一项就是我们要找的目标（http协议，填写的是表单，post方法，名字为login），双击打开可以看到详细信息，最后一个加号是表单的信息，展开发现它有四项，_xsrf,email,password,rememberme，第一项是随机生成的，接下来是用户名、密码、是否记住用户三项。\n    ![找到目标请求](https://ooo.0o0.ooo/2016/11/14/5829e85251a25.png)\n\n### 获得xsrf\n\n所以我们就得到了表单所要填写的内容，但是第一项_xsrf的值不能确定，我们得打开知乎首页，从首页的源代码获取。代码很简单，如下。（忽略了import和异常处理）\n\n```\n//选择Cookie策略，不然后面的代码可能会报Cookie错误\nRequestConfig requestConfig = RequestConfig.custom().setCookieSpec(CookieSpecs.STANDARD_STRICT).build();\n//新建客户端\nCloseableHttpClient httpClient = HttpClients.custom().setDefaultRequestConfig(requestConfig).build();\n//新建Get方法\nHttpGet httpGet = new HttpGet(\"[http://www.zhihu.com/](http://www.zhihu.com/)\");\n//执行Get\nCloseableHttpResponse response = httpClient.execute(httpGet);\n//现在responseHtml存储了获得的网页源代码\nString responseHtml = EntityUtils.toString(response.getEntity());\n//在源代码中找到xsrf的值\nString xsrfValue = responseHtml.split(\"<input type=\"hidden\" name=\"_xsrf\" value=\"\" )[1].split(\"\"=\"\">\")[0];\nSystem.out.println(\"xsrf:\"+xsrfValue);\n//关闭response\nresponse.close();\n```\n\n### 提交表单\n\n获得了xsrf值，接下来就是提交表单了。\n\n```\n//储存键值对的列表\nList list = new LinkedList();\n//存放表单需要的值\nlist.add(new BasicNameValuePair(\"_xsrf\", xsrfValue));\nlist.add(new BasicNameValuePair(\"email\",\"填入用户名\"));\nlist.add(new BasicNameValuePair(\"password\",\"填入密码\"));\nlist.add(new BasicNameValuePair(\"rememberme\",\"y\"));\nUrlEncodedFormEntity entity = new UrlEncodedFormEntity(list,Consts.UTF_8);\n//新建一个登陆的Post\nHttpPost httpPost = new HttpPost(\"[http://www.zhihu.com/login](http://www.zhihu.com/login)\");\nhttpPost.setEntity(entity);\n//执行Post\nhttpClient.execute(httpPost);\n```\n\n### 结果\n\n这样就登陆上知乎了，为了验证登陆的结果，可以加上如下代码。\n\n```\nhttpGet = new HttpGet(\"[http://www.zhihu.com/](http://www.zhihu.com/)\");\nresponse = httpClient.execute(httpGet);\nSystem.out.println(EntityUtils.toString(response.getEntity()));\nresponse.close();\n```\n\n如果在输出的结果中出现了`<span class=\"name\">你的用户名</span>`，就证明已经成功登陆知乎啦。"
		}
	]
}