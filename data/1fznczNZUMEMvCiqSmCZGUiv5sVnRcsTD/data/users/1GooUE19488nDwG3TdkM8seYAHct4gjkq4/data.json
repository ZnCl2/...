{
	"next_topic_id": 19,
	"topic": [
		{
			"topic_id": 1548753101,
			"title": "Welcome to the <ZeroNet Development> forum of the The First ZeroNet Council!",
			"body": "* [The Forum Rules](http://127.0.0.1:43110/1fzncGxeyDpbCz1CkvBkooEDsXGFq1GfK/?Topic:1549008843_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/)\n* [The Forum Manual & FAQ](http://127.0.0.1:43110/1fznc4VQbbrQAf4UzHrWS6W2mpGFqXSeD/?Topic:1616686869_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/)\n* [Why we need the ZeroNet Council](http://127.0.0.1:43110/1fzncZRMPGPBdyUBm2ATVSkRAsnvMq5VN/?Post:2)\n* [The Manifest of The First ZeroNet Council](http://127.0.0.1:43110/1fzncZRMPGPBdyUBm2ATVSkRAsnvMq5VN/?Post:1)\n",
			"added": 1548753100,
			"modified": 1616687477
		},
		{
			"topic_id": 1549014898,
			"title": "Features missing from ZeroTalk engine",
			"body": "TODO:\n\n* Moderation:\n  * Ability for the site owner and moderators to modify or delete any posts from UI. ✔\n  * Ability to manage users (ban, unban, modify limits) from UI.\n  * Ability to report a post to a moderator in a safe way (encripted, so only the moderator can see the report).\n  * 4-level moderation system:\n    * Administrator (Site Owner)\n    * Supermoderator (ability to modify `data/users/content.json`)\n    * Moderator (ability to modify posts directly)\n    * Corrector (ability to edit or hide posts, but users can choose if they trust a corrector or not)\n  * 3 levels of stickied posts: by Administrator, by Supermoderator, by Moderator/Corrector.\n  * Ability to split and merge topics.\n* UI\n  * Enable support for images in the markdown parser. ✔\n  * Post preview.\n  * The name of the last commentator (Now it is not shown: *\"1 comment on Jan 29, 2019 ━ started by geekless\"*)\n  * User profile pages.\n    * ZeroMe integration.\n    * Garbage collection: ability to easily find and remove comments of deleted topics, votes of deleted comments etc.\n  * The vote counter should start from zero. ✔\n  * The *self*-votes should be ignored. ✔\n  * The vote-based \"karma\" rating.\n  * Sorting the topic list by creation date, vote count, comment count, integral activity rating.\n  * Pinning comments on the top of the topic.\n* Data storage:\n  * Support for hubs. Each forum section should be placed on a different hub. Topics can be migrated between hubs.\n* Security:\n  * Each entry (post, comment, vote) should be signed individually, so that when a moderator edits and signs a user data, unmodified entries keep their signatures. The security state of each entry should be visible in the UI. (Unsigned votes probably should not be processed at all.)\n* Other:\n  * All the files of the engine should be moved to a single subfolder, so that the write access for updating the engine can be assigned to a separate set of keys, and shouldn't require the primary site key.\n\n[GitCenter/ZeroTalk++](http://127.0.0.1:43110/1H3qtUJRrghDHpY89CBeueVAZw8xbHuDLr/)",
			"added": 1549014896,
			"modified": 1616927772
		},
		{
			"topic_id": 1549045300,
			"title": "The primary key problem",
			"body": "### Role of the primary key in the functioning of the site\n\nWhat I call the primary private key is a private key, that matches to the site address.\n\nIf the primary private key is leaked or lost, that is a problem without solution, and the control over the site is lost forever.\n\nFor that reason, the primary key should be kept in a way as secure as possible and be used only in absolutely critical situations. In the everyday work, secondary keys should be used, so if they leaked or lost, new secondary keys can be created and signed with the primary key. Unfortunately, the commonly used ways of working with zites now are very careless about the primary key.\n\n### The current status\n\nHere are some steps we can take for the better security at FZNC sites:\n\n* The moderation should be done using the moderators' own keys, not the primary key.\n  Status: totally possible.\n  We already do that.\n\n* Appointment and removal of moderators (and supermoderators) should not require the primary key.\n  Status: not implemented, but looks possible.\n  We need an intermediate `data/content.json`, that lies between the root `content.json` and `data/users/content.json`.\n\n* The engine updates should not require the primary key.\n  Status: more investigation is required; the implementation may have some difficulties.\n  All the engine files should be placed in a separate subdirectory, so that a separate `content.json` can be used for signing. What can be a trouble is `index.html`, since his name and location is fixed on the level of the ZeroNet engine. That file should probably be a simple proxy to the real entry point and have no code that can require update.\n\n* Changing internal site settings should not require the primary key.\n  Status: not yet implemented.\n  Some engines, including ZeroTalk, make use of the root `content.json` to save settings. Those settings should probably be moved to `data/data.json` and controlled by the intermediate `data/content.json`.\n\n* Changing the site metadata should not require the primary key.\n  Status: currently impossible, modification of ZeroNet engine is required.\n  Metadata, such as the site title and description (and maybe some other in the future) are saved in the root `content.json`. We do need the primary key to modify the title, displayed in the \"Hello ZeroNet\" UI.\n\n\n### Possible emergencies and their consequences\n\nIn the case of an ideal separation of tasks of the primary and secondary keys, the following situations and their consequences are possible:\n\n| Primary key  | Secondary key | Result  |\n| -------------|---------------|---------|\n| okay         | lost          | The secondary key is dropped and a new one is created. |\n| okay         | leaked        | The secondary key is dropped and a new one is created. The site is restored from a backup, if there was some damage. |\n| lost         | okay          | The site keep working. |\n| lost         | lost          | The site keep working, but any management and moderation is not possible. |\n| lost         | leaked        | The site most likely be destroed; exclusive control over the address is lost. |\n| leaked       | (irrelevant)  | The site most likely be destroed; exclusive control over the address is lost. |",
			"added": 1549045297
		},
		{
			"topic_id": 1549168166,
			"title": "(discussion with Kaffie)",
			"body": "> I don't mind providing input, but I don't like the idea. The two big problems outlined appeared to be spam and death of solely managed zites.\n> \n> The second problem has already been solved. As there's ways to easily clone a site and move on, and also merger sites which should become the norm. Merger sites allow the backend posting to be decentralized, so there's no single point of failure (just post to a different hub, or have your own). While the front end can be easily cloned and managed if the original developer stops for whatever reason. See: ZeroMe. ZeroMe won't die, even if Nofish goes away. As the hubs are decentralized (no reliance on him adjusting permissions), and the front end can be cloned and managed.\n> \n> The first problem, with the spam, is indeed an issue. Having hub providers ban, and users be able to block (and prevent the downloading of a user's posts) are more than enough to stop spam. If need be, switch to a proof of work/stake/identity cert provider. For instance, have a cert provider that requires people to show they are an authentic user and are manually approved. From there we have \"trusted certs\" which can be allowed, and in the case of a raid, just ban posting from non-trusted cert providers. A variety of solutions could be done here.\n> \n> I'd rather not put major zites in the hands of a few. But rather focus on turning them into mergers so that moderation is decentralized. We saw the death of zites like ZeroTV and ZeroTorrent. The dev leaves, and they either don't update, or users eventually run out of space. Merger sites solve this entirely.\n> \n> Though I do like the idea of developing a set of core zites that have these decentralization and protection principles in mind.\n> \n> What we really need is focus. We have like 3 wikis with split users, and that lead to apathy. There's a ton of forums, but it's hard to find the ones people are talking on. Something like ZeroVoat but with merger sites would probably be ideal for a forum. KopyKate is great, but appears to be suffering from the centralized owner issue. No hubs = not much incentive to post as the next video site will pop up and end up starting from scratch.\n> \n> I think a good idea of the council would be to design proper/good merger site standards. So that the same sets of data can be accessed by a variety of sites. Proper post metadata, design structures, etc. That way we don't end up with duplicated work. Why have 3 wikis all dying with different standards, when we could have a single merger db accessed by 3 different wiki frontends?\n\n\nThank you for the feedback!\n\nAbout hubs - sure, it is the right design, but it's not a silver bullet. Popular sites have a value that grows over time - that is the address. If the address is referenced from a million places, when the site gets unmaintained, it will be the real pain. We faced with that on Ru-ZeroTalk. Its owner, @shift, set the limit of 30K and then disappeared for years. That was a real issue, since the address is referenced from the main ZeroTalk, from ZeroTalk clones in other languages, from blogs, from ZeroWiki etc. So if you want to run an alternative, you have to contact to all the owners of localized ZeroTalks and ask for changing the links in the top bar. Maybe some refuse or already dead or don't go online for years too. Balancer ran a couple of alternative forums, but they weren't too popular, maybe many users just didn't get to them. People just went to Ru-ZeroTalk, were taking for 2 or 3 days, and then disappeared, when limit reached.\n\nA few days ago, @shift was back and increased the limit greatly. Suddenly, it turned out we had a great number of active users there, haha.\n\nI agree, the hub system is a great improvement. But it still cannot solve that kind of situations in a transparent way.\nIf Ru-ZeroTalk had the hub support, we could run some alternative hubs. Nice. But it is still the problem letting users know they should activate the new hub. The external links still point to the main interface site; there is no posibility to stick the topic containing the notification, etc. There is no big difference: asking users for migrating to another site or asking users for migrating to another hub. It requires some manual actions from a user in the both cases.\nAn unmaintained site also gets no updates of the engine, no matter if it supports hubs or not. (Ru-ZeroTalk seems to run the 2 years old engine.)\n\nSo, the address issue is still the issue. Actually, I have no idea, how it can be solved in a perfect way. You either control the address or not. If not, the game is over.\nFor that reason, the community should develop principles of the collective management for the decentralized sites. Now zites are managed in the same way how the first pages on the Internet were managed: one person, who does everything. But as the network grows, it won't be enough.\n\nHere're my thoughts on the matter:\nhttp://127.0.0.1:43110/1fznczNZUMEMvCiqSmCZGUiv5sVnRcsTD/?Topic:1549045300_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/\n\nSo (in the best case, when the keys aren't leaked) we can distinguish 3 stages of the zite’s life:\n1) the primary key owner is alive and maintains the zite.\n2) the primary key owner is dead or the key is lost, but the secondary keys are okay.\n3) all the keys are lost, so the zite is completely unmaintained.\n\nThat is the fate of any cryptography-based site, and what we can do is to somewhat extend their life, cooperating together. A single person may die any day, but if there is some cooperation, together we can guarantee, maybe, 40-50 years of life for a site.\n\nSpam. The proof of work is not a solution for the same reason as blockchain is not a solution (of anything, I guess. lol). Corporations can provide a \"proof\" for a million spam accounts, while an individual tries to mine just a single one. Since Zeronet challenges all the centralized web, big money of big people is at stake. So we should be ready for the worst possible scenarios.\n\nPoW can be useful to some extent, while coroporations don't take us seriously. Here's my idea: http://127.0.0.1:43110/1BLoGBTid3NhGu8ts3fAfHJprnbrH3wfTV/?Post:41\nBut I'm actually not sure if that idea is worth a cent.\n\nProof of identity can be helpful too, but it brings us back to centralized things. The question is \"Do you trust the certification center or not?\". An answer may be complicated. Probably, you trust, but not absolutely. Probably, we need several independent centers. Probably we want to make sure they are truly independent, but we have no idea how it can be done. The certification center stops functioning someday, and sites should be updated to some new center or centers in order to allow new users access. But many sites are unmaintained...\n\nAnyway, all the things we discuss here are nice ideas, that have no implementation. We have no support for hubs in ZeroTalk. We have no support for PoW. We have just one generally accepted certification center, that requires from users no kinds of proof. Etc. A lot of work waits ahead, we now are just scratching the surface of the door to the P2P world.\n\nSo I just thought, I should do what I could. I know how to run a Clearnet forum. I know how to write code. So I'm going to try running a forum in ZeroNet and make the engine as powerful as possible, compared to popular forum engines for Clearnet. (http://127.0.0.1:43110/1fznczNZUMEMvCiqSmCZGUiv5sVnRcsTD/?Topic:1549014898_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/)\n\nThe same about the user management. Maybe we'll get some wonderful client-side tools for that, but until that, I just stick to the strategy that the forum should be switched to the whitelisting policy in the case of DoS. So we need some moderators who are technically able to do that (and some scripts).\nAnd so on.\n\n--------------------\n\nSeveral side considerations:\n\n> See: ZeroMe.\n\nLooks like ZeroMe should use the hub-based user registry too. The current registry sticks to the single certificate provider, so the profile seach doesn't work for alternative providers. A user should be able to choose, which registries he/she wants to use. Now it is not possible, since the address of the registry is hardcoded.\n\n> But rather focus on turning them into mergers so that moderation is decentralized.\n\nIt tends to single-user hubs finally. I totally like the idea, and I seemed to already post some thoughts about that in comments in Shouko's Blog: http://127.0.0.1:43110/blog.shouko.bit/?Post:5\n\nWith such design, there's no difference between a single-user hub and a blog, and there's little or no difference between a social network front-end and ZeroHello news feed.\n\nOn the other hand, we can succesfully use more centralized engines as well, depending on the purpose. When you post a message on a site that is controoled not by you, the owner can do with your data just anything. He/she can modify it, delete it, create some posts that you didn't actually write etc. The existing engines hides that possibility and gives you the false impression that your data is safe. But it is just a lack of interface for taking moderation actions. Technically, it is totally possible and is supported by ZeroNet engine, so the site owner can modify your files with a text editor right now.\n\nI believe, we should try all possible ways and design choices. If we are able to create a decentralized (in terms of the data storage) forum with the centralized moderation system, we should try and see what it can be useful for.\nIf we can create a completely decentralized social media, composed from thousands of hubs, we should try it too.\n\nThat is why both centralized moderation and decentralized hubs are planned in my ZeroTalk todo list.\n\n> I'd rather not put major zites in the hands of a few.\n\nAs I said above, the infrastructure depends on cryptographic addresses, and addresses are the major point of failure. We can potentially make a new fork of the front-end every week, but it doesn't address the issue, how to switch to new addresses and how we can determine if we can trust them.\n\nGet me right, it's not that I'm trying to control major components of the network. I run some sites and invite people to take a part in their management. Why? Because a team can reach better results than an individual.\n\nThere are many successful forums with a great community on Clearnet, that are maintained by enthusiasts, and we already know how to manage that kind of things. I'm not talking about commercial projects, but about such projects as for example lingvoforum.ru, where you can contact to the admin, and he is a real person, who is running his forum for his own money for 15+ years.\nSure, we should develop better tools, but we need to start somewhere.\n\n",
			"added": 1549168162
		},
		{
			"topic_id": 1549522375,
			"title": "The data structures of the major site engines",
			"body": "\n**ZeroBlog**\n\n* data/data.json\n  * post (array)\n    * `post_id`\n* data/users/*/data.json\n  * comment (array)\n    * `comment_id`\n    * `post_id`\n  * post_vote (hash map)\n    * `\"{post_id}\"` -> 1\n\n**ZeroTalk**\n\n* data/users/*/data.json\n  * topic  (array)\n    * `topic_id`\n  * comment (hash map)\n    * `\"{topic_id}_{auth_address}\"` ->\n      * `comment_id`\n  * topic_vote (hash map)\n    * `\"{topic_id}_{auth_address}\"` -> 1\n  * comment_vote (hash map)\n    * `\"{comment_id}_{auth_address}\"` -> 1\n\n**ZeroMe**\n\n* data/users/*/data.json\n  * post (array)\n    * `post_id`\n  * comment (array)\n    * `comment_id`\n    * `post_uri` (`\"{auth_address}_post_id\"`)\n  * post_like (hash map)\n    * `\"{post_uri}\"` -> date\n  * follow (array)\n    * `follow_id`\n    * `hub`\n    * `auth_address`\n\nBasically, it's all about the same, but the minor differences make the databases incompatible. I have had an idea of implementing an unified engine, that can transparently handle all 3 types of sites, but it seems not to be easily  possible with the existing DB design. dbschema doesn't support importing data of varying structure from files of the same name.\n\nIt is unclear, if the merger feature can solve the issue. At least, I see no evidence of that in the dbscheme of ZeroMe:\n\n```\n    \"db_file\": \"merged-ZeroMe/ZeroMe.db\",\n    \"version\": 3,\n    \"maps\": {\n        \".+/data/userdb/.+/content.json\": {\n            \"to_json_table\": [ \"cert_auth_type\", \"cert_user_id\" ],\n            \"to_table\": [\"user\"]\n        },\n        \".+/data/userdb/users.*json\": {\n            \"to_table\": [\"user\"]\n        },\n        \".+/data/users/.+/content.json\": {\n            \"to_json_table\": [ \"cert_auth_type\", \"cert_user_id\" ],\n            \"file_name\": \"data.json\"\n        },\n        \".+/data/users/.+/data.json\": {\n            \"to_table\": [\n                \"post\",\n                \"comment\",\n                \"follow\",\n                {\"node\": \"post_like\", \"table\": \"post_like\", \"key_col\": \"post_uri\", \"val_col\": \"date_added\"}\n            ],\n            \"to_json_table\": [ \"hub\", \"user_name\", \"avatar\", \"intro\" ]\n        }\n    },\n```\n`\".+/data/users/.+/data.json\"` has no merger prefix, but perhaps if I add `\"merged-ZeroMe\"` prefix, it will work. I should check it out. If so, it allows parsing different DB layouts into different tables without conflicts.\n\nSo... we have **ZeroMe** scheme, which should be supported just because it's the only hub-based media right now.\nWe also have to support **ZeroTalk** scheme to make the transparent migration between vanilla ZeroTalk and our engine possible.\nBut the both are kind of not perfect, so, ideally, we should support the 3rd, redesigned scheme as well.\nLooks complicated.\n",
			"added": 1549522370
		},
		{
			"topic_id": 1562248570,
			"title": "ZeroNet Roadmap by @geekless (unofficial)",
			"body": "## Core\n\n* P2P Connectivity:\n  * [Improved tracker exchange facility.](https://github.com/HelloZeroNet/ZeroNet/pull/2068)\n  * [TrackerList plugin.](https://github.com/geekless/ZeroNet/tree/TrackerList/plugins/disabled-TrackerList)\n  * Improve Bootstrapper to simplify the configuring onion-based `zero://` trackers.\n  * Move the BitTorrent-related code from SiteAnnouncer.py to a separate plugin.\n  * BitTorrent-compatible DHT, to make use of the existing DHT infrastructure. Should smoothly operate in both BitTorrent-compatible way and ZeroNet-specific way via Tor.\n  * Support for I2P.\n  * ZeroNet client should also be able to operate as a BitTorrent client, making possible to create, share, seed and download torrents via ZeroNet user interface, transferring data with all the power of ZeroNet connectivity facilities.\n  * Content-addressable storage. (Freenet-like? IPFS-like?)\n  * Distributed cryptographically-secured content cache. (Freenet-like or Freenet-compatible)\n  * (?) DHT-based distributed seach index for searching sites by keywords.\n  * (?) Implement a BitTorrent tracker as a ZeroNet plugin.\n\n* Content:\n  * [Support different cryptographies](https://github.com/HelloZeroNet/ZeroNet/pull/2053) (by @gitcenter)\n  * [PeerMessage plugin.](https://github.com/HelloZeroNet/ZeroNet/pull/2024) (by @gitcenter)\n  * [BackgroundProcessing plugin.](https://github.com/HelloZeroNet/ZeroNet/pull/1451) (by @gitcenter)\n  * [Delete data files of muted users](https://github.com/HelloZeroNet/ZeroNet/issues/1855)\n  * Support for entirely anonymous user certs. ([some thoughts](http://127.0.0.1:43110/1BLoGBTid3NhGu8ts3fAfHJprnbrH3wfTV/?Post:41))\n\n## Site Engines:\n\n* Improved blog engine. (See [ZeroBlog++ Roadmap](http://127.0.0.1:43110/19fZJhX7FGENeRRLB8ftgrziBJ5vsLtFog/?Post:10))\n* Improved forum engine. (See [ZeroTalk++ Roadmap](http://127.0.0.1:43110/1fznczNZUMEMvCiqSmCZGUiv5sVnRcsTD/?Topic:1549014898_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/))\n* Improved wiki engine. (@gitcenter is working on Kiwipedia)\n* Code hosting service. (The improved version of Git Center is in development by @gitcenter.)\n",
			"added": 1562248564
		},
		{
			"topic_id": 1616664124,
			"title": "ZeroNet development? We need new heroes! We need you!",
			"body": "Hi, guys!\n\nIn November 2020, I had been working on a patch set for ZeroNet, but the lack of spare time did not allow me to bring the work to its logical conclusion.\n\nHere is the change log draft of what has been done: https://github.com/zeronet-enhanced/ZeroNet/blob/massive-rework/ZNE-ChangeLog/ChangeLog-0.8.0.md\n\nNow, when the upstream seems to be almost dead, it's time to join forces and make an actively developed fork. There are many wonderful patches and ideas here and there on the network, but we seem to have no place to commit them to any more. So we need to move on on our own.\n\nIf you have some dev skills and motivation, feel free to contact me. https://github.com/zeronet-enhanced/ is an organization profile, and I can add new devs as \"members\" or \"owners\" for the organization.\n\nMy primary goals/interests in developing ZeroNet are:\n\n* Improving the stability.\n* Fixing the connectivity and scalability issues.\n* Implementing new transports, such as I2P. Or \"sync over Freenet\". Or maybe the offline update mode aka \"sync this node from a USB flash stick\".\n* Bringing support of more crypto algorithms besides good old Bitcoin's elliptic curves.\n* Implementing the multisign support of the root `content.json`: *right-way-this-time*. The current implementation is broken.\n* Fixing various flaws in user-level muting/blocklisting.\n* Implementing the darknet mode aka friend-2-friend mode, so one can use ZeroNet in a RetroShare-like manner.\n* Bringing support of flexible zite-level control over user space limits etc, so zite policies may at some extent be scaled and controlled by its users, when the owner abandoned it.\n* Implementing a declarative API that makes possible running a zite with dynamic AJAX updates and without executing an untrusted zite-level JS code in your browser. Running an untrusted code should be an option, not the core requirement.\n* ...etc\n\n\nNot my goals:\n\n* Breaking things just because they are implemented \"in a wrong way\".\n* Rewriting the engine in a shiny new programming language.\n\n**P.S. Related links:**\n\n* [\\[BUG\\] ZeroNet losts user data](http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1616552733_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/+BUG+ZeroNet+losts+user+data)\n* [nofish implemented a misfeature for wiping inactive users (??!)](http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1616648188_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/nofish+implemented+a+misfeature+for+wiping+inactive+users)\n\n**UPD1:**\n\nThere for sure should be the backward compatibility with the existing zites.\n\nAs far as I know there was just a single major compatibility breakage in the core during the whole ZeroNet development. But it broke only a single test nofish's zite, ZeroBoard. We should keep going this way.\n\n**UPD2:**\n\nSince nofish is missing for long time, I cannot consider the mainline ZeroNet forum as secure any more. We have no clue where is nofish and where are his private keys. I assume the worst: the keys are in wrong hands and the zite may be erased at any time.\n\nSo I move here all the important topics related to the development. I keep using ZeroTalk for reposting updates, but the official forum of ZeroNet Enhanced is here.\n\n[Topic at Talk.ZeroNetwork.bit](http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1616503348_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/)",
			"added": 1616664117,
			"modified": 1616664538
		},
		{
			"topic_id": 1616666284,
			"title": "What if the original infrastructure is lost?",
			"body": "[link](http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1603476273_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di/Can+we+get+a+regular+userdata+size+limit+increase+And+true+decentralization#)\n\n> > \"modified\": 1590595520\n> > 27 May 2020\n> \n> No updates since that.\n> \n> Which ZeroMe hubs are controlled by nofish? I guess at least these ones:\n> \n> > http://127.0.0.1:43110/1White24UrrwQrD86o6Vrc1apgZ1x1o51/data/users/content.json\n> > Jan 10 2020\n> \n> > http://127.0.0.1:43110/1BLueGvui1GdbtsjcKqCf4F67uKfritG49/data/users/content.json\n> > Aug 13 2019\n> \n> > http://127.0.0.1:43110/1SunAWK2VUT9GQK32MpwRfFPVgcBSJN9a/data/users/content.json\n> > Feb 18 2020\n> \n> > http://127.0.0.1:43110/1RedkCkVaXuVXrqCMpoXQS29bwaqsuFdL/data/users/content.json\n> > Nov 30 2019\n> \n> ZeroSite doesn't receive any automated updates as well.\n> \n> So I conclude, nofish's servers/laptops/whatever that ran the maintenance routines for his zites are offline for long time.\n> \n> We don't even know if he is alive or not. Is it safe to continue using these zites? They can be erased at any time, if the keys are in wrong hands.\n> \n> Fortunately, ZeroID keeps working. But can we trust it?\n\n* ZeroID is the single point of failure. We need a trusted alternative.\n* I should clone all the sources at github. I don't have all the copies.\n* There are many interesting discussions and pull requests there. How can we back up them?\n* Our own application image at Dockerhub.\n* A website for distributing binaries for Windows?\n* [ZeroName](http://127.0.0.1:43110/1Name2NXVi1RDPDgf5617UoW7xA6YrhM9F/) is one more point of failture. The default domain name resolution depends on it. It is possible to set up an intergation with Namecoin running locally, but it is somewhat complicated.\n* .bit domains, registered by nofish. We cannot rely on them any more?\n* HelloZero and the application updater. Obviously, the fork should use separate copies.\n* nofish's ZeroMe hubs. Well, it is a problem for their particular users. But it's a HUGE problem.\n* [ZeroSites](http://127.0.0.1:43110/Sites.ZeroNetwork.bit/). I intended to make a similar service with more features, but it were just plans for the distant future. Now we need to keep a backup at least, since that zite is a big deal for the network.",
			"added": 1616666276,
			"modified": 1616679258
		},
		{
			"topic_id": 1616822056,
			"title": "0.8.0 Roadmap",
			"body": "* ☑ Fix Docker image building issues.\n* Update the bundled Tor to the latest version.\n* Investigate and fix, if possible, sporadic tor connectivity problems.\n* Distribution for Windows... (Have no idea if this is difficult or not... I'm a Linux user.)\n* ☑ Our own ZeroHello.\n  * Import the favorites list from the old ZeroHello automatically.\n* Our own updater service.\n* Throw away the hardcoded ZeroHello site list. Add the capability to edit the default site list by trusted users.\n* Bundle @gitcenter's name.yo domain resolver.\n* Merge the latest commits from the upstream.\n\n**Related topics:**\n\n* [ZeroNet forgets delivering updates](http://127.0.0.1:43110/1fznczNZUMEMvCiqSmCZGUiv5sVnRcsTD/?Topic:1635324765_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/)",
			"added": 1616822047,
			"modified": 1635324841
		},
		{
			"topic_id": 1635274813,
			"title": "[docs] How ZeroNet dispatches updates",
			"body": "ZeroNet implements both pushing and pulling updates, as shown on the following diagram:\n\n```\nReceived an update from a peer (*push*) <---+t\n|                                           |o\n| Request some peers for updates (*pull*)   |\n| |                                         |a\n| |                                         |n\nv v                                         |o\nGot something new? --> No -> Do nothing     |t\n|                                           |h\n|                                           |e\nv                                           |r\nYes                                         |\n|      Files signed on-site                 |p\n|      |                                    |e\n|      |                                    |e\nv      v                                    |r\nSend updates to a few peers (*push*) -------+\n```\n\nPulling is implement with the command `listModified` in the [Network protocol](https://zeronet.io/docs/help_zeronet/network_protocol/#listmodified-site-since).\n\nTo put it simply, the command means \"Hey, what's new since the given date?\". The question is asked to a few random peers, and peers send the lists of changed files in response.\n\nPulling is performed:\n* Manually, when you press \"Update\" or \"Check files\" button.\n* Automatically on the app start up, after computer wake up etc.\n* Periodically with some rate.\n\nPushing is implemented with the command `update` ([link](https://zeronet.io/docs/help_zeronet/network_protocol/#update-site-inner_path-body-diffs)).\n\nPushing is performed:\n* When you publish data to a site.\n* Automatically when new data arrived from a peer either by request or voluntarily.\n\nIn addition to this basic scheme, there also is the push back mechanism. When a node receives a response to the `listModified` request, it detects files missing or outdated on the other peer and initiates pushing updates to that particular peer.",
			"added": 1635274803
		},
		{
			"topic_id": 1635324765,
			"title": "[network][bug] ZeroNet forgets delivering updates",
			"body": "The bug affects all versions.\n\nInfo: [How ZeroNet dispatches updates](http://127.0.0.1:43110/1fznczNZUMEMvCiqSmCZGUiv5sVnRcsTD/?Topic:1635274813_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/)\n\nZeroNet node keeps the last modification time for each site. When a node receives a new file, the modification time moves forward.\n\nWhen a node checks the network for updates, it takes modification time, substracts 24 hours and asks 3 random nodes: \"What's new since the given time?\"\n\nWhile there was just a single update source for each site (i.e. the site owner), everything worked as expected. The problem arises on sites with user-generated content, where any number of nodes can send and receive multiple updates at any time. We still have just a single global modification time for a site, but sites now in fact consist of thousands \"micro-sites\" for each single forum poster, blog commenter etc.\n\nThe problem scenario:\n\n* Node A signs new update on Sept 19. But it fails to publish the update to the whole network. Either it had poor network connectivity or no peers were online that day.\n* Node B publishes the update on Sept 21.\n* Node C goes online. It remembers that the latest update was on Sept 11. It substracts 24 hours and asks 3 random peers, what new has been going on on the site since Sept 10.\n* Those 3 peers already received the update from node B and so they resend the update to node C.\n* Node C forwards the modification time to Sept 21.\n* In this way, the update is distributed over the network, and all the nodes sets the latest modification time to Sept 21.\n* Node A goes online.\n* Nodes periodically ask each other for updates, but all of them calculate the \"since\" value as Sept 21 - 1 day = Sept 20. For that reason, when node A is asked for updates it does not return the information about the Sept 19 update. That update \"gone under the event horizon\".\n\nThis particular scenario may seem unlikely. However, losing updates is a quite common problem in practice. Depending on the network configuration, connectivity issues and rate of publishing new updates, it happens from time to time. Missing updates tend to accumulate on long-existing nodes that hosts large sites for years. Other nodes sync from these nodes and get plagued of the same.\n\nThe problem is compounded by another bug, related to the method used to store the modification time in the DB. (I'll post a different topic on this later.) And long-running nodes do suffer of both.\n\n**Why it is implemented in this way**\n\nOptimization. To save the network bandwidth and improve the update check time.\n\n**How it can be fixed**\n\nThe problem is `since` field in the `listModified` command. The deeper in the past we drop `since` value, the less probability of loosing updates. When `since` is zero, other nodes send us the full catalogue of the site \"since the beginning of time\", so we can clearly see if we miss some updates.\n\nIn the upcoming 0.8.x, I plan to implement automatic dropping `since` deep into the past on every Nth update check. It's already partially implemented, but I haven't yet decided, if it should be single-level or multi-level and what should be the rates.\n\n**How it can be overcome right now**\n\nManually:\n\nWhen you press \"Check files\" in the site menu on ZeroHello, it does 2 things:\n* Checks if some files are missing or have a wrong size and redownloads those.\n* Forcibly sets since = 0 and performs the update check.\n\nThis, however, doesn't help if peers you connected to miss the same updates.\n\nThis also doesn't help against another mtime-related bug, so you still may have missing updates. :-(",
			"added": 1635324754,
			"modified": 1635330980
		},
		{
			"topic_id": 1635414903,
			"title": "[idea] PoW-based size limits",
			"body": "#### *How it should work for end user:*\n\nA site owner sets up the settings how the space is allocated on the site depending on PoW calculation.\n\nA user's computer runs a PoW calculation in background. The more time it spends for calculation, the more space on a site the user is allowed to take.\n\nA user is also able to help another user calculating PoW by just clicking on his/her name on the site and pressing the appropriate option.\n\nAll the magic is done under the hood. You just leave the computer turned on for some time and when you back, your size limits are increased on all sites. (If you're lucky.)\n\nYou also don't have to register a unique user ID anywhere, you can post under plain bitcoin address.\n\n#### *How it may be implemented:*\n\n**The content verification part:**\n\nWhen a user's `contents.json` is arrived, ZeroNet calculates the PoW hash from the file's owner bitcoin address and the salt provided in `contents.json`:\n\n```\nhash = hash_calculation(address, salt)\n```\n\nZeroNet refers to `data/users/contents.json` to find out what is the rules for converting hash into the size limits.\n\nIt uses the provided size limits instead of the default ones.\n\n**The PoW calculation plugin:**\n\nA plugin implements an API for sites, that allows to put in queue a PoW calculation for a given address or a list of addresses.\n\nThere should also be some settings, how many CPU share the calculations may take, and how long a single calculation can last.\n\n**The site engines**\n\nWhen you visit a site, the site engine takes your cert address and tells to the plugin: \"Calculate the PoW for that address\". If the user chose some users to help to, it also queues calculation for those users.\n\nWhen the plugin finds any hash value better than the previously known, it writes the salt into a DB.\n\nWhen you revisit the site or navigate between pages, the site engine checks if the plugin found a better salt. If it did, the engine writes the salt into your `contents.json`.\n\nThe salt is suitable for any site, so when you navigate to another forum, blog etc, they update the saved salt as well.\n\nIf you have calculated a salt for another user, an entry for that is also written in your `contents.json`. The site engine periodically checks other users' files to find any better salt that may be calculated for you by others.",
			"added": 1635414891,
			"modified": 1635415446
		},
		{
			"topic_id": 1635439280,
			"title": "[bug] Nodes consider updated files old",
			"body": "Probably related to: https://github.com/HelloZeroNet/ZeroNet/issues/2476#issuecomment-679242160\n\nMore investigations needed:\n\n```\n[16:32:12] Site:1TaLkF..jipT CheckModifications: Peer:92.237.127.163 of 1TaLkF..jipT has older versions of 3 files\n[16:32:12] Site:1TaLkF..jipT CheckModifications:     data/users/1JT342LZCKfgp8r2FYMruVc2jvFXk3gL24/content.json: 1585793911 < 1608832528\n[16:32:12] Site:1TaLkF..jipT CheckModifications:     data/users/14ZGwsuBg5ZUhu3VZpp3yXeVQcES4N4DeN/content.json: 1608499273 < 1608499460\n[16:32:12] Site:1TaLkF..jipT CheckModifications:     data/users/19uAt7evT6ejhbTih1UTvPokD9KrBQ4RnY/content.json: 1608690350 < 1608765333\n[16:32:12] Site:1TaLkF..jipT CheckModifications: Peer:94.180.229.255 of 1TaLkF..jipT has older versions of 6 files\n[16:32:12] Site:1TaLkF..jipT CheckModifications: choosing 3 files to publish back\n[16:32:12] Site:1TaLkF..jipT CheckModifications:     data/users/1FQGz1xxSdqHiAizoXv9NSQadsZjLixG6n/content.json: 1577824337 < 1589619401\n[16:32:12] Site:1TaLkF..jipT CheckModifications:     data/users/1HUJnDBVdBTtNGsr1V5xZJiNLwd6T7jEW5/content.json: 1568050792 < 1601082088\n[16:32:12] Site:1TaLkF..jipT CheckModifications:     data/users/1Drej4kKBea1TryGPYfVZgzDxaB71rsKkK/content.json: 1587450356 < 1601772228\n[16:32:15] Site:1TaLkF..jipT CheckModifications: Peer:198.199.119.87 of 1TaLkF..jipT has older versions of 3 files\n[16:32:15] Site:1TaLkF..jipT CheckModifications:     data/users/1APx2pf1wNGFz4mWCN2sFzrtTGzrLKZUQ2/content.json: 1608533926 < 1618036694\n[16:32:15] Site:1TaLkF..jipT CheckModifications:     data/users/16nVSXbe2MD2vv2bZHGn4B6CiNvdpikmCE/content.json: 1610596490 < 1610598974\n[16:32:15] Site:1TaLkF..jipT CheckModifications:     data/users/1NRE1HXTrFT9uH9N84FbiQ59FAffw554UP/content.json: 1623188550 < 1632434121\n[16:32:16] Site:1TaLkF..jipT [OK] 94.180.229.255:25597: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 94.180.229.255:25597: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 92.237.127.163:20903: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 92.237.127.163:20903: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 92.237.127.163:20903: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 94.180.229.255:25597: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 198.199.119.87:11558: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 198.199.119.87:11558: File not changed 1/1\n[16:32:16] Site:1TaLkF..jipT [OK] 198.199.119.87:11558: File not changed 1/1\n[16:32:22] Site:1TaLkF..jipT CheckModifications: Peer:82.64.194.248 of 1TaLkF..jipT has older versions of 1 files\n[16:32:22] Site:1TaLkF..jipT CheckModifications:     data/users/12V9AjYgcFFTD5rF89JUNqYcQTm2knBnq4/content.json: 1611785076 < 1628437478\n[16:32:23] Site:1TaLkF..jipT [OK] 82.64.194.248:52452: File not changed 1/1\n```\n\nAnd sometimes they just returns no answer:\n\n```\n[16:34:38] Site:1UDbAD..MkoV CheckModifications: Peer:92.237.127.163 of 1UDbAD..MkoV has older versions of 26 files\n[16:34:38] Site:1UDbAD..MkoV CheckModifications: choosing 3 files to publish back\n[16:34:38] Site:1UDbAD..MkoV CheckModifications:     data/userdb/1PSaHi14Xh4Kjo3yyjDgbRtj4YLQgWV4QJ/content.json: 1574320071 < 1633348805\n[16:34:38] Site:1UDbAD..MkoV CheckModifications:     data/userdb/1AMUtxW7pjg4XgawDbAk297M1FfHWvSdSz/content.json: 1624056308 < 1631750222\n[16:34:38] Site:1UDbAD..MkoV CheckModifications:     data/userdb/1PyrAjbZiHyfW7jSWeUpDPSeMU7sAJ5HSs/content.json: 1620447763 < 1629301261\n[16:34:46] Site:1UDbAD..MkoV CheckModifications: Peer:205.201.19.11 of 1UDbAD..MkoV has older versions of 1 files\n[16:34:46] Site:1UDbAD..MkoV CheckModifications:     data/userdb/1HtM4FZiTesc4FynbSntKggoUYwVdkB6AK/content.json: 1610818112 < 1611091316\n[16:34:47] Site:1UDbAD..MkoV [OK] 205.201.19.11:26552: File not changed 1/1\n[16:34:52] Site:1UDbAD..MkoV [FAILED] 92.237.127.163:20903: None\n[16:34:52] Site:1UDbAD..MkoV [FAILED] 92.237.127.163:20903: None\n[16:34:52] Site:1UDbAD..MkoV [FAILED] 92.237.127.163:20903: None\n```",
			"added": 1635439267,
			"modified": 1635439307
		},
		{
			"topic_id": 1635440377,
			"title": "[bug] content-default.json problem",
			"body": "For some reason, ZeroNet is very lazy about `content-default.json` files. A site may be downloaded hours ago, but suddenly it turns out those files are missing:\n\n```\n[16:53:25] Site:14Zjt1..Qdfp CheckModifications: 1 new modified files from Peer:zfvvtwrc4wxi3ihulhhjnjniao2mu7mu7ldo7guf2ubhbgay7h7cx3qd.onion of 14Zjt1..Qdfp\n[16:53:25] Site:14Zjt1..Qdfp CheckModifications:     data-default/users/content-default.json: 1436226671 > 0\n```\n\nAs I noticed before, sometimes one have to press `Update` several times to persuade Zeronet finally download those files.",
			"added": 1635440363
		},
		{
			"topic_id": 1635452073,
			"title": "Problems of ZeroSites",
			"body": "Lots...\n\n* The peer count has not been updated for ages, should be disabled. Sort entries by the like count.\n* No way to see who submitted an entry.\n* No way to mute a user.\n* Long site descriptions are just truncated, no way to see the text.\n* The submission date is not displayed.\n* No way to search through DB.\n* No way to filter sites by user name.\n  * ...and show only your own sites as well.\n* No way to know your size limits.\n\nI guess all the issues can be fixed relatively easy. I haven't looked at the code yet, and from a distance everything looks easy.",
			"added": 1635452058,
			"modified": 1635481018
		},
		{
			"topic_id": 1635453498,
			"title": "[bug] \"Old-style\" sign verification",
			"body": "...was removed in 2019:\n\n* [Add faster libsecp256k1 support for sign verification, Remove old style signing support](https://github.com/HelloZeroNet/ZeroNet/commit/bc937967272a0206791aa219052e321d6b682068)\n* [We don't support old style sign verification anymore](https://github.com/HelloZeroNet/ZeroNet/commit/65d19d350c9851bdffdc5353c806c4e53821c75a)\n\nI believed the only site used it so far was ZeroBoard, which was broken and useless anyway. But I wandered through [OldZeroTalk](http://127.0.0.1:43110/1TaLk3zM7ZRskJvrh3ZNCDVGXvkJusPKQ/) and found this:\n\n* http://127.0.0.1:43110/1GamESVFyJfkmbxtcrLR2VX4m4VFmwyeRY\n\nThe site has 17 peers and doesn't load because:\n\n> [20:31:48] Site:1GamES..yeRY content.json: verify error: VerifyError: Invalid old-style sign in ContentManager.py line 1025 > 999\n\nWho knows how many more sites like that.\n\nIn my opinion, the decision of removing the old-style sign support contradicts the main ZeroNet idea: **be able to keep and share data forever as long as there are people interested in**.\n\nSo I should bring it back.\n\nThe limitation will be as follows:\n\n* Old-style signs are recognized only for files signed before Mar 2019.",
			"added": 1635453482,
			"modified": 1635481044
		},
		{
			"topic_id": 1635487571,
			"title": "ZeroForum",
			"body": "Just discovered this:\nhttp://127.0.0.1:43110/Me.ZeroNetwork.bit/?Post/12h51ug6CcntU2aiBjhP8Ns2e5VypbWWtv/1NWh3WAty57FH8at1WtrZigMrdhrDkuPzh/1631323453\n\nThe idea is nice. But why a completely new engine and data format? Wouldn't it be better to develop a solution that is compatible with existing engines? (ZeroTalk, ZeroMe?.. Millchan?..)",
			"added": 1635487554
		},
		{
			"topic_id": 1635871309,
			"title": "IPv6 trackers issue",
			"body": "> [styromaniac@kxoid.bit](http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1635637273_1PxcegJ1mKoMHeS73f9MvXeUWDq8BdnSv3):\n> ZeroNet bug: IPv6 (including Yggdrasil) trackers are only visible in TOR Always mode.\n> I can't see IPv6 trackers via ZeroHello on any other TOR mode. They're probably not connected either.\n\nProbably related to:\n\n```python\n    def isIpv6Supported(self):\n        if config.tor == \"always\":\n            return True\n        # Test if we can connect to ipv6 address\n        ipv6_testip = \"fcec:ae97:8902:d810:6c92:ec67:efb2:3ec5\"\n        try:\n            sock = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n            sock.connect((ipv6_testip, 80))\n            local_ipv6 = sock.getsockname()[0]\n            if local_ipv6 == \"::1\":\n                log.debug(\"IPv6 not supported, no local IPv6 address\")\n                return False\n            else:\n                log.debug(\"IPv6 supported on IP %s\" % local_ipv6)\n                return True\n        except socket.error as err:\n            log.warning(\"IPv6 not supported: %s\" % err)\n            return False\n        except Exception as err:\n            log.error(\"IPv6 check error: %s\" % err)\n            return False\n\n```",
			"added": 1635871291
		}
	],
	"topic_vote": {
		"1616711390_1CgdRv2BFaVqNfmK3qGVYDuA7EDmk8dJVS": 1
	},
	"next_comment_id": 23,
	"comment": {
		"1549014898_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 3,
				"body": "> [ssdifnskdjfnsdjk](#comment_1_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS): => auto increase users limit of published content (USED: x/20K) by lets say 1K every day to prevent reaching limit on admin abandoned sites in future\n\nThat's not technically possible.",
				"added": 1549447487
			},
			{
				"comment_id": 4,
				"body": "> [ssdifnskdjfnsdjk](#comment_1_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS): => downvotes \n\nI'm thinking about implementing this feature as well: https://github.com/HelloZeroNet/ZeroTalk/issues/20",
				"added": 1549447604
			},
			{
				"comment_id": 5,
				"body": "> [ssdifnskdjfnsdjk](#comment_1_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS): => auto increase users limit of published content (USED: x/20K) by lets say 1K every day to prevent reaching limit on admin abandoned sites in future\n\nPerhaps, this approach can be helpful:\nhttp://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1549416429_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm/\n\n> [geekless](#comment_30_1GooUE19488nDwG3TdkM8seYAHct4gjkq4): One of the drawbacks of the nofish's archiving approach is the need of the central server, that archives the data on schedule. The timestamp based hub system probably solves that issue.\n> \n> The other approach I think of is the combination of optional and non-optional files, that is managed by a user (probably, automatically, by the forum software, not manually). New posts and comments are kept in non-optional data.json and old ones are moved to a set of optional data-{timestamp}.json-s. The non-optional limit can be set relatively small, and the optional one can be large (many gigabytes?).\n> \n> A site visitor can set a limit, how old data files he/she wants to download in order to browse the old topics.\n> \n> That approach requires no server and no need to connect new hubs. Users manage the data by themselves in transparent way.",
				"added": 1549447874
			},
			{
				"comment_id": 7,
				"body": "http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:1597641256_1GVfTPghbWnBKbRWpYpkRCKgMi4wFecie2/New+Functions+on+ZeroNet",
				"added": 1597645592
			}
		],
		"1549168166_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 2,
				"body": "@kaffie:\n\nMay I add your ZeroMail message here in the topic for the context?",
				"added": 1549252967
			}
		],
		"1549045300_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 6,
				"body": "> [quantumworld](#comment_4_1AdWXZdPUimpXoGNXUBde2CHR7gLf4onxE): How do you keep the secondary key from overriding the primary key if there was a malicious attack from one of the parties that had the key?\n\nNot sure, what you mean. The secondary key can sign the data in the appropriate subfolder only. It cannot be used to sign `content.json` of the parent folder.",
				"added": 1549451508
			},
			{
				"comment_id": 8,
				"body": "> [gitcenter](#comment_1_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): There is no implementation available.\n\nhttp://127.0.0.1:43110/1DocsYf2tZVVMEMJFHiDsppmFicZCWkVv1/site_development/content_json/#signers_sign\n\nBut it was broken when I checked it the last time.\n\nNot sure if it works now.\n\nP.S.: And as I can see it's broken by design, since it lacks **the timestamp**.",
				"added": 1603519370
			}
		],
		"1616666284_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 9,
				"body": "> [xorgadget](#comment_1_1Nv7HrYvwPjyGyx6iUG7KZrj4TNduUBcah): Can we clone ZeroID?\n\nZeroID (and any ID provider) requires [the server side](https://github.com/HelloZeroNet/ZeroID/tree/master/publisher-http).\n\nMaybe it is more practical to run a clone of [kxoid](http://127.0.0.1:43110/1GTVetvjTEriCMzKzWSP9FahYoMPy6BG1P/?/kxoid). The server side in this case can be located at a laptop and hidden with Tor. No public IP needed.",
				"added": 1616667976
			},
			{
				"comment_id": 10,
				"body": "What about ZeroID, all the registered names will keep working. But the private key owner can sign any number of name duplicates, so \"geekless@zeroid.bit\" is not necessarily me any more, until you look at the person's public key behind the nick. `1GooUE19488nDwG3TdkM8seYAHct4gjkq4` in my case.",
				"added": 1616668228,
				"modified": 1616668326
			},
			{
				"comment_id": 12,
				"body": "> [xorgadget](#comment_4_1Nv7HrYvwPjyGyx6iUG7KZrj4TNduUBcah): Plugin is ok... so...\n\n...the server is down? %-)\n\n@gitcenter should know better. He worked together with @krixano.",
				"added": 1616670372
			},
			{
				"comment_id": 13,
				"body": "[@gitcenter's most recent activity](http://127.0.0.1:43110/1iNDExENNBsfHc6SKmy1HaeasHhm3RPcL/data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di/content.json) is Jan 02 2021.\n\nBy the way, it's totally possible for HelloZero to inform the user about the last actions of persons s/he subscribed to. (Within all the downloaded zites.) So ZeroMe's follow feature is not so useful. It can be implemented without ZeroMe profile. :-D And much more, one can follow any types of updates, such as likes or files being shared, not just posts.",
				"added": 1616672897
			},
			{
				"comment_id": 15,
				"body": ">  @gitcenter should know better.\n\nI dropped him an email yesterday, hope he'll visit us someday...",
				"added": 1617253803,
				"modified": 1617253816
			}
		],
		"1616711390_1CgdRv2BFaVqNfmK3qGVYDuA7EDmk8dJVS": [
			{
				"comment_id": 14,
				"body": "v3 seems working when opening a site by direct link `http://127.0.0.1:43110/<site>/?zeronet_peers=<v3-onion-address>:<port>`.\n\nBut trackers refuses accepting those addresses. So all the existing trackers are useless for v3 without upgrading.",
				"added": 1617176476
			}
		],
		"1616822056_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 16,
				"body": "Designing a new sidebar:\n\n![](http://127.0.0.1:43110/1CiXRY9ATZSoZqBzwMfXEMsKtPRt2aQoF2/data/users/1GooUE19488nDwG3TdkM8seYAHct4gjkq4/zeronet-new-sidebar.png)",
				"added": 1635342393
			}
		],
		"1635414903_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 17,
				"body": "> [musickiller](#comment_1_15NScazvsCHNmiXi6o6nvaMiwjeJYizgZQ):  As for implementation - can't we just add some PoW string to each post/comment\n\nIt depends on what is your goal.\n\nIf the goal is to prevent dispatching spam over the network, the answer is no. There are no posts and comments at the content sharing level. The bunch of all posts is transmitted as a whole.\n\nIf the goal is... uh... hide spam from the site's pages or something like that, it's totally possible. In that case, it should be implemented at the site engine level. In pure JS as a proof of concept at least, to find out what support for the implementation can be helpful on the side of ZeroNet core.\n\nI have no practical considerations about that so far.\n\n> I don't think that \"help others\" option is really needed. That will only encourage spammers to create multiple accounts, mine all at once.\n\nThey can mine all at once with no problem anyway. The only limit is available hardware capabilities. The feature is for ordinary users who can help each other in this way without need to set up a mining farm.",
				"added": 1635418065,
				"modified": 1635421837
			},
			{
				"comment_id": 18,
				"body": "> [musickiller](#comment_1_15NScazvsCHNmiXi6o6nvaMiwjeJYizgZQ): [...]\n\nWhat about per-post metadata, you reminded me to write a note about the post verification:\n\nhttp://127.0.0.1:43110/14Zjt1CcoLXs6xu3huy5sYoK4PRMC1Qdfp/?Topic:1635420651_1GooUE19488nDwG3TdkM8seYAHct4gjkq4/",
				"added": 1635421112,
				"modified": 1635421167
			}
		],
		"1635439280_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 19,
				"body": "```\n[16:45:52] Site:1N5h8H..MXgV CheckModifications: Peer:ytidto27pgvqlckr.onion of 1N5h8H..MXgV has older versions of 1 files\n[16:45:52] Site:1N5h8H..MXgV CheckModifications:     data/users/1GooUE19488nDwG3TdkM8seYAHct4gjkq4/content.json: 1635331146 < 1635408755\n[16:45:53] Site:1N5h8H..MXgV [OK] ytidto27pgvqlckr.onion:26552: File not changed 1/1\n```",
				"added": 1635439702
			}
		],
		"1635453498_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 20,
				"body": "Damn, I already did that, but threw away the patch because I considered it useless. Now I changed my mind and have to do the same work of porting pieces of old py2 code to py3 codebase.",
				"added": 1635453952
			}
		],
		"1635871309_1GooUE19488nDwG3TdkM8seYAHct4gjkq4": [
			{
				"comment_id": 21,
				"body": "@styromaniac, \n\nDo you have the opportunity to edit this function in `FileServer.py` and check if something changes?\n\nJust replace all `return False` with `return True`.",
				"added": 1635871470
			},
			{
				"comment_id": 22,
				"body": "We also seem to have multiple errors in the tracker connection logic. Right now I see \"Trackers: 4/61\" on ZeroHello, which barely means almost all the trackers are down. More likely some bug in connecting to them.",
				"added": 1635918779
			}
		]
	},
	"comment_vote": {
		"3_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS": 1
	}
}