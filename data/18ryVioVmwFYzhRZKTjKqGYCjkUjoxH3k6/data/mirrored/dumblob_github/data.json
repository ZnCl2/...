{"cert_user_id": "dumblob@github", "next_topic_id": 2, "topic": [{"topic_id": 1604065706, "title": "Important questions (security, performance, sustainability, ...)", "body": "I'm aware of the statement, that ZeroNet is **not** a replacement for the current client\\<-\\>server based model. But independently of it, I'd like to ask the following questions as an immediate reaction to an article about ZeroNet I read. I consider these questions very important for basically any to-be-successfull P2P project (they might find their place in FAQ or even directly in documentation).\r\n\r\n1. How is the locality of data ensured (in parallel to their non-local persistence to avoid their disappearance)? It's nonsensical to have everything everywhere (like torrent) and forever. Except for that, based on measurements of big companies (Google, Facebook, Twitter, etc.), most of the web is served to mobile or embedded devices and these usually employ just tiny and often volatile storage.\r\n\r\n1. Is it possible to locally dynamically filter data (e.g. someone travels and wants to be sure, that in the country she/he is currently, noone will find any persisted data which do not comply with the local legislation). At least it shall be possible to easily manually tag content which must **not** be persisted.\r\n\r\n1. Is the content securely and smartly (e.g. ZeroNet measures statistics about network congestion, considers constraints set up by the administrator - e.g. maximum size of the storage, maximum bandwidth used for up/down, etc.) cached to minimize overall pressure and to increase availability and stability? Or is it just a naive implementation of the torrent protocol (i.e. opening of thousands of connections one by one, and thus effectively DDoSing small SOHO devices with NATs, which count up to the vast majority of all leaf nodes of the internet network infrastructure)?\r\n\r\n1. Does it work stably and without any issues **absolutely without** ani existing trackers? This is closely related to the first question as uTP, PEX, DHT, Local Peer Discovery, etc. technologies for finding peers are easily blockable, which is also what happens in reality.\r\n\r\n1. Is it possible to download all content in logical blocks (e.g. script, image, HTML, ...) to allow prioritization, parallelism, partial downloads and partial viewing (including the wishes of the user - e.g. I want to block all pictures, so I will decide to not load them until I enter the \"album\" section of the web)? Or is it rather a \"git-like\" monolithic design, where everything is divided into same-sized blocks mutually absolutely unrelated and these are first then downloaded in parallel from different peers (which has by the way the consequence of long waiting times for the whole content to load without the possibility of at least a preview/shortened_version/smaller_version, and at the same time is highly prone to starvation as each block is requested only from one or just very few seeds which are of course blocked by state censorship)?\r\n\r\n1. How is data reuse ensured? And how about multicast data streaming? It's a core feature for big networks, streaming (audio, video, real-time games, ...) and things like `jQuery` `D3.js` `NODE.js` or `Video.js` rely on well-behaving and high-throughput CDNs.\r\n\r\n1. Is there any full-featured active scraper (possibly distributed across ZeroNet to avoid it's disappearance or unavailability in censored countries) of all the content in ZeroNet? This is an absolute must-have and a strong requirement for massive spreading of the network. An index engine using e.g. the Namecoin database might be a good start.\r\n\r\n1. What everything prevents direct use (i.e. without any architectural changes) of existing rich web applications (full of JS, server-client communication, etc.) in ZeroNet? Could you list all the points?\r\n\r\n1. How about fine-grain unlimited control over user access? Because all data are replicated, everybody can read them, but in case only part of them is public and the rest is specific for a certain group of people, how to make absolutely sure noone else can read it? Do I really need to encrypt the whole page/application (with it's whole history - ouch, I already feel the size pain) and build my own sign-in solution which will then decrypt only certain parts of the downloaded data based on the access-rules built in the page/application? How about the current bottle neck of one and only one site-owner managing the access rights (currently it seems only **full** `rw` access can be granted) to the site-owner's page/application (I want to make sure my forum will stay fully active and functional even after the state starts to censor my forum and the forum users)?\r\n\r\n1. How does one sets up a secure fully functional bidirectional (to and from the \"usual\" internet) gateway for the case I want to access ZeroNet from a device not having a ZeroNet client (which actually currently approaches 100% of all devices and will probably stay so for a long time)? Anywhere any very detailed howto?\r\n\r\n1. How does ZeroNet fights against peers and seeds appearing in different blacklists from the \"usual\" internet?", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1484575530, "modified": 1484638741, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/772", "source_type": "github"}], "next_comment_id": 4, "comment": {"1604065706_mirrored_dumblob_github": [{"comment_id": 1, "body": "Thanks for quick answers @HelloZeroNet .\r\n\r\n1. The question was rather about pruning and trying to avoid full duplication of the \"whole internet\" (all ever visited sites - i.e. thousands and more at least) and also the question asked about data locality (I don't want to download the content from someone from Europe if I'm sitting in Australia and few hundreds of Australians already visited the original web site from Europe).\r\n\r\n1. In many legislations, there are e.g. black lists of web sites with censored content or any other identifiers technically describing the censored content (so to answer your question - the \"who\" is the state). Should there be any trial, all the data will be either easily accessed (because of saved passwords) or decrypted (because of weak ciphering, some mistake or just because of access to enormous or breakthrough computing resources) or because of torture or whatever, it's way better to not have these data persisted at all. These countries include Germany (huge audio and video prohibition - just try any German VPN on youtube), Czech republic (online hazard games), USA (various...), Australia, China, Turkey, Sveden, ...\r\n\r\n1. Any plans or ways to improve it? Just react briefly as I know this fine-tuning is tedious and a lot of work.\r\n\r\n1. Great to hear there are some plans!\r\n\r\n1. Are all these logical pieces downloaded from the same source or from different ones? If from different ones, how is starvation prevented?\r\n\r\n1. It's not at all about having centralized data. It's about very smart redistribution of common data, so that it's locally (e.g. from the same country or state) very quickly accessible. This can be achieved through data flow monitoring in the whole network and automated redistribution, through some ranking, through smart caching, etc. Is anything for data reuse on the schedule?\r\n\r\n1. That's sad - noone will know about any content and noone will get interested except for those few with external strong motivation (usually money - i.e. black market).\r\n\r\n1. Any plans on providing such \"backend\" (I can imagine a faster variant of the universal Ethereum computing platform)?\r\n\r\n1. Any plans to extend it for more fine-grained control? Are actually already currently supported roles (i.e. groups of users having the same permissions)?\r\n\r\n1. Could this public gateway be easily protected with a password? I don't want everyone using my gateway :wink:.\r\n\r\n1. Ok, in case Tor is blocked (e.g. Syria, Turkey, etc.), is there any other way how to \"hide\" yourself?", "added": 1484594365, "modified": 1484594365, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/772#issuecomment-272974948", "source_type": "github"}, {"comment_id": 2, "body": "Ok, now it's clear to me what is the direction of ZeroNet. I must admit I'm a bit disappointed, but on the other hand I know very well how difficult it is to build a well-behaving P2P network serving for universal purposes.\r\n\r\nBy the way, could you please edit your last comment and add some foo strings to the points on which didn't comment as otherwise the numbering does not match :wink: (Markdown is number-agnostic)?", "added": 1484636939, "modified": 1484636939, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/772#issuecomment-273082248", "source_type": "github"}, {"comment_id": 3, "body": ">but I think we should search for use-cases where it could work instead of focusing on what is it not good for.\r\n\r\nOf course. I'll keep an eye on ZeroNet and it's use. Keep going!", "added": 1484638741, "modified": 1484638741, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/772#issuecomment-273100137", "source_type": "github"}]}}