{"cert_user_id": "geekless@github", "next_topic_id": 25, "topic": [{"topic_id": 1604066696, "title": "[Scalability] The proposal of the new policy of processing data/user/*/* files", "body": "ZeroNet has blacklists now, which allows us not seeing some unwanted persons, but doesn't help protecting agains massive automated attacks.\r\n\r\nOn the other hand, we can implement whitelists. If ZeroNet refuses to recieve and save the user data from users not in whitelist, that policy is totally resistant to spam. But it has a significant drawback: a new person has no chance to be whitelisted, since no one in the whole world recieves and sees her messages by default.\r\n\r\nSo we need some mix of the both possibilities.\r\n\r\nMy proposal is that we can have a 3-way White/Gray/Black classification, and the Gray area is dinamically adjusted depending on the disk space available and other conditions (such as an automatical spam detection).\r\n\r\nProbably we should also use 2 size limits per a site, not just one: the soft limit and the hard limit.\r\n\r\nHow it works:\r\n\r\n* **Blacklisted user IDs.** ZeroNet never saves data that is blacklisted on the local host.\r\n\r\n* **Whitelisted user IDs.** ZeroNet processes data from whilested users in the usual way.\r\n\r\n* **Any other (gray) user IDs.** ZeroNet receives the data and dispatches it to other hosts in the usual way, but if the site grows above the soft limit, some data can be deleted from the local host.\r\n\r\nWe should have some rule to decide, which data should be deleted and when it should happen. I have the following considerations:\r\n\r\n* All gray users are equal for us (until the spam detection is implemented), so we can choose them randomly.\r\n\r\n* But the recently modified data can potentially be more important, so we should keep it for some time. That means the pure random selection is not an option.\r\n\r\n* The algorithm must not give the same results on different hosts, or else Zeronet drops the same users from all the hosts at once.\r\n\r\nSo I propose the following algorithm:\r\n\r\n* ZeroNet generates a random number (X) for the local host and saves it localy.\r\n\r\n* The random number X is used in the combination with the user ID to evaluate the affinity factor. It can be `affinity = X xor user_id` or `affinity = sha(X xor user_id)` or so. The affinity factor is pretty random per user, but it remains unchanged over time, so we can rely on it.\r\n\r\n* For every gray user, the function is evaluated: `current_affinity = f(time_from_the_last_modification, affinity_factor)`. The details of f() is not important here, but the result of f() should be inversely proportional fo time_from_the_last_modification and directly proportional to affinity_factor.\r\n\r\n* When the soft size limit is exceeded, ZeroNet tries to delete the data with the lowest current_affinity first and proceeds until the site size gets back to the limit. Data with the high affinity are probably never will be deleted, which guards us from accidental erasing data of inactive users from whole the network.\r\n\r\nWith that mechanics, the new posts, comments, messages etc are always displayed, so we can read them and react (answer the post, like it, add the author to the white or black list and so on). If the site runs out of the disk limit, the older content gradually fades out. It looks quite natural, since if we didn't react to the content, we were probably not interested. And if we want to see the whole site, we can increase the limit at any time and Zeronet will redownload the data.\r\n\r\nThe sites should have an API to whitelist users automatically. For example, when you *follow a user* at ZeroMe, *like a post*, *press the Reply button*, the site code knows you are interested, and the user is whitelisted.\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1535733671, "modified": 1535733671, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1559", "source_type": "github"}, {"topic_id": 1604066851, "title": "Please allow the Percent sign as a valid file name character", "body": "Example URL with Percent-encoding: `somedomain/%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80.html`\r\nDisplayed in a browser as: `somedomain/\u041f\u0440\u0438\u043c\u0435\u0440.html`\r\n\r\nI was trying to make a ZeroNet mirror for some clearnet sites. Surprisingly, it is not generally possible, since ZeroNet doesn't allow using Percent-encoded URIs as valid file names. The Percent sign is missing from the list of allowed characters in `ContentManager.py`:\r\n\r\n```\r\n    def isValidRelativePath(self, relative_path):\r\n        if \"..\" in relative_path:\r\n            return False\r\n        elif len(relative_path) > 255:\r\n            return False\r\n        else:\r\n            return re.match(\"^[a-z\\[\\]\\(\\) A-Z0-9_@=\\.\\+-/]+$\", relative_path)\r\n\r\n    def sanitizePath(self, inner_path):\r\n        return re.sub(\"[^a-z\\[\\]\\(\\) A-Z0-9_@=\\.\\+-/]\", \"\", inner_path)\r\n```\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1538708756, "modified": 1563447382, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1678", "source_type": "github"}, {"topic_id": 1604066853, "title": "Zeroname plugin breaks plugin chaining in get() method", "body": "#### Steps to reproduce:\r\n\r\n  1. Copy Zeroname plugin under another name and replace the logic with one that resolves `dummydomain`.\r\n  2. Try opening `dummydomain`.\r\n  3. Try opening usual Namecoin domains.\r\n\r\n#### Observed Results:\r\n\r\n  * Depending on the plugin load order, either `dummydomain` or Namecoin domains don't resolve.\r\n\r\n#### Expected Results:\r\n\r\n  * Both domain resolving plugins should work in a chain, allowing to resolve both types of domains.\r\n\r\n#### Patch:\r\n\r\n```\r\ndiff --git a/plugins/Zeroname/SiteManagerPlugin.py b/plugins/Zeroname/SiteManagerPlugin.py\r\nindex 9691a32..f73ee0c 100644\r\n--- a/plugins/Zeroname/SiteManagerPlugin.py\r\n+++ b/plugins/Zeroname/SiteManagerPlugin.py\r\n@@ -76,8 +76,8 @@ class SiteManagerPlugin(object):\r\n                     if site_domain != address:\r\n                         site.settings[\"domain\"] = address\r\n             else:  # Domain not found\r\n-                site = self.sites.get(address)\r\n+                site = super(SiteManagerPlugin, self).get(address)\r\n \r\n         else:  # Access by site address\r\n-            site = self.sites.get(address)\r\n+            site = super(SiteManagerPlugin, self).get(address)\r\n         return site\r\n```", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1538732978, "modified": 1538999474, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1679", "source_type": "github"}, {"topic_id": 1604066859, "title": "Allow using other domain resolver plugins along with Zeroname", "body": "Fixes several closely related bugs in Zeroname Plugin, those prevent correct work of other domain resolvers:\r\n\r\n1. get(), isDomain(): when an address doesn't look like a valid domain, pass control to the next plugin in a chain.\r\n\r\n3. Move the .bit-domain matching logic to a separate method isBitDomain().\r\n\r\n2. get(), need(): use isBitDomain(), not isDomain() to check if domain resolving is needed in order to not interfere with other domain resolvers.\r\n\r\n4. Also rewrite isAddress() to make it look similar to the new implementation of isDomain().", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1538793451, "modified": 1563166671, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/1683", "source_type": "github"}, {"topic_id": 1604066868, "title": "fileQuery: does the implementation match the documentation?", "body": "[The documentation](https://github.com/HelloZeroNet/Documentation/blob/master/docs/site_development/zeroframe_api_reference.md) states:\r\n\r\n> ### fileQuery _dir_inner_path, query_\r\n> Simple json file query command\r\n> \r\n> Parameter            | Description\r\n>                  --- | ---\r\n> **dir_inner_path**   | Pattern of queried files\r\n> **query**            | Query command (optional)\r\n> \r\n> **Return**: <list> Matched content\r\n> \r\n> **Query examples:**\r\n> \r\n>  - `[\"data/users/*/data.json\", \"topics\"]`: Returns all topics node from all user files\r\n>  - `[\"data/users/*/data.json\", \"comments.1@2\"]`: Returns `user_data[\"comments\"][\"1@2\"]` value from all user files\r\n>  - `[\"data/users/*/data.json\", \"\"]`: Returns all data from users files\r\n>  - `[\"data/users/*/data.json\"]`: Returns all data from users files (same as above)\r\n> \r\n> **Example:**\r\n> ```coffeescript\r\n> @cmd \"fileQuery\", [\"data/users/*/data.json\", \"topics\"], (topics) =>\r\n> \ttopics.sort (a, b) -> # Sort by date\r\n> \t\treturn a.added - b.added\r\n> \tfor topic in topics\r\n> \t\t@log topic.topic_id, topic.inner_path, topic.title\r\n> ```\r\n\r\nSince there is no definition there, what the \"pattern of queried files\" is, it is logical to assume that the pattern works similar to the usual wildcard file pattern. So, if one queries \"folder/data.json\", she should get back exactly 1 result. In fact it is not so:\r\n\r\n```\r\nPage.cmd(\"fileQuery\", \"data/users/content.json\", function(r) {console.log(r.length);})\r\n7\r\n```\r\nThe result contains files from the whole `data/users/` subtree.\r\n\r\nIf the pattern has no `/*/` part, the implementation silently assumes it before the last part of the file path:\r\n\r\n```\r\n    if \"/*/\" in path_pattern:  # Wildcard search\r\n        root_dir, file_pattern = path_pattern.replace(\"\\\\\", \"/\").split(\"/*/\")\r\n    else:  # No wildcard\r\n        root_dir, file_pattern = re.match(\"(.*)/(.*?)$\", path_pattern.replace(\"\\\\\", \"/\")).groups()\r\n```\r\n\r\nIs it actually how it is supposed to work? What is wrong: the documentation or the implementation?\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1538924092, "modified": 1538997797, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1689", "source_type": "github"}, {"topic_id": 1604066875, "title": "The user-specific `max_size` is ignored when it is less than `max_size` from `permission_rules`", "body": "For example, `geekless@zeroid.bit` still has `max_size` of 10000 bytes:\r\n\r\n```json\r\n{\r\n\t\"user_contents\": {\r\n\t\t\"cert_signers\": {\r\n\t\t\t\"zeroid.bit\": [\r\n\t\t\t\t\"1iD5ZQJMNXu43w1qLB8sfdHVKppVMduGz\"\r\n\t\t\t]\r\n\t\t},\r\n\t\t\"permission_rules\": {\r\n\t\t\t\".*\": {\r\n\t\t\t\t\"files_allowed\": \"data.json\",\r\n\t\t\t\t\"max_size\": 10000\r\n\t\t\t}\r\n\t\t},\r\n\t\t\"permissions\": {\r\n\t\t\t\"geekless@zeroid.bit\": {\r\n\t\t\t\t\"max_size\": 5000\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nShouldn't per-user permissions always take the precedence over permission rules?", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1538982413, "modified": 1538997340, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1694", "source_type": "github"}, {"topic_id": 1604066879, "title": "ZeroID-based domain names", "body": "The current proof-of-concept implementation:\r\nhttps://github.com/geekless/ZeroNet/tree/IDName/plugins/IDName\r\n\r\n- [x] Domains `<id>.zeroid` are resolved with the ZeroID cert database.\r\n   - [ ] Cert sign verification is not yet implemented.\r\n   - [ ] Support for any ID provider can be added in the same way, if it exports a machine-readable database. But now it is only ZeroID, so no code for managing toplevel domains is present.\r\n\r\n- [x] Subdomains `<anything>.<id>.zeroid` are resolved with a table from `<id>.zeroid/content.json`\r\n   - [ ] No indication for long-running operations in UI yet. (Modification of the core Zeronet engine is probably needed.)\r\n\r\n- [ ] Domain cache is silly yet.\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1539066301, "modified": 1539692200, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696", "source_type": "github"}, {"topic_id": 1604067123, "title": "[patch] Make the site block check usable from other plugins and core modules", "body": "Please add an API to check if an address is blocked.\r\n\r\nThe patch adds method SiteManager.isAddressBlocked(), that can be overloaded by filter plugins to provide the actual implementation. It also adds the corresponding code to ContentFilterPlugin and moves a few lines from ContentFilterPlugin to ContentFilterStorage to avoid the code duplication.\r\n\r\n[isAddressBlocked.txt](https://github.com/HelloZeroNet/ZeroNet/files/2827088/isAddressBlocked.txt)\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1549258848, "modified": 1549258848, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1888", "source_type": "github"}, {"topic_id": 1604067346, "title": "Fix the order of commands in Dockerfile to make use of the caching", "body": "Fix the order of commands in Dockerfile to make use of the caching of intermediate Docker images.\r\n\r\nIn py2 version, `COPY . /root` was placed after `RUN apk ...`, so that the result of `RUN apk ...` can be cached by Docker.\r\n\r\nIn py3 version, the commands were reordered to make the file `/root/requirements.txt` available for `pip install`. That prevents caching, and the docker image every time is rebuild from scrach.\r\n\r\nTo enable the caching back again, we can `COPY` just the single file `requirements.txt` before running other commands. Since the file is unmodified most of the time, the resulting image can be effectively cached. The other ZeroNet files are copied after doing `RUN apk ...`, as in the previous version.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1562146548, "modified": 1563166564, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2066", "source_type": "github"}, {"topic_id": 1604067349, "title": "AnnounceShare rework", "body": "* Allow sharing trackers of any supported protocols, not just `zero://`. (Right now it includes BitTorrent over HTTP(S) and BitTorrent over UDP.)\r\n  * The supported protocols are autodetected, not hardcoded, so the modification of the plugin is not required in case of implementation of new protocols.\r\n* Working trackers are now counted on per protocol basis. The plugin tries to maintain the up-to-date list of functional trackers for each available protocol.\r\n* Be more conservative on removal trackers from the list. Don't delete trackers on errors if no successful connections have been established recently, in the case of the network connectivity issues.\r\n* Other small code refactorings, such as replacing magic constants with meaningful names.\r\n\r\nThe updated plugin is meant to be used as a general-purpose tracker exchange facility, not limited to sharing locally running autodetected `Boostrapper`s.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1562168624, "modified": 1562343576, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068", "source_type": "github"}, {"topic_id": 1604067362, "title": "Move the BitTorrent related code from SiteAnnouncer.py", "body": "Move the BitTorrent related code from SiteAnnouncer.py to a separate plugin.\r\n\r\nThe support of BitTorrent trackers can now be enabled/disabled by enabling/disabling the plugin.\r\n\r\nconfig.disable_udp is not in use by the core now, but I haven't moved its definition to the plugin. Since it's a general-purpose network option, it can be used in other modules in the future. For example, in a DHT implementation, or in a UDP-based trasport protocol.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1562469638, "modified": 1563166529, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2078", "source_type": "github"}, {"topic_id": 1604067372, "title": "Chrome is going to block interactive content in iframes", "body": "https://chromium-review.googlesource.com/c/chromium/src/+/1675916\r\n\r\nShould we reimplement the domain isolation via proxy, instead of the iframe-based solution?", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1562639185, "modified": 1562764702, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2086", "source_type": "github"}, {"topic_id": 1604067374, "title": "The domain naming scheme proposal", "body": "Some related comments and discussions:\r\n* https://github.com/HelloZeroNet/ZeroNet/issues/83#issuecomment-94332474\r\n* https://github.com/HelloZeroNet/ZeroNet/issues/83#issuecomment-489451550\r\n* https://github.com/HelloZeroNet/ZeroNet/issues/1696\r\n* https://github.com/HelloZeroNet/ZeroNet/issues/104\r\n* https://github.com/HelloZeroNet/ZeroNet/issues/2049\r\n\r\nThe idea:\r\n* ZeroNet site should be accessible with both `http://127.0.0.1:43110/...` and `http://.../` via a proxy.\r\n* When accessing zites with a proxy, the pseudo-TLD `.zeronet` is used:\r\n  `http://<address>.zeronet/`\r\n* When accessing zites with `http://127.0.0.1:43110/`, the `.zeronet` part can be ommitted and is ignored by the core if present.\r\n  These point to the same site:\r\n  * `http://127.0.0.1:43110/<address>` - the classic way.\r\n  * `http://127.0.0.1:43110/<address>.zeronet` - for similar look&feel to the proxied addresses.\r\n* The second-level domains are handled by plugins.\r\n\r\n* * *\r\n\r\nThe proposed second-level domain schemes:\r\n\r\n* **Namecoin domains**: `.bit.zeronet`\r\n  * Proxied: http://Talk.ZeroNetwork.bit.zeronet\r\n  * Non-proxied: http://127.0.0.1:43110/Talk.ZeroNetwork.bit\r\n\r\n* **Cert-based**, as proposed in https://github.com/HelloZeroNet/ZeroNet/issues/1696: `.user.zeronet`\r\n  * Proxied: http://geekless.zeroid.user.zeronet, http://geekless.kxoid.user.zeronet\r\n  * Non-proxied: http://127.0.0.1:43110/geekless.zeroid.user, http://127.0.0.1:43110/geekless.kxoid.user\r\n\r\n* **Opennic-based** (https://github.com/HelloZeroNet/ZeroNet/issues/104): `.opennic.zeronet`\r\n  * Proxied: http://example.libre.opennic.zeronet\r\n  * Non-proxied: http://127.0.0.1:43110/example.libre.opennic\r\n\r\n* **Onion-based** (the protocol proposal hasn't been published yet): `.onion.zeronet`\r\n  * Proxied: http://nd6bvylhggfd41g.onion.zeronet\r\n  * Non-proxied: http://127.0.0.1:43110/nd6bvylhggfd41g.onion\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1562650133, "modified": 1571674596, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087", "source_type": "github"}, {"topic_id": 1604067378, "title": "Fix a websocket connection issue when running in the proxy mode", "body": "Steps to reproduce:\r\n\r\n1. Run Zeronet with --ui_trans_proxy in a docker container or other isolated environment.\r\n2. Set 127.0.0.1:43110 as a HTTP Proxy in a browser.\r\n3. Navigate to http://talk.zeronetwork.bit/\r\n4. The site stops loading, since the websocket connection isn't functional.\r\n\r\nThe bug consists of 2 related issues:\r\n\r\n1. Connect to the proper address.\r\n\r\n   When running in the proxy mode, Zeronet sends its env[\"SERVER_NAME\"] and env[\"SERVER_PORT\"] as the websocket destination address in the wrappper HTML code (variable `server_url`). That looks wrong for me, since the values of those variables have nothing to do with the actual address accessible by the browser. ZeroNet can run in a container, the connection also can be tunneled over SSH and so on. In all those cases, the internal SERVER_NAME makes no sense as the destination at all.\r\n\r\n   The code for sending and handling `server_url` is deleted. The browser should connect to the websocket in the same way as it connects to the ZeroNet UI in general.\r\n\r\n2. Send the proper response for CONNECT requests.\r\n\r\n   A browser tries to connect to the correct address, but it still fails. A browser, when connecting via a HTTP proxy to a webdocket, uses the [HTTP tunneling protocol](https://en.wikipedia.org/wiki/HTTP_tunnel), and ZeroNet lacks support for it. To fix this, it's enoght to send the \"200 OK\" response on the CONNECT method request.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1562767735, "modified": 1562768776, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2089", "source_type": "github"}, {"topic_id": 1604067387, "title": "[py3] Bootstrapper doesn't work properly", "body": "All the peers always go under the last hash:\r\n\r\n```\r\n899a95274c3926a0f1f78107db2a031df8d47e1163cad122a7700a86e205ade2 (added: 2019-07-14 17:54:46, peers: 0)\r\n\r\n180c87574fe8ba38f875b0e191ebef42b5bcbd63309d1505f5bbe490612c02aa (added: 2019-07-14 17:54:46, peers: 0)\r\n\r\ncbf016172e859fbf1e4e829bce19131bd618c3b391d29c38ba59bf7e71676aba (added: 2019-07-14 17:54:46, peers: 0)\r\n\r\n983e3fdb5adbe4d545b87f416db3dfcf6b2c3fc748112e65f706e40cb25da28b (added: 2019-07-14 17:54:46, peers: 0)\r\n\r\n9d6385bda46b0290306b57088392973fa5d48c897d6680a51fd81bf2cc16912b (added: 2019-07-14 17:54:46, peers: 0)\r\n\r\n7d72273a9342ab7e83dc0ba6ff27cb4d77a14ceef0d3068a1e8cde5b28a746ab (added: 2019-07-14 17:54:46, peers: 0)\r\n\r\n46d9151ac42b7ad9a40d60c723dc4c3d1c3d273490dd761dd3c5a8db47bfaf6a (added: 2019-07-14 17:54:46, peers: 9)\r\n- onion fyfc7uvp3tx4hgu5 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion 7aa3um27odsdvsse 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion ou4yfzgsumny3dxb 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion pxj65m524fhimys4 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion ybyr6tmqz4up2buq 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion chiqu2zwihhodwuf 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion sklooftclad6idat 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion 3tl6lfm5cisd24tg 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n- onion bsbndbylbejj43mh 15441 added: 2019-07-15 03:03:03, announced: 2019-07-15 03:03:03\r\n```\r\n\r\nThe issue is in `updateHashCache()`:\r\n\r\n```diff\r\n     def updateHashCache(self):\r\n         res = self.execute(\"SELECT * FROM hash\")\r\n-        self.hash_ids = {str(row[\"hash\"]): row[\"hash_id\"] for row in res}\r\n+        self.hash_ids = {row[\"hash\"]: row[\"hash_id\"] for row in res}\r\n         self.log.debug(\"Loaded %s hash_ids\" % len(self.hash_ids))\r\n```\r\n\r\nKeys in `hash_ids` look like:\r\n\r\n```\r\n[06:57:03] Db:TrackerZero {\"b'\\\\xc0H-\\\\xb4z\\\\x1b\\\\x97\\\\x8a|\\\\xa5\\\\xa0\\\\xc1?\\\\xb3\\\\x9ec\\\\xff0\\\\x9fM\\\\t}\\\\xdf\\\\x1e\\\\x92\\\\xec\\\\xf8\\\\xda\\\\xe2q\\\\xa74'\": 1, ... \r\n```\r\n\r\nShould be:\r\n\r\n```\r\n[07:00:14] Db:TrackerZero {b'\\xc0H-\\xb4z\\x1b\\x97\\x8a|\\xa5\\xa0\\xc1?\\xb3\\x9ec\\xff0\\x9fM\\t}\\xdf\\x1e\\x92\\xec\\xf8\\xda\\xe2q\\xa74': 1, ...\r\n```\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1563163842, "modified": 1563366222, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2095", "source_type": "github"}, {"topic_id": 1604067390, "title": "Add missing @helper.encodeResponse", "body": "Add missing @helper.encodeResponse in StatsPlugin.py and BootstrapperPlugin.py", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1563166365, "modified": 1563170348, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2096", "source_type": "github"}, {"topic_id": 1604067862, "title": "Updates of cert-based content.json's may be lost", "body": "\r\n\r\nZeroNet version: rev 4462, from the official docker image\r\n\r\n### Describe the problem:\r\n\r\nOn a long-running ZeroNet instance, that periodically goes offline for long time intervals as well, I noticed, that some sites miss the content available on several fresh ZeroNet instances.\r\n\r\nI've looked through the source code, and as far as I could understand, here is the place where we prefer to save the network traffic and time at the risk of getting an incomplete update:\r\n\r\n```\r\n        if since is None:  # No since defined, download from last modification time-1day\r\n            since = self.settings.get(\"modified\", 60 * 60 * 24) - 60 * 60 * 24\r\n```\r\n\r\nSo, looks like we actually get incomplete updates. On long-running instances, those incomplete updates tend to accumulate, and the user silently misses an undetermined amount of the cert-based data, such as forum posts, comments, likes etc.\r\n\r\n#### Steps to reproduce:\r\n\r\nCommand: `zeroframe.cmd(\"siteUpdate\", [\"1TaLkFrMwvbNsooF4ioKAY9EuxTBTjipT\"])`\r\nResult: no files to update\r\n\r\nCommand: `zeroframe.cmd(\"siteUpdate\", [\"1TaLkFrMwvbNsooF4ioKAY9EuxTBTjipT\", false, 0])`\r\nResult: \"Updating: 70 left\"\r\n\r\nCommand: `zeroframe.cmd(\"siteUpdate\", [\"1Apr5ba6u9Nz6eFASmFrefGvyBKkM76QgE\"])`\r\nResult: no files to update\r\n\r\nCommand: `zeroframe.cmd(\"siteUpdate\", [\"1Apr5ba6u9Nz6eFASmFrefGvyBKkM76QgE\", false, 0])`\r\nResult: \"Updating: 160 left\"\r\n\r\nCommand: `zeroframe.cmd(\"siteUpdate\", [\"1SunAWK2VUT9GQK32MpwRfFPVgcBSJN9a\"])`\r\nResult: no files to update\r\n\r\nCommand: `zeroframe.cmd(\"siteUpdate\", [\"1SunAWK2VUT9GQK32MpwRfFPVgcBSJN9a\", false, 0])`\r\nResult: \"Updating: 70 left\"\r\n\r\nAnd so on.\r\n\r\n\r\nCurrently, I found no way to force the core to make the full update from UI.\r\n\r\n* ZeroHello -> site -> Update doesn't work. (Fetches no missing files.)\r\n* ZeroHello -> site -> Check files doesn't work either.\r\n\r\nThe only way to force the full update is to run `zeroframe.cmd(\"siteUpdate\", [\"address\", false, 0])` from the browser console.\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1584195619, "modified": 1598276853, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2476", "source_type": "github"}, {"topic_id": 1604067874, "title": "Allow opening the sidebar while content.json is not loaded", "body": "If one opens the sidebar of a site not being downloaded yet, the following error occurs:\r\n\r\n```\r\n  Internal error: KeyError('content.json',): 'content.json'\r\n  UiWebsocket.py line 79 > 235 > Sidebar/SidebarPlugin.py line 527 > 120 > ContentDbDict.py line 59\r\n```\r\n\r\nAlso, the sidebar is not visible.\r\n\r\nThis fixes the both issues.\r\n\r\nFor sites without peers, the only way to delete the site was to navigate to ZeroHellow, scroll the left panel to \"Connecting sites\", and delete the site from the list. Now those sites can be deleted from the sidebar.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1584450737, "modified": 1595250010, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2485", "source_type": "github"}, {"topic_id": 1604067884, "title": "Update Tor version?", "body": "Today I noticed 100% CPU consumption caused by the Tor process, which is the part of nofish/zeronet docker image.\r\nThe program restart fixes the issue, but it comes back again a few hours later.\r\nNo apparent connection with ZeroNet network load.\r\n\r\nThe Tor version is:\r\n\r\n```\r\nTor 0.3.3.12 (git-f3e21c27631ee406) running on Linux with Libevent 2.1.8-stable, OpenSSL LibreSSL 2.7.5, Zlib 1.2.11, Liblzma N/A, and Libzstd N/A.\r\n```\r\n\r\nThe high CPU load may possibly be related to:\r\n\r\n```\r\n  TROVE-2020-002 is a vulnerability affecting\r\n  all released Tor instances since 0.2.1.5-alpha. Using this\r\n  vulnerability, an attacker could cause Tor instances to consume a huge\r\n  amount of CPU, disrupting their operations for several seconds or\r\n  minutes. This attack could be launched by anybody against a relay, or\r\n  by a directory cache against any client that had connected to it. The\r\n  attacker could launch this attack as much as they wanted, thereby\r\n  disrupting service or creating patterns that could aid in traffic\r\n  analysis. This issue was found by OSS-Fuzz, and is also tracked\r\n  as CVE-2020-10592.\r\n```\r\n\r\nThe vulnerability was fixed in versions 0.3.5.10, 0.4.1.9, 0.4.2.7.\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1584960675, "modified": 1585034863, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2493", "source_type": "github"}, {"topic_id": 1604067886, "title": "Unhandled exception: FileNotFoundError", "body": "  * ZeroNet version: rev 4471\r\n  * Operating system: Linux\r\n\r\nWhen I open a site, where I have logged in with my ID, but haven't posted any messages yet, the following exception occurs:\r\n\r\n```\r\n[11:32:23] - Unhandled exception: FileNotFoundError: [Errno 2] No such file or directory: './data/1Vp5LH4wegCaqeB72yMw2jgNdVm4aR7ET/data/users/1GooUE19488nDwG3TdkM8seYAHct4gjkq4/content.json' in gevent/threadpool.py line 137 > SiteStorage.py line 372 > 258\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/gevent/threadpool.py\", line 137, in __run_task\r\n    thread_result.set(func(*args, **kwargs))\r\n  File \"/root/src/Site/SiteStorage.py\", line 372, in loadJson\r\n    with self.open(inner_path, \"r\", encoding=\"utf8\") as file:\r\n  File \"/root/src/Site/SiteStorage.py\", line 258, in open\r\n    return open(file_path, mode, **kwargs)\r\nFileNotFoundError: [Errno 2] No such file or directory: './data/1Vp5LH4wegCaqeB72yMw2jgNdVm4aR7ET/data/users/1GooUE19488nDwG3TdkM8seYAHct4gjkq4/content.json'\r\n```\r\n\r\nDoesn't seem to have any negative impact on the functioning of the site or ZeroNet. However I guess the missing file condition can be handled in more graceful way.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1585039482, "modified": 1585039482, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2494", "source_type": "github"}, {"topic_id": 1604067892, "title": "Where is the source code of the Windows ZeroNet package?", "body": "Is it possible to build the Windows version from sources?", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1585186279, "modified": 1585208009, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2498", "source_type": "github"}, {"topic_id": 1604068022, "title": "Tor often fails reconnecting to the network when the computer wakes up", "body": "  * ZeroNet version: rev4496\r\n  * Operating system: Linux\r\n  * Tor status: always\r\n\r\nIs there something we can do on the ZeroNet side? Force Tor reconnecting?\r\n\r\nA typical log:\r\n\r\n```\r\nAug 19 07:08:22.000 [warn] Giving up launching first hop of circuit to rendezvous point [scrubbed] for service bhcnlqbcxzw4pfvg.\r\nAug 19 07:08:22.000 [warn] Giving up launching first hop of circuit to rendezvous point [scrubbed] for service tl2ts7caihmuako5.\r\nAug 19 07:08:24.000 [warn] Giving up launching first hop of circuit to rendezvous point [scrubbed] for service fumtihogvaififs7.\r\nAug 19 07:22:02.000 [notice] Your system clock just jumped 817 seconds forward; assuming established circuits no longer work.\r\nAug 19 07:22:02.000 [notice] Tried for 838 seconds to get a connection to [scrubbed]:30355. Giving up. (waiting for circuit)\r\nAug 19 07:22:02.000 [notice] Tried for 838 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:22:02.000 [notice] Tried for 836 seconds to get a connection to [scrubbed]:80. Giving up. (waiting for circuit)\r\nAug 19 07:22:02.000 [notice] Tried for 823 seconds to get a connection to [scrubbed]:23836. Giving up. (waiting for circuit)\r\nAug 19 07:22:02.000 [notice] Tried for 823 seconds to get a connection to [scrubbed]:64586. Giving up. (waiting for circuit)\r\n[07:22:03] ConnServer Wakeup detected: time warp from 1597820877.609987 to 1597821723.8360925 (846.2261064052582 sleep seconds), acting like startup...\r\nAug 19 07:22:35.000 [notice] We'd like to launch a circuit to handle a connection, but we already have 32 general-purpose client circuits pending. Waiting until some finish. [15075 similar message(s) suppressed in last 600 seconds]\r\nAug 19 07:24:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:80. Giving up. (waiting for circuit)\r\nAug 19 07:24:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\r\nAug 19 07:26:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:80. Giving up.\r\nAug 19 07:26:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:443. Giving up.\r\n[07:26:03] ConnServer Server port opened ipv4: False, ipv6: False\r\nAug 19 07:27:44.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:10815. Giving up. (waiting for circuit)\r\nAug 19 07:27:44.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:23333. Giving up. (waiting for circuit)\r\nAug 19 07:27:44.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:10815. Giving up. (waiting for circuit)\r\nAug 19 07:28:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:16843. Giving up.\r\nAug 19 07:28:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:23333. Giving up.\r\nAug 19 07:28:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:10815. Giving up.\r\nAug 19 07:28:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:10815. Giving up.\r\nAug 19 07:28:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:23333. Giving up.\r\nAug 19 07:28:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:28:06.000 [notice] No circuits are opened. Relaxed timeout for circuit 91807 (a Hidden service client: Fetching HS descriptor 4-hop circuit in state doing handshakes with channel state open) to 150830ms. However, it appears the circuit has timed out anyway.\r\nAug 19 07:28:09.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15555. Giving up.\r\nAug 19 07:28:11.000 [warn] Failed to find node for hop #1 of our path. Discarding this circuit.\r\nAug 19 07:28:11.000 [warn] Error launching circuit to node [scrubbed] for service [scrubbed].\r\nAug 19 07:28:15.000 [warn] Guard tjwtechsvr3 ($45582F4CCEE4BBF79B44A4853FDD82908B08D4A8) is failing a very large amount of circuits. Most likely this means the Tor network is overloaded, but it could also mean an attack against you or potentially the guard itself. Success counts are 124/248. Use counts are 0/0. 237 circuits completed, 0 were unusable, 113 collapsed, and 242 timed out. For reference, your timeout cutoff is 151 seconds.\r\nAug 19 07:28:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up.\r\nAug 19 07:28:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:32737. Giving up.\r\nAug 19 07:28:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:33092. Giving up.\r\nAug 19 07:28:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:22490. Giving up.\r\nAug 19 07:28:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up.\r\nAug 19 07:28:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:28:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up.\r\nAug 19 07:28:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up.\r\nAug 19 07:28:29.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:35212. Giving up.\r\nAug 19 07:28:29.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up.\r\nAug 19 07:28:31.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:14714. Giving up.\r\nAug 19 07:28:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:19447. Giving up.\r\nAug 19 07:28:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:13974. Giving up.\r\nAug 19 07:28:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:16805. Giving up.\r\nAug 19 07:28:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:36130. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:28:45.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:18282. Giving up.\r\nAug 19 07:28:49.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:17541. Giving up.\r\nAug 19 07:28:49.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:12462. Giving up.\r\nAug 19 07:28:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15442. Giving up. (waiting for circuit)\r\nAug 19 07:28:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:11558. Giving up. (waiting for circuit)\r\nAug 19 07:29:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:24529. Giving up. (waiting for circuit)\r\nAug 19 07:29:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:28522. Giving up. (waiting for circuit)\r\nAug 19 07:29:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:29:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:35071. Giving up. (waiting for circuit)\r\nAug 19 07:29:09.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:51604. Giving up. (waiting for circuit)\r\nAug 19 07:29:09.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:29:11.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:37810. Giving up. (waiting for circuit)\r\nAug 19 07:29:11.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:23168. Giving up. (waiting for circuit)\r\nAug 19 07:29:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:29:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:29:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:29:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:29:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:24529. Giving up. (waiting for circuit)\r\nAug 19 07:29:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:17234. Giving up. (waiting for circuit)\r\nAug 19 07:29:29.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:22222. Giving up. (waiting for circuit)\r\nAug 19 07:29:31.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:29:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:24142. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:29:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:29:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:24529. Giving up. (waiting for circuit)\r\nAug 19 07:29:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:25563. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:29:45.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:18621. Giving up. (waiting for circuit)\r\nAug 19 07:29:45.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26917. Giving up. (waiting for circuit)\r\nAug 19 07:29:49.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:29:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:7300. Giving up. (waiting for circuit)\r\nAug 19 07:30:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:28419. Giving up. (waiting for circuit)\r\nAug 19 07:30:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:24529. Giving up. (waiting for circuit)\r\nAug 19 07:30:09.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15442. Giving up. (waiting for circuit)\r\nAug 19 07:30:09.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:09.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:11.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:11.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:11.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:17.000 [notice] Your network connection speed appears to have changed. Resetting timeout to 151s after 18 timeouts and 1000 buildtimes.\r\nAug 19 07:30:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:38443. Giving up. (waiting for circuit)\r\nAug 19 07:30:23.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:34264. Giving up. (waiting for circuit)\r\nAug 19 07:30:25.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:29.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:24142. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:29.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:18366. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:29.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:31.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:18366. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:31.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:43.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:45.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:45.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:45.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:30:49.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:25588. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:30:49.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:37867. Giving up. (waiting for circuit)\r\nAug 19 07:30:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:26552. Giving up. (waiting for circuit)\r\nAug 19 07:30:51.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:31:03.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:37867. Giving up.\r\nAug 19 07:31:05.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:15441. Giving up. (waiting for circuit)\r\nAug 19 07:31:11.000 [notice] Tried for 120 seconds to get a connection to [scrubbed]:25563. Giving up. (waiting for rendezvous desc)\r\nAug 19 07:32:34.000 [notice] We'd like to launch a circuit to handle a connection, but we already have 33 general-purpose client circuits pending. Waiting until some finish. [39053 similar message(s) suppressed in last 600 seconds]\r\nAug 19 07:33:11.000 [warn] Hidden service uzdq6mrgwhoamth7 exceeded launch limit with 10 intro points in the last 300 seconds. Intro circuit launches are limited to 10 per 300 seconds. [1089 similar message(s) suppressed in last 300 seconds]\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: circuit is doing handshakes\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:33:11.000 [warn]   Intro point 0 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn]   Intro point 1 at [scrubbed]: no circuit\r\nAug 19 07:33:11.000 [warn]   Intro point 2 at [scrubbed]: no circuit\r\nAug 19 07:34:51.000 [warn] Failed to find node for hop #1 of our path. Discarding this circuit.\r\nAug 19 07:36:47.000 [notice] Heartbeat: Tor's uptime is 1 day 23:59 hours, with 65 circuits open. I've sent 796.95 MB and received 731.86 MB.\r\nAug 19 07:36:47.000 [notice] Average packaged cell fullness: 74.386%. TLS write overhead: 3%\r\nAug 19 07:37:02.000 [notice] Your network connection speed appears to have changed. Resetting timeout to 302s after 18 timeouts and 48 buildtimes.\r\nAug 19 07:40:58.000 [warn] Failed to find node for hop #1 of our path. Discarding this circuit.\r\nAug 19 07:40:58.000 [warn] Error launching circuit to node [scrubbed] for service [scrubbed].\r\n[07:41:11] ConnServer Internet offline\r\nAug 19 07:41:15.000 [warn] Error launching circuit to node [scrubbed] for service [scrubbed].\r\nAug 19 07:41:15.000 [warn] Error launching circuit to node [scrubbed] for service [scrubbed].\r\nAug 19 07:41:16.000 [warn] Error launching circuit to node [scrubbed] for service [scrubbed].\r\nAug 19 07:41:16.000 [warn] Error launching circuit to node [scrubbed] for service [scrubbed].\r\nAug 19 07:42:34.000 [notice] We'd like to launch a circuit to handle a connection, but we already have 38 general-purpose client circuits pending. Waiting until some finish. [34943 similar message(s) suppressed in last 600 seconds]\r\nAug 19 07:43:13.000 [warn] Hidden service 53l46thboizj3ytn exceeded launch limit with 11 intro points in the last 300 seconds. Intro circuit launches are limited to 10 per 300 seconds. [125 similar message(s) suppressed in last 300 seconds]\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 3 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 4 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 3 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 4 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is doing handshakes\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is doing handshakes\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: no circuit\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: no circuit\r\nAug 19 07:43:13.000 [warn] Service configured in [EPHEMERAL]:\r\nAug 19 07:43:13.000 [warn]   Intro point 0 at [scrubbed]: circuit is waiting to see how other guards perform\r\nAug 19 07:43:13.000 [warn]   Intro point 1 at [scrubbed]: circuit is doing handshakes\r\nAug 19 07:43:13.000 [warn]   Intro point 2 at [scrubbed]: circuit is doing handshakes\r\n```\r\n\r\nFinally after a while:\r\n\r\n```\r\n[07:47:18] ConnServer Internet online\r\n```\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1597814081, "modified": 1600064036, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2607", "source_type": "github"}, {"topic_id": 1604068024, "title": "Should we rename all black/white lists to deny/allow lists?", "body": "\r\n\r\n```\r\n$ grep -r -E -i '(black|white)l' .\r\n./src/Connection/Connection.py:            self.server.peer_blacklist.append((handshake[\"target_ip\"], handshake[\"fileserver_port\"]))\r\n./src/Connection/ConnectionServer.py:        self.peer_blacklist = SiteManager.peer_blacklist\r\n./src/Connection/ConnectionServer.py:        self.whitelist = config.ip_local  # No flood protection on this ips\r\n./src/Connection/ConnectionServer.py:        if ip in self.ip_incoming and ip not in self.whitelist:\r\n./src/Connection/ConnectionServer.py:            if (ip, port) in self.peer_blacklist and not is_tracker_connection:\r\n./src/Connection/ConnectionServer.py:                raise Exception(\"This peer is blacklisted\")\r\n./src/Test/TestConnectionServer.py:        whitelist = file_server.whitelist  # Save for reset\r\n./src/Test/TestConnectionServer.py:        file_server.whitelist = []  # Disable 127.0.0.1 whitelist\r\n./src/Test/TestConnectionServer.py:        # Reset whitelist\r\n./src/Test/TestConnectionServer.py:        file_server.whitelist = whitelist\r\n./src/Site/Site.py:        self.peer_blacklist = SiteManager.peer_blacklist  # Ignore this peers (eg. myself)\r\n./src/Site/Site.py:            if (ip, port) in self.peer_blacklist:\r\n./src/Site/Site.py:                return False  # Ignore blacklist (eg. myself)\r\n./src/Site/SiteManager.py:    peer_blacklist = [(\"127.0.0.1\", config.fileserver_port), (\"::1\", config.fileserver_port)]\r\n./src/Site/SiteManager.py:    peer_blacklist = []\r\n./src/File/FileServer.py:                SiteManager.peer_blacklist.append((ip_external, self.port))  # Add myself to peer blacklist\r\n./src/File/FileServer.py:                SiteManager.peer_blacklist.append((res_ip[\"ip\"], self.port))\r\n./src/File/FileServer.py:                SiteManager.peer_blacklist.append((ip, self.port))\r\n./src/Ui/UiWebsocket.py:            whitelist = getattr(config, \"ui_restrict\", [])\r\n./src/Ui/UiWebsocket.py:            # binds to the Internet, no IP whitelist, no UiPassword, no Multiuser\r\n./src/Ui/UiWebsocket.py:            if (\"0.0.0.0\" == bind_ip or \"*\" == bind_ip) and (not whitelist):\r\n./src/Tor/TorManager.py:            SiteManager.peer_blacklist.append((onion_address + \".onion\", self.fileserver_port))\r\n./plugins/ContentFilter/ContentFilterStorage.py:        # Site blacklist renamed to site blocks\r\n./plugins/ContentFilter/ContentFilterStorage.py:        if \"site_blacklist\" in self.file_content:\r\n./plugins/ContentFilter/ContentFilterStorage.py:            self.file_content[\"siteblocks\"] = self.file_content[\"site_blacklist\"]\r\n./plugins/ContentFilter/ContentFilterStorage.py:            del self.file_content[\"site_blacklist\"]\r\n./plugins/ContentFilter/ContentFilterStorage.py:        self.include_filters = collections.defaultdict(set)  # Merged list of mutes and blacklists from all include\r\n./plugins/ContentFilter/ContentFilterPlugin.py:                \"Blacklisted site\", extra_headers, show_loadingscreen=False, script_nonce=script_nonce\r\n./plugins/Sidebar/media/Sidebar.coffee:\t\t\toptions = [\"Delete this site\", \"Blacklist\"]\r\n./plugins/Sidebar/media/Sidebar.coffee:\t\t\t\t@wrapper.displayPrompt \"Blacklist this site\", \"text\", \"Delete and Blacklist\", \"Reason\", (reason) =>\r\n./plugins/Sidebar/media/all.js:        options = [\"Delete this site\", \"Blacklist\"];\r\n./plugins/Sidebar/media/all.js:            return _this.wrapper.displayPrompt(\"Blacklist this site\", \"text\", \"Delete and Blacklist\", \"Reason\", function(reason) {\r\n./plugins/Sidebar/languages/pt-br.json:\t\"Blacklist\": \"Blacklist\",\r\n./plugins/Sidebar/languages/pt-br.json:\t\"Blacklist this site\": \"Blacklistar este site\",\r\n./plugins/Sidebar/languages/pt-br.json:\t\"Delete and Blacklist\": \"Deletar e blacklistar\",\r\n./plugins/Sidebar/languages/zh.json:\t\"Blacklist\": \"\u9ed1\u540d\u5355\",\r\n./plugins/Sidebar/languages/zh.json:\t\"Blacklist this site\": \"\u62c9\u9ed1\u6b64\u7ad9\u70b9\",\r\n./plugins/Sidebar/languages/zh.json:\t\"Delete and Blacklist\": \"\u5220\u9664\u5e76\u62c9\u9ed1\",\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:    whitelisted_client_ids = {}\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:    whitelisted_client_ids = whitelisted_client_ids\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:    def whitelistClientId(self, session_id=None):\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:        if client_id in self.whitelisted_client_ids:\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:            self.whitelisted_client_ids[client_id][\"updated\"] = time.time()\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:        self.whitelisted_client_ids[client_id] = {\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:                if session_id not in self.sessions and self.getClientId() not in self.whitelisted_client_ids:\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:                    # Invalid session id and not whitelisted ip: display login\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:                self.whitelistClientId()\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:                self.whitelistClientId(session_id)\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:       for client_id in list(self.whitelisted_client_ids):\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:            if self.whitelisted_client_ids[client_id][\"session_id\"] == session_id:\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:                del self.whitelisted_client_ids[client_id]\r\n./plugins/disabled-UiPassword/UiPasswordPlugin.py:        yield json.dumps(self.whitelisted_client_ids, indent=4)\r\n./CHANGELOG.md:- Local site blacklisting\r\n```", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1597917630, "modified": 1598312691, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2608", "source_type": "github"}, {"topic_id": 1604068027, "title": "Corrupted site DB", "body": "Linux, rev4496, the DockerHub image\r\n\r\nI got corrupted (or incomplete?) `zerotalk.db` on one of my ZeroNet installations while \u043epening http://127.0.0.1:43110/1KNMtbcaDhKXaU96MeU57BawDD9KX9xqkn\r\nThe site displays no data with that database.\r\n\r\nOn another installation the zite works properly.\r\nI also tried opening it with an clearnet gate (rev4496) and with the old py2 Windows version. Both are okay.\r\n\r\n[zerotalk-wrong-db.zip](https://github.com/HelloZeroNet/ZeroNet/files/5122892/zerotalk-wrong-db.zip)\r\n[zerotalk-db.zip](https://github.com/HelloZeroNet/ZeroNet/files/5122893/zerotalk-db.zip)\r\n\r\nI tried in various combinations:\r\n* Rebulding the database\r\n* Restarting ZeroNet\r\n* Checking the site from UI and forcing the full update from the browser console\r\n\r\nOther files in the site folder are the same on the both installations. Checking the two folders with `meld` says it's only zerotalk.db that differs.\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1598337877, "modified": 1598356945, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2610", "source_type": "github"}], "next_comment_id": 64, "comment": {"1604066851_mirrored_geekless_github": [{"comment_id": 1, "body": "@HelloZeroNet,\r\n\r\n> I'm not fan of utf8 filenames: it's harder to handle, debug and not compatible with every file systems\r\n\r\nIt is NOT utf8 filenames. It is good old plain ascii filenames, that are totally compatible with any file system. Can you tell me, what a file system is not compatible with the percent sign?\r\n", "added": 1538732091, "modified": 1538732091, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1678#issuecomment-427350432", "source_type": "github"}, {"comment_id": 2, "body": "@HelloZeroNet,\r\n\r\n> @geekless when you enter `somedomain/%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80.html` to the browser will send the exactly same request as you enter `somedomain/\u041f\u0440\u0438\u043c\u0435\u0440.html`\r\n\r\nWhen I enter `somedomain/\u041f\u0440\u0438\u043c\u0435\u0440.html` to the address bar, the browser converts it into `somedomain/%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80.html` and sends the request to the server. The server can just read the file `%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80.html` from the storage and send it back.\r\n\r\nDo you see any unicode characters in `%D0%9F%D1%80%D0%B8%D0%BC%D0%B5%D1%80.html`? I don't. For that reason, I cannot understand your utf8-related argument on closing the issue.\r\n\r\nThe persent encoding allows to display meaningful URLs in the address bar, while operating ascii file names in the back-end. It will greatly improve the usability of such zites as ZeroUp etc.\r\n\r\nWe also will be able just take a directory with lots of files with unicode names, and convert it into a zite by just automatically renaming files into the percent-encoded format. No metadata or JS code is needed with that approach.\r\n\r\nI have a library, 15 GB in size, that has unicode characters almost in every file name. Now I see no way to easily convert it into a zite. I should save file names into a separate list, then rename the files into something meaningless, and then generate `index.html` with lots of `<a href='meaningless_url'>meaningful file name</a>` lines.\r\n\r\nWith the percent-encoding, I can create a zite of that kind with a 5-line shell script, and still the file names remain useful and self-descriptive.", "added": 1538735221, "modified": 1538735221, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1678#issuecomment-427365601", "source_type": "github"}], "1604066853_mirrored_geekless_github": [{"comment_id": 3, "body": "Just noticed isDomain() is also a part of SiteManager API. So it should be chained too.\r\n\r\n\r\n```\r\ndiff --git a/plugins/Zeroname/SiteManagerPlugin.py b/plugins/Zeroname/SiteManagerPlugin.py\r\nindex 9691a32..de837bd 100644\r\n--- a/plugins/Zeroname/SiteManagerPlugin.py\r\n+++ b/plugins/Zeroname/SiteManagerPlugin.py\r\n@@ -30,7 +30,10 @@ class SiteManagerPlugin(object):\r\n \r\n     # Return: True if the address is domain\r\n     def isDomain(self, address):\r\n-        return re.match(\"(.*?)([A-Za-z0-9_-]+\\.bit)$\", address)\r\n+        is_domain = re.match(\"(.*?)([A-Za-z0-9_-]+\\.bit)$\", address)\r\n+        if not is_domain:\r\n+            is_domain = super(SiteManagerPlugin, self).isDomain(address)\r\n+        return is_domain\r\n \r\n     # Resolve domain\r\n     # Return: The address or None\r\n@@ -76,8 +79,8 @@ class SiteManagerPlugin(object):\r\n                     if site_domain != address:\r\n                         site.settings[\"domain\"] = address\r\n             else:  # Domain not found\r\n-                site = self.sites.get(address)\r\n+                site = super(SiteManagerPlugin, self).get(address)\r\n \r\n         else:  # Access by site address\r\n-            site = self.sites.get(address)\r\n+            site = super(SiteManagerPlugin, self).get(address)\r\n         return site\r\n```", "added": 1538739131, "modified": 1538739131, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1679#issuecomment-427386235", "source_type": "github"}], "1604066859_mirrored_geekless_github": [{"comment_id": 4, "body": "Not sure why we are discussing the IDName stuff in that pull request, but...\r\n\r\n@imachug:\r\n\r\n> Though, it will probably be kaffie@zeroid.bit, not kaffie.zeroid.bit because it'll be a subdomain, and not kaffie.zeroid because zeroid is not a valid cert provider.\r\n\r\nThe .bit suffix should be avoided, since it is related to the Namecoin address space. So `kaffie.zeroid.bit` is an impossible option. \r\n\r\n`kaffie@zeroid.bit` could be an option, hmmm... \r\n\r\nMy first thought was to use `.zeroid.bit` suffixes, but I gave up that idea, as it brings ambiguity to the domain space.\r\n\r\n> not kaffie.zeroid because zeroid is not a valid cert provider.\r\n\r\nThere is no direct connection between the provider name and the top-level domain name. It should be a subject of agreement. If you recognize `.zeroid` domain names as provided by that specific provider, and I do the same, we can refer to those domains in unambiguous way.\r\n\r\n@Thunder33345:\r\n\r\n> the using names as domain thing will be hardcoded to only support zeroid and nothing more regardless if registry is unique or not\r\n\r\nAny trusted ID provider, that exports its database in a machine-readable way, can be easily supported. I see no reasons why it shouldn't be done.\r\n\r\n> say i have thunder33345.zeroid. how would i \"point\" my username to a site??\r\n\r\nYour cert address **is** your site address. You don't have to do any additional \"pointing\".\r\n\r\n> but that means now if i want to visit test.thunder33345.zeroid, i also have to download my main site, effectively forcing users to download it to resolve the subdomains which probably isnt a good idea either\r\n\r\nDo you have any other ideas, how it should be done?", "added": 1539042490, "modified": 1539042800, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/1683#issuecomment-428041726", "source_type": "github"}, {"comment_id": 5, "body": "@imachug:\r\n\r\nTrying to recall, why I gave up `user@zeroid.bit` domains too.\r\n\r\nThe `.bit` suffix gives the false impression, that the domain belongs to the Namecoin space and can be misleading. It is better to distinguish domain types by their last part. It's more clear for human beings and it also simplifies the code.\r\n\r\nBy the way, the current implementation of Zeroname still isn't compatible with such domains, as it uses regexp `(.*?)([A-Za-z0-9_-]+\\.bit)$`.\r\n\r\n\r\nWhat is your opinion?", "added": 1539045091, "modified": 1539045091, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/1683#issuecomment-428047838", "source_type": "github"}], "1604066868_mirrored_geekless_github": [{"comment_id": 6, "body": "@HelloZeroNet:\r\n\r\n> It's not recommended to use this feature\r\n\r\nLooks like fileQuery is not appropriate for the case when the data should be read, modified and written back, anyway.\r\n\r\nRight now I'm using such kind of code:\r\n\r\n```coffee\r\n\t\tuser_content_json = null\r\n\t\tcontent_json = null\r\n\t\treturn $.when()\r\n\t\t.then => $.Deferred (d) =>\r\n\t\t\tapp.cmd \"fileGet\", [\"data/users/#{user_address}/content.json\"], (res) =>\r\n\t\t\t\tif res\r\n\t\t\t\t\ttry\r\n\t\t\t\t\t\tuser_content_json = JSON.parse(res)\r\n\t\t\t\t\tcatch\r\n\t\t\t\t\t\tuser_content_json = null\r\n\t\t\t\tif user_content_json\r\n\t\t\t\t\td.resolve()\r\n\t\t\t\telse\r\n\t\t\t\t\td.reject()\r\n\t\t.then => $.Deferred (d) =>\r\n\t\t\tapp.cmd \"fileGet\", [\"data/users/content.json\"], (res) =>\r\n\t\t\t\tif res\r\n\t\t\t\t\ttry\r\n\t\t\t\t\t\tcontent_json = JSON.parse(res)\r\n\t\t\t\t\tcatch\r\n\t\t\t\t\t\tcontent_json = null\r\n\t\t\t\tif content_json\r\n\t\t\t\t\td.resolve()\r\n\t\t\t\telse\r\n\t\t\t\t\td.reject()\r\n\t\t.then => $.Deferred (d) =>\r\n\t\t\t# processing\r\n\t\t\t...\r\n\t\t\t...\r\n\t\t\t...\r\n\t\t\td.resolve()\r\n\t\t.then => $.Deferred (d) =>\r\n\t\t\tcontent_json.modified = Time.timestamp()\r\n\t\t\tjson_raw = unescape(encodeURIComponent(JSON.stringify(content_json, undefined, '\\t')))\r\n\t\t\tapp.cmd \"fileWrite\", [\"data/users/content.json\", btoa(json_raw)], (res) =>\r\n\t\t\t\tif res == \"ok\"\r\n\t\t\t\t\td.resolve()\r\n\t\t\t\telse\r\n\t\t\t\t\td.reject()\r\n\t\t.promise()\r\n```\r\n\r\nNot sure if it covers all corner cases, though.\r\n", "added": 1538956975, "modified": 1538956975, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1689#issuecomment-427711560", "source_type": "github"}], "1604066879_mirrored_geekless_github": [{"comment_id": 7, "body": "Please continue here the discussion from PR #1683.\r\n@DaniellMesquita, @imachug, @Thunder33345 \r\n", "added": 1539066514, "modified": 1539066514, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428124455", "source_type": "github"}, {"comment_id": 8, "body": "@Thunder33345:\r\n\r\n> but i dont think the .id is perfect for services that dont ends with id, but again id should be good enough to indicate this is derived from ID registries\r\n\r\nThat would be nice, but we have a problem: `.id` is a TLD of Indonesia.\r\n\r\nAlthough we don't have to follow ICANN rules as well as they don't have to follow our ones too, it would be good to maintain some level of compatibility. Theoretically, DNS domains can be used in ZeroNet as well. (Issue #104)", "added": 1539067975, "modified": 1539067975, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428131807", "source_type": "github"}, {"comment_id": 9, "body": "@Thunder33345:\r\n\r\n> is it nessarily to visit my main domain to find the subdomain(which is listed in the contents.json)?\r\n\r\nSince the only thing we know from the ZeroID database is your Bitcoin address, we have no other option except loading the data from that address. In my implementation, the resolver reads the `domain_records` field of `/content.json` to resolve subdomains.\r\n\r\nThere can be another option \u2014 the centralized domain database containing user-provided records, that works in the same way as ZeroTalk, ZeroBlog, ZeroMe and so on. It has the same drawbacks as those engines have: can be easily DoSed, the data can be censored by the site owner, no way to easily migrate out if the site becomes unmaintained.", "added": 1539069193, "modified": 1539069278, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428137748", "source_type": "github"}, {"comment_id": 10, "body": "> i think crawling the root domain is an necessarily evil then\r\n\r\nIt's opposite for me.\r\n\r\nEven the ZeroID database itself is not necessary to resolve the domain, so adding one more centralized entry in the system looks like bad idea. What you need to resolve a certificate into site, is certificate itself. It can be resolved not only with my resolver, but, for example, in the front-end, by the means of JS site engine. So asking the domain owner for subdomains is the most simple and reliable option.\r\n\r\nIMO, ZeroTalk way to deal with user content dosn't scale in the long run, so I am against it being used in the domain system.", "added": 1539070812, "modified": 1539070852, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428144759", "source_type": "github"}, {"comment_id": 11, "body": "@DaniellMesquita:\r\n\r\nKxoID uses a different certificate registry format. We probably should ask Krixano to support the ZeroID-like one.", "added": 1539076327, "modified": 1539076327, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428168235", "source_type": "github"}, {"comment_id": 12, "body": "@krixano:\r\n\r\n> I'm wondering what I'd need to change for KxoId. I already add usernames, auth addresses, and signed certificates to the db.\r\n\r\nZeroID uses a key-value registry: http://127.0.0.1:43110/1iD5ZQJMNXu43w1qLB8sfdHVKppVMduGz/data/certs_1.json\r\n\r\nEither we should support 2 different formats in the resolver or both providers should use the same.", "added": 1539095724, "modified": 1539095724, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428282156", "source_type": "github"}, {"comment_id": 13, "body": "> zeroid's uses optional files.\r\n\r\nIt is because when registering a new id, the front-end doesn't need the full certificate database, it only checks if that id is already registered using the shortened list in `users*.json`.\r\n\r\nNo doubt I can add the support of kxoid (probably I'll write the code tomorrow, if I'll have some time), but having 2 different formats in the core of the domain system is really not nice. What if soon we'll have to add 3rd and 4th?..\r\n", "added": 1539096861, "modified": 1539096861, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1696#issuecomment-428288164", "source_type": "github"}], "1604067011_mirrored_DaniellMesquita_github": [{"comment_id": 14, "body": "I've been thinking about the bridging feature for long time, but I still have no good ideas on implementing it. ", "added": 1539096200, "modified": 1539096200, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1702#issuecomment-428284677", "source_type": "github"}], "1604067346_mirrored_filips123_github": [{"comment_id": 15, "body": "Oh, that's complicated...\r\n\r\nI guess, it is better to start the refactoring from several easier tasks. For example, moving the BitTorrent tracker related code into the separate plugin, in the same way as the support of zero:// trackers is incapsulated in a single plugin.", "added": 1562136145, "modified": 1562136145, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2063#issuecomment-508021912", "source_type": "github"}, {"comment_id": 16, "body": "@0polar\r\n\r\n> Custom protocol for file download and big file (not BitTorrent)\r\n\r\nHow is it possible to use BitTorrent for dynamic data?", "added": 1562494470, "modified": 1562494470, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2063#issuecomment-508998929", "source_type": "github"}, {"comment_id": 17, "body": "> Yes, BitTorrent can't proactively push metadata (content.json), but can be used to normally download files.\r\n\r\nMakes sense.\r\n\r\nI have an idea to make a plugin to download torrent files into the ZeroNet data folder. The data may be addressable by something like `http://127.0.0.1:43110/torrent/<info_hash>/<file_name>`.\r\n\r\n", "added": 1562500670, "modified": 1562500670, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2063#issuecomment-509006126", "source_type": "github"}], "1604067349_mirrored_geekless_github": [{"comment_id": 18, "body": "Further work, after merging the PR:\r\n\r\n* [TrackerList](https://github.com/geekless/ZeroNet/tree/TrackerList/plugins/disabled-TrackerList) is a plugin, that fetches a list of trackers from any external source (URL or local file path) and adds them into the AnnounceShare list. If the trackers are functional, AnnounceShare spreads them throughout the network to other peers. (The plugin is functional, but some features are missing and additional testing is required.)\r\n\r\n* Rewrite the interaction of AnnounceShare and Bootstrapper:\r\n  * Now: *AnnounceShare checks if Bootstrapper is running and if so, adds the IP addresses to the shared list*.\r\n  * Should be: *Bootstrapper reports the listened addresses to AnnounceShare, according to its own configuration*.\r\n\r\n* Add optional support of .onion addresses to Bootstrapper, including addresses persistent between program restarts.\r\n", "added": 1562168735, "modified": 1562168735, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068#issuecomment-508212690", "source_type": "github"}, {"comment_id": 19, "body": "> Sharing torrent trackers are disabled, because it requires more resources, than zero:// ones and does not support tor (onion) addresses.\r\n\r\nThe default config contains 6 torrent trackers and just 3 zero ones.\r\n\r\n> So I would avoid using them if possible.\r\n\r\nShouldn't it be up to a user to choose, which transport and tracker types s/he prefers to use?\r\n", "added": 1562241526, "modified": 1562241710, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068#issuecomment-508510987", "source_type": "github"}, {"comment_id": 20, "body": "I run a ZeroNet instance on a cheap VPS ($5/month), and it sends announces to 50+ trackers, most of them are torrent ones. (Actually, to 80+ trackers, but 30 are mostly not responding.) I cannot notice any slowdown in practice.\r\n\r\nWe can utilize the existing large torrent infrastructure for the benefit of ZeroNet network in this way. In my opinion, Zeronet should be able to transfer the data over any possible transports and networks, and it's up to users to choose ones that are more appropriate to their conditions.\r\n\r\nIn fact, the BitTorrent protocol cannot be disabled in the current implementation, since it's not moved to a plugin, but implemented in the core. The issue you talk about can be solved automatically if the code is moved to a plugin. Just disable the plugin and ZeroNet doesn't try to connect to any torrent trackers.", "added": 1562243381, "modified": 1562243381, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068#issuecomment-508519707", "source_type": "github"}, {"comment_id": 21, "body": "I modified the code, now it allows to set a separate limit value for each protocol.\r\nBy default, the limit is 5 for zero:// and 2 for any other protocol, so it now avoids using too many torrent trackers.", "added": 1562249123, "modified": 1562249123, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068#issuecomment-508540672", "source_type": "github"}, {"comment_id": 22, "body": "You update torrent trackers in the hardcoded list every time they get down, but you don't agree for users to be able to add torrent trackers by themselves, and the network to be able to switch those trackers automatically. What's wrong, really?\r\n\r\nI'm now experimenting with DHT, and I can say, it requires much more network queries per an entry, than a torrent tracker. Are you going to reject a DHT PR for the same network performance reason? Please tell me now, so I didn't waste my time in that case.", "added": 1562254734, "modified": 1562255031, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068#issuecomment-508555026", "source_type": "github"}, {"comment_id": 23, "body": "> Note that if peers are spread among too many trackers that would not be good\r\n\r\nSplitting peers over many trackers is a vulnerability, that is mitigated neither in the core, nor in the original plugin, nor in this pull request. The redesigned plugin adds nothing to the possibility of that kind of attack for splitting the network into unbound subnets. Actually, since the code in the PR has higher limits for trackers than the original code, and also adds additional shuffling of the received data, it is harder, nor easier to sucessfully attack the plugin.\r\n\r\n", "added": 1562343305, "modified": 1562343305, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2068#issuecomment-508842949", "source_type": "github"}], "1604067348_mirrored_madsci1016_github": [{"comment_id": 24, "body": "This should probably be reported to https://github.com/HelloZeroNet/ZeroHello/issues", "added": 1562396617, "modified": 1562396617, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2075#issuecomment-508913766", "source_type": "github"}], "1604067349_mirrored_Merith-TK_github": [{"comment_id": 25, "body": "You need your old users.json to put in place of the new one to recover the old ID.", "added": 1562379176, "modified": 1562379176, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2076#issuecomment-508898006", "source_type": "github"}, {"comment_id": 26, "body": "> In most cases, it's next to impossible to recover JSON, as it may be corrupted in several ways. So I don't think that making a \"make everything work\" button is possible.\r\n\r\nMaybe, some plugin for making an encrypted (with a user-supplied password) backup of the master seed? And for restoring it as well.", "added": 1562405806, "modified": 1562405806, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2076#issuecomment-508923410", "source_type": "github"}], "1604067362_mirrored_geekless_github": [{"comment_id": 27, "body": "> Looking good, but the Travis CI failed, because of the missing `from Debug import Debug`\r\n\r\nFixed.", "added": 1562641143, "modified": 1562641143, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2078#issuecomment-509500575", "source_type": "github"}], "1604067372_mirrored_geekless_github": [{"comment_id": 28, "body": "I guess they are smart enough to avoid blocking iframes located at 1) the same domain as the main page; 2) at 127.0.0.1. I quickly looked through the code in the commit and found no those kinds of checks. But maybe I missed something or it's handled somewhere else. So, let's see how will it be done in the release.\r\n\r\nBy the way, there was a prrof-of-concept SOCK5 proxy implementation for Zeronet somewhere, wasn't it? Or am I confusing something?", "added": 1562643326, "modified": 1562643326, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2086#issuecomment-509509476", "source_type": "github"}, {"comment_id": 29, "body": "> The iframe is not just for domain isolation, but also to display the wrapper that for example allows to display permission requests or the sidebar.\r\n\r\nIf iframes get blocked, it will cause serious usability issues for ZeroNet. The user will have to manually navigate to the Settings page every time a zite needs permissions or for signing the content.", "added": 1562652032, "modified": 1562652032, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2086#issuecomment-509556598", "source_type": "github"}, {"comment_id": 30, "body": "It can be partially solved, if we add an API to display a Sidebar-like panel, but without any real privileges to do dangerous operations. When the user presses a button on the panel, the browser is redirected to the actual admin page, where the confirmation is displayed and then the required operation is performed.\r\n\r\nWell, not an ideal solution, maybe there can be a better way.", "added": 1562652321, "modified": 1562652422, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2086#issuecomment-509558489", "source_type": "github"}, {"comment_id": 31, "body": "> This blocking of ads is again one of the Google decisions to make web better... \r\n\r\nBetter for Google itself, yes. Better for its own ad network.", "added": 1562736761, "modified": 1562736761, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2086#issuecomment-509965927", "source_type": "github"}, {"comment_id": 32, "body": "> And it's already supported: 4ffd642\r\n\r\nDoesn't seem to work as expected in the latest rev. Trying to debug...", "added": 1562736937, "modified": 1562736937, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2086#issuecomment-509967036", "source_type": "github"}], "1604067374_mirrored_geekless_github": [{"comment_id": 33, "body": "In my opinion, the protocol handler cannot be considered as a proper solution, since 1) the protocol used by browser is actually HTTP, 2) Zeronet also makes use of the ws:// protocol under the hood (websocket connections). 3) there can be other widely used protocols in the future, which will also be supported by ZeroNet. (As discussed in https://github.com/HelloZeroNet/ZeroNet/issues/83#issuecomment-489451550.)\r\nThe issue is about the name resolution, and the protocol has litle or nothing to do with the name resolution.\r\n\r\n`.zeronet` looks a bit overwhelmed, but `.zero` is already registered by ICANN. Despite the fact that ICANN can steal just any TLD name for its own purposes, I think we should do our best to avoid the name clash with the existing TLDs. At least, if the name conflict appears in the future, it will be possible to say that `.zeronet` is long in use and other applicants are playing foul game.\r\n\r\n", "added": 1562668713, "modified": 1562669169, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509645922", "source_type": "github"}, {"comment_id": 34, "body": "The problem of the proposed approach is that domain names are case-insensitive, while cryptographic hash addresses are case-sensitive. So `http://127.0.0.1:43110/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D/` turns into `http://1hello4uzjaletfx6nh3pmwfp3qbrbtf3d.zeronet/` which is totally wrong.\r\n\r\nSo, for those addresses, we need the hash part to be in the path after the domain name.\r\nThe other way is to switch to a case-insensitive encoding, but it breaks the compatibility with the Bitcoin address format as well as with most other widely used formats for public keys. (Except for .onion ones)\r\n\r\nWe can use something similar to `http://zero/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D/` or `http://zeronet/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D/`, and it looks as a working option as long as we are able to provide the domain isolation with iframes.\r\n\r\nIf the iframe support gets dropped by browsers some day, a case-insensitive encoding is the only option.", "added": 1562670875, "modified": 1562670907, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509660159", "source_type": "github"}, {"comment_id": 35, "body": ">  Also, can we register `.zeronet` TLD? \r\n\r\nWe should have $185000 and be some kind of legal entity (company or organization).", "added": 1562714238, "modified": 1562714238, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509880227", "source_type": "github"}, {"comment_id": 36, "body": "> > I hope there is some solution for turning URL into lowercase. If there's not, we would need to use some other solution.\r\n> \r\n> Simple & silly solution: Just ignore the address case. The probability of two addresses matching case-insensitive but not totally equal is small enough to ignore it.\r\n\r\nThat requires implementing the case-insensitive matching on the tracker side as well.", "added": 1562736649, "modified": 1562736649, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509965262", "source_type": "github"}, {"comment_id": 37, "body": "> offtopic: BTW, your idea about making a tracker to collect site list won't work, because ZeroNet uses the SHA1 of the address for announcing, not the address itself.\r\n\r\nAll of a sudden! On zero:// too?", "added": 1562737073, "modified": 1562737073, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509967920", "source_type": "github"}, {"comment_id": 38, "body": "> Not really. Just send the lower-cased address to trackers.\r\n\r\nSo, looks like we should send sha sums of the both case-sesitive and lowercased addresses.", "added": 1562737210, "modified": 1562737210, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509968713", "source_type": "github"}, {"comment_id": 39, "body": "> SHA1 is used for clearnet, SHA256 for zero://.\r\n\r\nThat's a great news, on the other side. So one have to know the actual address to visit a site. Trackers and DHT cannot be used to get access to new or private sites.\r\n\r\n> For backward compatibility?\r\n\r\nYes.", "added": 1562737499, "modified": 1562737499, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509970482", "source_type": "github"}, {"comment_id": 40, "body": "So, it becomes more and more like a workable solution.\r\n\r\nWhat' worrying me most now is the cert-based model.\r\n\r\nFirst, we have an optional `.zeronet` part to refer to the ZeroNet address space.\r\n\r\nSecond, we need a part to distinguish the cert addresses. I propose using `.user`, but it may be `.cert`, `.id` etc. (What's better?)\r\n\r\nThird, the cert proveder itself. Here the problems begin. Since the provider names have the dot inside (zeroid.bit, kxoid.bit etc), the dot cannot be used to split the user and the provider parts without additional considerations.\r\n\r\nWe can arbitrarily discard suffix `.bit` from the provider names, but that raises the question why? And how should we handle providers with other suffixes, if any?\r\n\r\nNext, where we should save the provider list. Should it be hardcoded, fetched from some site etc? Or we can just scan the network for widely used provider names and their public keys, and if there isn't any ambiguity or conflicts, use those names?\r\n\r\nIn this way, the plugin, implementing the addressing model, can automatically detect that `zeroid.bit` is a provider name, and the next part is a user name.\r\n\r\nThe latter looks good at the first glance, but I have some doubts about the security. Okay, lets say we have domain  `subdomain.geekless.zeroid.bit.user.zeronet` (looks scary lol). It is parsed as:\r\n * Subdomain `subdomain`, signed by:\r\n * User `geekless`, signed by:\r\n * Provider `zeroid.bit`.\r\n\r\nWhat if someone adds a new provider and names it `geekless.zeroid.bit`? Now the domain is parsed as:\r\n * User `subdomain`, signed by:\r\n * Provider `geekless.zeroid.bit`.\r\n\r\nAnd it poins to a different location.\r\n\r\nWe should check for provider names, that looks like a subdomain of other providers and most likely block the domain resolution for them. On the other hand, someone can use the provider name `bit`, which blocks the genuine providers.\r\n", "added": 1562741515, "modified": 1562741608, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-509994287", "source_type": "github"}, {"comment_id": 41, "body": "`subdomain.geekless@zeroid.bit` still points to `zeroid.bit`, as it's parsed on the browser side. It's not possible to force a browser sending a HTTP query with a host name containing `@`.", "added": 1562742583, "modified": 1562743897, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510000395", "source_type": "github"}, {"comment_id": 42, "body": "The both conflict with the Namecoin address space.", "added": 1562743572, "modified": 1562743572, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510005336", "source_type": "github"}, {"comment_id": 43, "body": "> We have `-` however, which is (probably?) disallowed in ZeroNet hostnames. \r\n\r\nAllowed. For example, http://127.0.0.1:43110/zerome-reloaded.bit\r\n\r\n", "added": 1562743794, "modified": 1562743794, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510006377", "source_type": "github"}, {"comment_id": 44, "body": "> is zeroid.bit a certificate provider? yes, stop here.\r\n\r\nYeah, but my point is that the plugin should scan and detect cert providers automatically. (We can just scan `data/users/content.json` on all the downloaded sites.) So it has no internal knowledge of what is `zeroid.bit` and what is `bit`.\r\n\r\nSo, what should we do? I guess, if the plugin detects a conflict, it should just stop and report the issue to the user. The user have to manually blocklist `bit` as a fraud provider.\r\n\r\nIn this case, it's the same solution as for spammers and sites with inappropriate content -- blocklists. Sites, trying to fake a provider identity, should be considered as inappropriate and included in some blocklist for that reason.\r\n\r\nThinking about it, it looks like a good solution for me now.  No one should have power to dictate terms to ZeroNet users. The ZeroNet distribution may have some reasonable defaults, but it's up to user to finally decide, what is trustful and what is fraud.", "added": 1562745382, "modified": 1562745506, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510013742", "source_type": "github"}, {"comment_id": 45, "body": "Cert provider keys have nothing to do with Namecoin domains. They are declared in `data/users/content.json`:\r\n\r\n```\r\n  \"cert_signers\": {\r\n   \"kxoid.bit\": [\"12F5SvxoPR128aiudte78h8pY7mobroG6V\"],\r\n   \"zeroid.bit\": [\"1iD5ZQJMNXu43w1qLB8sfdHVKppVMduGz\"]\r\n  },\r\n```\r\n\r\nThe Namecoin domain may expire or point to another address. Namecoin itself may go down or be contolled by a single miner etc. Additionally, ZeroName as a domain cache is a single point of failure by itself.\r\n\r\nI don't think temporary conditions of that kind should have any influence on a sound name resolution system.", "added": 1562746256, "modified": 1562746256, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510017671", "source_type": "github"}, {"comment_id": 46, "body": "> Ok. So, what's the best solution then? Crawl all user content.json's to gather a cert_user_id <-> address mapping?\r\n\r\nAs the last resort, yes. For zeroid.bit, the database is located at the address of the provider key, so we can access it directly.\r\n\r\n12F5SvxoPR128aiudte78h8pY7mobroG6V (kxoid.bit) is just an empty site now, redirecting to another location. I don't know what is a proper way to access its database, if any.\r\n\r\nWe should develop a uniform protocol for accessing the provider DB in the future.", "added": 1562747722, "modified": 1562747722, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510024613", "source_type": "github"}, {"comment_id": 47, "body": "zero:// can be used to redirect the link to an OS-level handler to run a ZeroNet instance, which then opens the (appropriately modified) link in a browser.\r\n\r\nBut that goes beyond the scope of this proposal.", "added": 1562755765, "modified": 1562755765, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2087#issuecomment-510069103", "source_type": "github"}], "1604067378_mirrored_geekless_github": [{"comment_id": 48, "body": "We still have some bugs here.\r\n\r\n`UiRequest` doesn't allow what it should:\r\n\r\n> http://zero/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D/ returns \"Forbidden\"\r\n\r\nAnd allows what it shouldn't:\r\n\r\n> CONNECT accepts just any connection. So Firefox tryes to open SSL connections to *.firefox.com and sends encripted garbage there.\r\n\r\nBut this should be fixed in a different pull request, I guess, as it affects other parts of `UiRequest`.", "added": 1562768308, "modified": 1562768776, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2089#issuecomment-510151409", "source_type": "github"}], "1604067418_mirrored_blurHY_github": [{"comment_id": 49, "body": "Theoretically, it's possible to tunnel the traffic over Tox.", "added": 1562840657, "modified": 1562840657, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2090#issuecomment-510483737", "source_type": "github"}, {"comment_id": 50, "body": "A great advantage of the current Zeronet implementation and architecture is that it's plain and simple, so it can be relatively easily reimplemented in any language, on top of any OS-level API. This potentially makes ZeroNet as much immortal as HTTP protocol itself. There are lots of compatible HTTP server implementations, in the same way I hope there will be lots of ZeroNet ones.\r\n\r\nI guess, it's not likely that ZeroNet can run directly on top of IPFS, as their data operating models aren't compatible. At the same time, IPFS can be used as a transport layer, as well as Tox, I2P etc.\r\n\r\nProbably, the code needs some refactoring for make creating transport layer implementations easier. Unfortunately, no one has engaged in this yet.\r\n", "added": 1562855937, "modified": 1562856032, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2090#issuecomment-510582842", "source_type": "github"}], "1604067608_mirrored_ZeroNetTickBot_github": [{"comment_id": 51, "body": "GPLv3+", "added": 1582452347, "modified": 1582452347, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2273#issuecomment-590066844", "source_type": "github"}], "1604067862_mirrored_geekless_github": [{"comment_id": 52, "body": "Bump.\r\n\r\nWhen a run `zeroframe.cmd(\"siteUpdate\", [\"1TaLkFrMwvbNsooF4ioKAY9EuxTBTjipT\", false, 0])` multiple times, I get the same +58 files to be updated. May or may not be related to this issue.\r\nMaybe there are two independent bugs there? Something wrong with timestamps?\r\n", "added": 1597919881, "modified": 1597919881, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2476#issuecomment-677669889", "source_type": "github"}, {"comment_id": 53, "body": "\r\nUpdates:\r\n\r\n```\r\n[13:39:58] Site:1TaLkF..jipT CheckModifications: 58 new modified file from Peer:144.91.66.2  of 1TaLkF..jipT\r\n\r\n[13:39:58] Site:1TaLkF..jipT New downloadContent pool: len: 58, only if bad: True\r\n\r\n[13:40:00] WorkerManager:1TaLkF..jipT Tasks: 37, started: 16, bad files: 37, total started: 58\r\n\r\n[13:40:02] Site:1TaLkF..jipT Ended downloadContent pool len: 58, skipped: 0\r\n[13:40:02] Site:1TaLkF..jipT Ended downloadContent pool len: 1, skipped: 0\r\n\r\n[13:41:20] Site:1TaLkF..jipT CheckModifications: 58 new modified file from Peer:51.254.123.155 of 1TaLkF..jipT\r\n[13:41:20] Site:1TaLkF..jipT New downloadContent pool: len: 58, only if bad: True\r\n\r\n[13:41:31] Site:1TaLkF..jipT Ended downloadContent pool len: 58, skipped: 0\r\n[13:41:31] Site:1TaLkF..jipT Ended downloadContent pool len: 46, skipped: 0\r\n\r\n[13:41:38] Site:1TaLkF..jipT CheckModifications: 58 new modified file from Peer:51.254.123.155 of 1TaLkF..jipT\r\n[13:41:38] Site:1TaLkF..jipT New downloadContent pool: len: 58, only if bad: True\r\n\r\n[13:41:49] Site:1TaLkF..jipT Ended downloadContent pool len: 58, skipped: 0\r\n```\r\n\r\nOne of the files:\r\n\r\n```\r\n[13:40:01] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n\r\n[13:41:28] Site:1TaLkF..jipT Bad files: {'data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json': 2, 'data/users/1PyXS9BpytiuWuoscnwwR3AfF7kxjWCJZF/content.json': 2, ....\r\n\r\n[13:41:29] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n[13:41:29] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n\r\n[13:41:42] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n\r\n[13:41:49] Site:1TaLkF..jipT Bad files: {'data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json': 2, 'data/users/1PyXS9BpytiuWuoscnwwR3AfF7kxjWCJZF/content.json': 2, ....\r\n\r\n[13:42:05] Site:1TaLkF..jipT Bad files: {'data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json': 2, 'data/users/1PyXS9BpytiuWuoscnwwR3AfF7kxjWCJZF/content.json': 2, ....\r\n\r\n[13:42:09] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n[13:42:09] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n[13:42:09] Site:1TaLkF..jipT data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json loadContent same json file, skipping\r\n```\r\n\r\n\"loadContent same json file, skipping\", but the file getto the bad files list over and over. Something wrong in content.db?\r\n\r\ncontent.db:\r\n\r\n```\r\nsqlite> select content.* from content inner join site on site.site_id = content.site_id where site.address = '1TaLkFrMwvbNsooF4ioKAY9EuxTBTjipT' and inner_path like '%12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN%';\r\n75588|51|data/users/12Hdq1xCKHHYUJHxBWbvU6Eg1XoSgyHAMN/content.json|604|840|0|1530116227\r\n```\r\n\r\nDoesn't match the timestamp inside the file:\r\n\r\n```\r\n \"modified\": 1530181780,\r\n```\r\n", "added": 1597923025, "modified": 1597923025, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2476#issuecomment-677701335", "source_type": "github"}], "1604067952_mirrored_imachug_github": [{"comment_id": 54, "body": "The bug seems to be introduced in https://github.com/HelloZeroNet/ZeroNet/commit/d569d9488a521379c57e7e580089c95e2b2e3ba7\r\nAlso, no tests for that specific case have been added.\r\n\r\nAFAIK, the unix-like `.blabla-config` files have been working on all Windows versions and are used by a lot of ported software.\r\n\r\nThe docs:\r\n\r\n> https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file\r\n> Do not end a file or directory name with a space or a period. Although the underlying file system may support such names, the Windows shell and user interface does not. However, it is acceptable to specify a period as the first character of a name. For example, \".temp\".", "added": 1584434621, "modified": 1584434621, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2484#issuecomment-600025944", "source_type": "github"}, {"comment_id": 55, "body": "Seems fixed.", "added": 1584962596, "modified": 1584962596, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2484#issuecomment-602627409", "source_type": "github"}], "1604067884_mirrored_geekless_github": [{"comment_id": 56, "body": "Thanks!\r\n\r\nUsing the updated version for several hours and haven't noticed any troubles yet.", "added": 1585029332, "modified": 1585029332, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2493#issuecomment-603112543", "source_type": "github"}], "1604067995_mirrored_mx5kevin_github": [{"comment_id": 57, "body": "> Users cannot seed the files and zites 24/7\r\n\r\nThat's probably only true for really big files.\r\n\r\nI tried opening abandoned blogs listed in an abandoned catalog, and [all 67 zites opened](http://127.0.0.1:43110/1BLoGBTid3NhGu8ts3fAfHJprnbrH3wfTV/?Post:20). For the last 4 ones, it took some time, but finally peers went online too.\r\n\r\nBut I totally agree, the support of Freenet storage would be nice. Probably, [the content addressable storage](https://github.com/HelloZeroNet/ZeroNet/issues/2589) should be implemented first in order to get a proper integration with Freenet.\r\n\r\nBut ZeroNet currently lacks manpower for those big pieces of work, so this suggestion makes sense if only it's you who are ready to write the code.", "added": 1597818019, "modified": 1597818226, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2589#issuecomment-676013673", "source_type": "github"}], "1604068013_mirrored_mx5kevin_github": [{"comment_id": 58, "body": "There seems to be several unrelated topics here. Better to make a separate issue for each.\r\n\r\n> -4.Large file download in single user zites: Download all large files automatically with the website. For simple users don\u2019t have to worry about why the content (like larger fideo or etc files) isn\u2019t available and why not. Advanced users should have a button if they do not want to download larger files on the site then this can be turned off at the touch of a button.\r\n\r\nIt can be implemented on a site engine level, and in more sophisticated way than just simple downloading all the data.\r\n\r\nFor example, when you visit my blog, it starts downloading all the videos visible on the page.\r\nSome users complained it's the wrong strategy and I shouldn't use their drive as a CDN without their consent.\r\n", "added": 1597912949, "modified": 1597912949, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2602#issuecomment-677578918", "source_type": "github"}, {"comment_id": 59, "body": "@slrslr \r\n> +1, see [this zerotalk topic](http://127.0.0.1:43110/1TaLkFrMwvbNsooF4ioKAY9EuxTBTjipT/?Topic:1571214527_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS)\r\n\r\nThere are two sides of the issue.\r\n\r\nFirst, if ZeroNet works poorly on downloading big files, we should fix it. ZeroNet should resume downloading after restart, after computer wake up, when new peers go online etc.\r\n\r\nSecond, the culture. When a person publish a torrent, she knows she should stay online until several peers finish the download. And when we are in ZeroNet, we are probably used to the fact that everything works as a magic. Everything except big files. So we need some culture of sharing files in the same way as in BitTorrent.", "added": 1597913966, "modified": 1597913966, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2602#issuecomment-677601031", "source_type": "github"}], "1604068097_mirrored_slrslr_github": [{"comment_id": 60, "body": "@HelloZeroNet\r\n\r\n> Which is weird, because the document says \"The VACUUM command works by creating a temporary file and then rebuilding the entire database into that temporary file. Then the content of the temporary file is copied back into the original database file and the temporary file is deleted. \", so I would expect it to be 1 fragment.\r\n\r\nWell, if it copies \" the content of the temporary file\" as stated (and not the file itself), no surprise there stay many fragments.\r\n", "added": 1597916255, "modified": 1597916255, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2604#issuecomment-677639015", "source_type": "github"}, {"comment_id": 61, "body": "As a dirty solution we can:\r\n\r\n1. Export the DB as SQL.\r\n2. Create a new DB.\r\n3. Import SQL.\r\n4. Delete the old DB.\r\n5. Rename the new DB.\r\n\r\nThe question is when and how we should do it in order not to be too laggy for the user at an unpredictable moment in time.", "added": 1597916621, "modified": 1597916621, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2604#issuecomment-677641767", "source_type": "github"}, {"comment_id": 62, "body": "One more hack: https://stackoverflow.com/questions/827010/sqlite-pre-allocating-database-size", "added": 1597916898, "modified": 1597916898, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2604#issuecomment-677643902", "source_type": "github"}], "1604068022_mirrored_geekless_github": [{"comment_id": 63, "body": "@ihateiframes\r\n\r\nI run tor in a docker container alongside ZeroNet. It is how the official ZeroNet docker image works by default.\r\n\r\nSo if tor actually needs restarting every time (which should be considered as a bug in tor, in my opinion), it have to be implemented on ZeroNet side.\r\n", "added": 1600064035, "modified": 1600064035, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2607#issuecomment-691929086", "source_type": "github"}]}}