{"cert_user_id": "antilibrary@github", "next_comment_id": 25, "comment": {"1604064736_mirrored_HelloZeroNet_github": [{"comment_id": 1, "body": "Having a native implementation of IPFS makes complete sense for me as well.\r\nWe would need to allow support for files packs, for example, instead of having each individual file available for redistribution we could have a 'folders' like approach: all images, video course XYZ, all video courses of category ABC, books from id 1 to 100, all books in my shelves, music album DEF, all classical music albums, etc.\r\nThe packs could be defined by the site owner with the list of IPFS hashs (or a query that leads to this list), once a pack is selected to be seeded by the user, it will be downloaded and seeded through the local ipfs daemon.\r\nThe current implementation UI is hard to use because nobody will select individual files from the lists. The user is not interested in seeding specific files, he is interested in helping the site by providing a bit of bandwidth and space. \r\nFor a site like 0chan, the site owner could pack the different categories and users can then seed the whole category (eg: seed all files from /dev/). For a site like ZeroWiki, the same thing applies, the site owner could split the seeding in sections like: seed all images, seed all pages, seed history of all pages, OR he could use different wiki categories like: seed all content of category ABC.\r\nOne of the benefits of having it in IPFS is that the files are also available to the opennet via the IPFS gateway. So in the case of sites like antilibrary.bit, once users start to seed the book packs, the book files will be available for everyone to download via Zeronet gateways + ipfs gateway (eg: https://bit.no.com:43110/Antilibrary.bit/ ).", "added": 1487492779, "modified": 1487492779, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/7#issuecomment-280912790", "source_type": "github"}, {"comment_id": 2, "body": "We could benefit from this: https://ipfs.io/blog/23-js-ipfs-0-23/", "added": 1490549171, "modified": 1490549171, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/7#issuecomment-289312306", "source_type": "github"}, {"comment_id": 3, "body": "Couldn't a native implementation of IPFS be of help?\n\n-------- Original Message --------\nOn 5 Aug 2017, 16:38, ZeroNet wrote:\n\n> Same goal, but torrent files using it's own non-standard encoding (bencode) and outdated, not secure anymore sha1 encryption.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, [view it on GitHub](https://github.com/HelloZeroNet/ZeroNet/issues/7#issuecomment-320450457), or [mute the thread](https://github.com/notifications/unsubscribe-auth/AXMdbv8Lxdti56dDXIL-ZAWM3H8J_D9uks5sVIyFgaJpZM4DRs1k).", "added": 1501943879, "modified": 1501943879, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/7#issuecomment-320457229", "source_type": "github"}, {"comment_id": 4, "body": "@japzone1 @HelloZeroNet IPFS not only already has good support for big files but they have a whole team dedicated to improving the project. If we use IPFS we can reap all the benefits of their development.\r\nOn the points raised:\r\n\r\n- They do think of [supporting Tor](https://github.com/ipfs/notes/issues/37), but then again, may not be a good idea, and if users want to share big files they should think about using VPN or something else.\r\n- There are efforts to make [IPFS portable](https://github.com/ligi/IPFSDroid)\r\n- On their code of conduct, because of their architecture, there is just so much they can do to avoid copyright infringements (eg: blacklisting a hash on their gateway - one could create a gateway without the blacklist; blacklist a nodeId on their nodes - one could have his own network of nodes and clients could still connect to them). \r\n\r\nMy general feeling is that by reinventing the well on this one we may be creating more work for ZeroNet devs (a whole new part of the system will need to be maintained) and we are isolating ourselves by not being 'compatible' with anything else. For example, if you store big files on IPFS, the site owner could decide to have many interfaces of his site to allow users to get those files, his ZeroNet site could be just *one* of the interfaces, the others could be on the tor network, ipfs itself, or even clear net. \r\n", "added": 1503204915, "modified": 1503204915, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/7#issuecomment-323569974", "source_type": "github"}], "1604064889_mirrored_HelloZeroNet_github": [{"comment_id": 5, "body": "@HelloZeroNet any plans to implement this?\r\nI ask because if you say that this is not in the roadmap for the next months I'll probably implement it on the site level.\r\nThanks\r\n", "added": 1482077508, "modified": 1482077508, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-267839793", "source_type": "github"}, {"comment_id": 6, "body": "@HelloZeroNet thanks for that. I have just implemented compression (zlib) of one column of the site database as well. \ud83d\ude04 \r\nFor my use case I'm looking forward for the DB compression because that's what will be updated daily (unlike the site static files which are downloaded only once). In my compression implementation I've left all columns which I use to query uncompressed. I wonder how compressing them will impact on the query time.\r\nAlso, given that I'm compressing just one column (book description) I'm aware that this should not affect the diff syncing of the json files between nodes. I wonder how this will be affected once the whole json is compressed and only one row is updated/added  to that json.\r\nThanks \r\n", "added": 1487490557, "modified": 1487490557, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-280910888", "source_type": "github"}, {"comment_id": 7, "body": "I fail to understand the use case for archiving user files. What would it be for? \r\nIn my mind the whole purpose of allowing user content is to have it available for other users to benefit from it. \r\nTake a forum for example, if a question is answered it should be always available (potentially through search). \r\nIn sites with user content, usually the user content itself is the main value provided by the site. Imagine if stack overflow archived user answers. \r\nAlso, the down side of the feature is that, from what I understood, there will be no way for the user to know that the content he may be looking for exists but it is archived. \r\n\r\nI'm looking forward to the database compression feature \u263a\ufe0f", "added": 1488623132, "modified": 1488623132, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-284151339", "source_type": "github"}, {"comment_id": 8, "body": "IMHO this may lead to site owners using the feature incorrectly and archiving useful content. Sites with old content communicate maturity of the network.\r\nGiven that we are talking about text content the archive feature may be the easiest solution for the problem but maybe not the right solution. The right solution would be to improve the architecture of the network in such a way that it becomes highly effective in compressing and transferring content at speed (eg: transferring only diff of the json files to all nodes instead of only the nodes that are online). Allowing users to 'remove' content (by making it optional) will only mask the underlying issue, which should be optimising the network.\r\nIt is better to bet on making the network more efficient at transferring content than implementing features that will remove content from the network. Even if the site owner thinks that this is a good idea for his site, the users that took their time to create the content will dislike the fact that the site owner took the content the user created 'down'. And they may not abandon the site, but the network altogether. \r\nText is highly compressible and in years to come connections will continue to get faster and hard drives bigger. Even these days, downloading big files is a common thing for users. The upside of saving some 50MB by allowing the site owner to make content optional is lower than the downside of having user generated content (which took time and energy) removed from the site.", "added": 1488625019, "modified": 1488625019, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-284152943", "source_type": "github"}, {"comment_id": 9, "body": "You talk about large sites as a problem that needs solution. Have you heard of any site that had to delete and start again because it was too successful and accumulated too much user content?\r\nIf you are referring to ZeroTalk I think 8MB is only a problem if the content is useless. If this is the case, instead of allowing archival of the content we could work feature that would increase the quality of the content (eg: up/down voting) in such a way that users will prefer to store and help distribute large sites instead of small ones.\r\nI agree with merging multiple small json files into a big one. That definitely makes sense.\r\nSo maybe the archive feature can become an 'optimize site' feature where zeronet will compress and merge the json files in the most efficient way for that site. \r\nI just think that making it easy to transform text content in optional will result in a net loss for the network. I would resort to this as a last resort feature to deal with big sites.\r\nAlso, maybe my definition of big is different. For me big (when talking about text) means 200MB+. Which, if we're talking about user generated content, is a nice problem to have.", "added": 1488628499, "modified": 1488628499, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-284156309", "source_type": "github"}, {"comment_id": 10, "body": "But from what I got, these 10 minutes would decrease if you merge the files. And that should probably be enough.\r\nI'm all for removing bad content from the network (eg: spam, trolling, etc), but creating an archive feature as you propose may make it too easy to archive useful content, and despite the site owner best intentions, the network will be worse by the lack of content (we need to keep in mind that downloading optional files will be an advanced user skill given that it already requires an understanding of how the network works, which cannot be expected from new users).\r\nAlso, an initial wait time to download the site is a good price to pay given the benefits of having the site available on your phone offline (and all other benefits of having a site on zeronet instead of the internet). \r\nWe cannot compete in speed with normal internet sites, and we should not. We should invest in features that exploit what makes zeronet sites different from internet sites.", "added": 1488630722, "modified": 1488630722, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-284158581", "source_type": "github"}, {"comment_id": 11, "body": "Yeah, that makes sense.\r\nI understand the use case for the feature now. Sounds like a good idea. ;)", "added": 1488636716, "modified": 1488636716, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-284165609", "source_type": "github"}, {"comment_id": 12, "body": "I must say though that this sounds very much like an overlapping feature with the merger sites. Or maybe I'm using merger sites incorrectly.", "added": 1488638060, "modified": 1488638060, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-284167102", "source_type": "github"}, {"comment_id": 13, "body": "Are database files zip supported now?", "added": 1502199544, "modified": 1502199544, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-321012149", "source_type": "github"}, {"comment_id": 14, "body": "#1053 is related to one use case where the database may have outdated content. I'm referring to the use case where the db won't have outdated content and all the site owner wants is to zip the json files to speed up download. \r\n", "added": 1502200739, "modified": 1502200739, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/98#issuecomment-321017733", "source_type": "github"}], "1604065509_mirrored_ghost_github": [{"comment_id": 15, "body": "The merger site issue is now affecting Antilibrary's German books database because that DB is now bigger than the default allowed size.\r\nAgain, the problem happens when one tries to add a merger site which is bigger than the default allowed size. It will fail silently to the user and show the message `Site too large...` on the console.\r\nA proper solution would be to inform the user about the size of the merger site and ask if he wants to proceed.\r\n", "added": 1490889562, "modified": 1490889562, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/598#issuecomment-290511325", "source_type": "github"}, {"comment_id": 16, "body": "I don't think so as the issue is still affecting merger sites.\r\nIf you try to add a merger site bigger than the default size it will fail silently. You do get the notification asking to add the merger site, but it doesn't add as the error happens in the background and there is no notification asking if the user wants to increase the merger site size.", "added": 1502816083, "modified": 1502816083, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/598#issuecomment-322570912", "source_type": "github"}], "1604065560_mirrored_ghost_github": [{"comment_id": 17, "body": "I think you can close this issue alright.", "added": 1480689527, "modified": 1480689527, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/636#issuecomment-264513888", "source_type": "github"}], "1604065601_mirrored_antilibrary_github": [{"comment_id": 18, "body": "[This is still happening](http://127.0.0.1:43110/Me.ZeroNetwork.bit/?Post/1ABFCaadiEa3bFNzN3RcCZSsviqMNTbz3m/1GNgaj5EqkzqYECYbXa889fGLyuseHRi3t/1492069252) :(\r\n", "added": 1492143886, "modified": 1492143886, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/689#issuecomment-294106726", "source_type": "github"}], "1604065662_mirrored_antilibrary_github": [{"comment_id": 19, "body": "@HelloZeroNet As a work around for this, is there a way to exit the zeronet process once a given site has been updated? I need to run it as part of a sequence of commands and all I need from zeronet is for one site to be updated and it can exit, so the next commands can use the updated json files for that site.\r\n", "added": 1505050846, "modified": 1505050865, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/737#issuecomment-328354843", "source_type": "github"}], "1604066042_mirrored_grinapo_github": [{"comment_id": 20, "body": "I'm getting the same error when trying to post to my ZeroMe:\r\n\r\n```\r\n[21:14:21] Site:1MeFqF..q7nH Signing: data/users/1KN1Au7SRmeTmffcxxoyPABDpvqmq7i\r\nMbN/data.json\r\n[21:14:21] Site:1RedkC..uFdL data/users/1KN1Au7SRmeTmffcxxoyPABDpvqmq7iMbN/conte\r\nnt.json parse error: IntegrityError: foreign key constraint failed in ContentMan\r\nager.py line 222 > ContentDbDict.py line 61 > ContentDbPlugin.py line 203 > Cont\r\nentDb.py line 97 > Db.py line 87 > DbCursor.py line 80 > DbCursor.py line 49\r\n[21:14:21] Site:1RedkC..uFdL Opening site data directory: data/1RedkCkVaXuVXrqCM\r\npoXQS29bwaqsuFdL/data/users/1KN1Au7SRmeTmffcxxoyPABDpvqmq7iMbN/...\r\n```", "added": 1500916667, "modified": 1500916667, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1040#issuecomment-317541817", "source_type": "github"}], "1604066049_mirrored_antilibrary_github": [{"comment_id": 21, "body": "@HelloZeroNet sorry to trouble you with this one but, can you give me any ideas? [Some new users are having a hard time because of this](https://www.reddit.com/r/antilibrary/comments/6pe56q/no_books_found/) and I have no idea what file can be holding the wrong pattern :(\r\nThanks", "added": 1501036552, "modified": 1501036552, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1043#issuecomment-317952696", "source_type": "github"}, {"comment_id": 22, "body": "That's great. Thanks!", "added": 1501208320, "modified": 1501208320, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1043#issuecomment-318562286", "source_type": "github"}], "1604066319_mirrored_antilibrary_github": [{"comment_id": 23, "body": "Awesome. I've created [this PR](https://github.com/HelloZeroNet/Documentation/pull/79) to avoid this questions again.\r\nThanks", "added": 1516763752, "modified": 1516763752, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1257#issuecomment-360031821", "source_type": "github"}], "1604066327_mirrored_antilibrary_github": [{"comment_id": 24, "body": "I'm not sure if I understand how to use this.\r\nI've added to the zite and I grant the permission but when the page reloads the zite is still inside an iframe. I'm on rev 3354.\r\nThe same happens with the zite you provided as example.\r\nShouldn't it load the zite html directly instead of using the iframe?\r\nThanks", "added": 1520875920, "modified": 1520876152, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1262#issuecomment-372452441", "source_type": "github"}]}, "next_topic_id": 6, "topic": [{"topic_id": 1604065601, "title": "File write error: undefined", "body": "The command `Page.cmd \"fileWrite\"` is showing a UI message error with the text: `File write error: undefined` but there is nothing in the debug logs nor console.\r\n\r\n**To Reproduce:**\r\n\r\n1 - Download the site Antilibrary.bit and add the spanish DB (just because it is the smallest one)\r\n2 - Import this CSV ([goodreads_library_export.txt](https://github.com/HelloZeroNet/ZeroNet/files/627852/goodreads_library_export.txt)) on the Goodreads CSV Importer in Antilibrary.bit (inside My Books)\r\n\r\nThe file will import and you should get this error after 2 or 3 seconds.\r\nI can't find a way to debug this problem because it seems that this error is showing just in the UI.\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1480689960, "modified": 1492143886, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/689", "source_type": "github"}, {"topic_id": 1604065662, "title": "Seeding mode", "body": "It would be nice to have a command line parameter to run zeronet.py in a mode that it will just download a zite (provided in the command line) and keep it up to date, nothing else. This would be useful to make the zite files available to other apps.\r\nExample: `python zeronet.py --seeding-mode zite_hash`\r\n\r\n**Use case:**\r\n\r\nThis would allow us to run zeronet programmatically to keep a given zite downloaded and up to date so that the json files from that zite can be used by a different script.\r\n\r\nAntilibrary.bit has a use case of adding books requested by the users. \r\nWhen a user requests a new book that is not in the main zite DB, the book ISBN gets added to the user data.json file in the merger db [userdb](http://127.0.0.1:43110/19sZZmJPz1hADwitEs2XT6U46uHLGbHqKi/).\r\nThere is a docker container that runs the script that adds the new books. This script will scan the userdb folder for new books requested. Currently I need to have a local instance of zeronet running with the userdb zite added to it, and then I will share the files with the docker container. This requires manual intervention because there is no way to start zeronet in such a way that it will auto-download a zite.\r\nIf there was a way to run zeronet in this seeding mode I could automate the deploy of this script to run 2 containers, one with zeronet in the seeding mode for the userdb and the other with the script. No manual intervention would be necessary.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1483554753, "modified": 1510566504, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/737", "source_type": "github"}, {"topic_id": 1604066049, "title": "'Potentially unsafe part of the pattern' error not descriptive enough", "body": "When I try to add a books database (merger db) to the site antilibrary.bit on a fresh 0.5.7 zeronet install I get the error `ERROR Db:Antilibrary Potentially unsafe part of the pattern: ]+` on the console and no json files from the merger site are loaded into the database.\r\nThis is happening if you try to add any book database to the site (go to My Books, Settings).\r\n\r\nThe problem is, the error is not descriptive of where the issue is. I've searched the content.json and dbschema.json for the pattern `]+` but found nothing.\r\n\r\nThe error should at least point to the file that contains the unsafe pattern.\r\n\r\nI'm still unable to fix the issue because I don't know where the problem is.", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1500862953, "modified": 1502957354, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1043", "source_type": "github"}, {"topic_id": 1604066319, "title": "No console window on Windows", "body": "### Step 1: Please describe your environment\r\n\r\n  * ZeroNet version: rev3223\r\n  * Operating system: Windows 10 Pro\r\n  * Web browser: Chrome 63.0.3239.132 \r\n  * Tor status: not available\r\n  * Opened port: no\r\n  * Special configuration: none\r\n\r\n### Step 2: Describe the problem:\r\n\r\nThe Console window will not open when I run ZeroNet.exe nor there is a `Show Console` option in the icon tray menu\r\n\r\n#### Steps to reproduce:\r\n\r\n  1. Install the zeronet windows bundle and run\r\n  2. Try to open the console window (to debug an error or something)\r\n\r\n#### Observed Results:\r\n\r\n  Icon tray screenshot: https://ibb.co/nip5Ww\r\n\r\n#### Expected Results:\r\n\r\n- A Show Console item in the menu\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1516650396, "modified": 1516774325, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1257", "source_type": "github"}, {"topic_id": 1604066327, "title": "Allow zite loading outside iframe (for zite owners)", "body": "Would it be possible to add a flag that would allow a zite to be loaded directly instead of loading it inside an iframe?\r\nThe problem is that for tools like [vuejs-devtools](https://github.com/vuejs/vue-devtools/) to work the vue.js script must be present in the page itself. If the script is inside the iframe the tool won't detect the script.\r\nI could use that tool by loading the file directly on `file://` but then the zeronet api will not work.\r\nMaybe the flag can be set only if the person is the site owner.\r\n\r\nThanks", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1517071249, "modified": 1566322090, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/1262", "source_type": "github"}]}