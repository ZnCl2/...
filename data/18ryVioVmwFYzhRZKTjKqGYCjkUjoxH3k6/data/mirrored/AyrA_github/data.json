{"cert_user_id": "AyrA@github", "next_topic_id": 2, "topic": [{"topic_id": 1604067431, "title": "Files with \"!\" in the name are not added to content.json", "body": "### Step 1: Please describe your environment\r\n\r\n  * ZeroNet version: 0.7.0 rev4172\r\n  * Operating system: W7x64\r\n  * Web browser: N/A\r\n  * Tor status: N/A\r\n  * Opened port: N/A\r\n  * Special configuration: No\r\n\r\n### Step 2: Describe the problem:\r\n\r\nFiles that contain `!` (exclamation mark) in the file name are not added to `content.json`.\r\n\r\nNote: This seems to apply to a few other characters too, including but not limited to `'` (apostrophe) and `&` (ampersand)\r\n\r\n#### Steps to reproduce:\r\n\r\n  1. Create blank site\r\n  2. Create a few files with and without an exclamation mark in the name\r\n  3. Set site name and description, then sign and publish the site\r\n\r\n#### Observed Results:\r\n\r\nWhen setting the site name and description, the directory is scanned for files, and they are added to the `content.json` index. Files containing an exclamation mark are not added to the file and thus are not shared with others.\r\n\r\nThe debug log contains lines like these (some information has been redacted by me):\r\n\r\n    [2019-08-11 \u2588\u2588\u2588] ERROR    Site:\u2588\u2588\u2588 - [ERROR] Invalid filename: Test !.txt\r\n    [2019-08-11 \u2588\u2588\u2588] INFO     Site:\u2588\u2588\u2588 - [SKIPPED] Test !.txt\r\n\r\n#### Expected Results:\r\n\r\nNo seemingly arbitrarily chosen file name rules in place. Any file name made up of a valid file name character sequence should be added to the `content.json` file.\r\n\r\n### Related problems\r\n\r\nFiles/directories that start with `.` or end in `-old` or `-new` are hardcoded to be excluded too.\r\n\r\nThis puts a huge dampener on sharing git repositories or creating the `.well-known` folder. (See [RFC 5785](https://tools.ietf.org/html/rfc5785))\r\n\r\n### Notes\r\n\r\nThe offending location for the character selection seems to be https://github.com/HelloZeroNet/ZeroNet/blob/5a746769d01db58d54ff121e784fbf87374a89f3/src/Content/ContentManager.py#L603\r\n\r\n", "parent_topic_uri": "1604062980_users_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di", "added": 1565475212, "modified": 1573078708, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2136", "source_type": "github"}], "next_comment_id": 14, "comment": {"1604067431_mirrored_AyrA_github": [{"comment_id": 1, "body": "I want to add here that it took me **hours** to find this problem because the local client that hosts the files is totally happy delivering them to the browser regardless if the names are considered valid.\r\n\r\nIf you need a regex for all valid file names: `^[^\\x00-\\x1F\\x22\\x2A\\x2F\\x3A\\x3C\\x3E\\x3F\\x5C\\x7C]+$`\r\n\r\nWithout hex where avoidable: `^[^\\x00-\\x1F\"*/:<>?\\\\|]+$`", "added": 1565520117, "modified": 1565520117, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2136#issuecomment-520229491", "source_type": "github"}], "1604067437_mirrored_joshiemoore_github": [{"comment_id": 2, "body": "@imachug I would say all characters should be allowed. It's not the task of zeronet to decide if a file name is valid, this is the decision of the operating system. The only reasonable check for file names is to make sure they don't try to run directory traversal attacks. Everything else can be solved by aliasing the sha256 value of the file name as real name.", "added": 1566138887, "modified": 1566138887, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2141#issuecomment-522340539", "source_type": "github"}, {"comment_id": 3, "body": "> for example, using sha256 instead of file names will break Git repositories hostings on ZeroNet.\r\n\r\nGit repository hosting is already completely broken because you don't allow stuff to start with a dot. Which means `.gitignore` and the entire `.git` folder are out. You also deny files to end in `-old` or `-new`. ZeroNet has many arbitrarily made up file name rules that don't matter. `content.json` is also completely blacklisted regardless if it's the actual page file or a file made up by the user in a subdirectory. In other words, if I create a git repository with a .gitignore and a content.json file in them I will publish an empty site because all important things were excluded.", "added": 1566139586, "modified": 1566139586, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2141#issuecomment-522341349", "source_type": "github"}, {"comment_id": 4, "body": "A side effect of aliasing is also that you get free file deduplication with it.", "added": 1566139646, "modified": 1566139646, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2141#issuecomment-522341412", "source_type": "github"}, {"comment_id": 5, "body": "@imachug It's normal for you that <code>.*</code> works. Invalid files always work on the machine that hosts the page.\r\n\r\nThe offending code is here:\r\n\r\nhttps://github.com/HelloZeroNet/ZeroNet/blob/5a746769d01db58d54ff121e784fbf87374a89f3/src/Content/ContentManager.py#L622", "added": 1566153806, "modified": 1566153806, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2141#issuecomment-522358311", "source_type": "github"}, {"comment_id": 6, "body": "I want to add here, that `..` in the middle of a file name is also considered invalid according to ZeroNet:\r\n\r\nhttps://github.com/HelloZeroNet/ZeroNet/blob/5a746769d01db58d54ff121e784fbf87374a89f3/src/Content/ContentManager.py#L598\r\n\r\nThis looks like a beginners level check for a directory traversal attack.\r\n\r\nHere's how to properly check for this:\r\n\r\n1. Combine the base (trusted) path and the relative (user supplied) path using built-in filesystem library function (this gets a rooted but not necessarily resolved path string)\r\n2. Use same filesystem library to convert the path into an absolute path (this resolves `..` and other constructs)\r\n3. Check if the result still starts with the base path.\r\n", "added": 1566154103, "modified": 1566154103, "source_link": "https://github.com/HelloZeroNet/ZeroNet/pull/2141#issuecomment-522358648", "source_type": "github"}], "1604067730_mirrored_slrslr_github": [{"comment_id": 7, "body": "Why not use multiple processes, one for each page? I know you claim that interprocess communication via TCP is slow, but unless you try to move more than half a gigabyte per second you're fine. The value was obtained by testing with a single thread on a rather old machine (Intel Core i7 CPU 960@3.20GHz), this is a 10 year old processor that really struggles to play 4k movies.\r\n\r\nZeroNet could provide each page under a different local address (each machine has over 16 millions after all, might as well use them). This would grant better page isolation and allows pages to be run natively in the browser rather than a sandboxed iFrame.", "added": 1575454732, "modified": 1575454732, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561642176", "source_type": "github"}, {"comment_id": 8, "body": "Of course this would break backwards compatibility, but that's what changes in the major version number are for. Just implement both systems for half a year to allow everyone to migrate.\r\n\r\nI don't understand what you mean by proxy support. In regards to non-loopback addresses, I could not find anything either.", "added": 1575455729, "modified": 1575455729, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561648296", "source_type": "github"}, {"comment_id": 9, "body": "@HelloZeroNet \r\n\r\n> Multi processing would also mean that we need to load the database and all the libs to every process, so the memory usage would radically increase.\r\n\r\nYou don't have to duplicate a single thing.\r\n\r\nYou load the things that all processes need (for example the logger, tracker handler, and peer finder logic) into a master process. This means you don't even need to reference the libraries for those things in the child processes at all. The database of each site is loaded by the process that hosts the site, you don't need to load the database of every site into every process simultaneously. Even if you do, SQLite will not load things into memory that you are not asking it to.\r\n\r\n> It's not about bw, but serialization/synchronization.\r\n\r\nI would not try to implement too much synchronization. Two sites should not be able to block eachother and/or the master process for no good reason.\r\n\r\nAll operating systems this product is intended to run on have a decades long history of optimizations in them. They all figured out pretty well for example how to handle a disk queue from multiple processes. Sites will not write to disk anyways unless they are syncing, which is not too much data anyways and you're almost certainly bottlenecked by the network and not the disk, even if a traditional rotary HDD.\r\n\r\n> - Single thread: 0.2s\r\n> - Cross-process: 105s\r\n\r\nThis statistic is irrelevant without seeing how many calls per second were achieved (and how this is problematic to you but not IIS, Apache, Exchange Server, etc.). They all would stall to death if this number was too low.\r\n\r\nI'm not sure what number of calls per second you expect. The idea of having multiple processes, one for each site, is that almost no interprocess communication is necessary at all. The processes should mostly operate independently of each other apart from a few things:\r\n\r\n**Child --> Master**:\r\n\r\n- Ask for peers\r\n- Send statistics\r\n- Send log messages (unless log is kept on a per-site basis)\r\n- Test if master is alive (and kill itself if not anymore)\r\n\r\n**Master --> Child**:\r\n\r\n- Start child process\r\n- Ask for statistics\r\n- Send peers\r\n- Pause/Continue/Delete commands (Delete is technically not necessary as you can just kill the process and purge the directory)\r\n- Assign IP/Port for showing to user (unless given as start argument)\r\n- Send shutdown command\r\n- Notify of settings change\r\n- Check if child is alive (Does not actually needs communication as you can just check for the PID)\r\n\r\nPython is not alone with this problem. NodeJS offers clustering in a multiprocess architecture because it has the same single-threading problem.\r\n\r\n@imachug If only 127.0.0.1 is available you could still use multiple ports to circumvent the problem. Browsers do site isolation based on the entire origin, not the IP. This still gives you space for approx 50000 sites you could open at the same time and I've never seen a system that can simultaneously open that many browser tabs without offloading some to disk.", "added": 1575461693, "modified": 1575461693, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561689924", "source_type": "github"}, {"comment_id": 10, "body": "> you don't need 100 connection to same peer if you want to get updated modification of 100 sites, so you would need to sync/send all data between processes.\r\n\r\nYou can share sockets between processes (This is usually how multiprocess webservers do it). Also you mentioned before that the bandwidth is not an issue, so having the connection handled by the master process would not be so bad. Multiple sockets to a single client would also make it easier for a client to prioritize a site over another.\r\n\r\n> The python process takes 32MB with all libraries loaded, before loading any site data or make any connections. So it would be 3.2GB if you have 100sites.\r\n\r\nZeroNet currently uses 1 gigabyte after running for 10 minutes with 8 sites. It's just syncing, I have not opened one of them yet. If memory consumption of multiple processes is a concern for you, you should address this issue first and figure out what makes the process try to hold everything ever received in memory.\r\n\r\nThat's not how libraries work usually anyways. Half the idea of a library is that you can write code once and use it. The other half is that it doesn't needs to be loaded into memory n times if n applications need it, but only once.\r\n\r\nYou also don't need to have 100 processes open for 100 sites all the time. After a site has synced you can exit the process if nobody requests anything from it and have the master just wait for peer events to spin it up again (This is pretty much how FastCGI works).\r\n\r\nIf you don't want to share socket handles, networking could also be handled by an individual process (this is how an Exchange Server does it for example). The site process then merely gets file download/update notifications from the networking process and itself mostly works as a web server.\r\n\r\n> Also I don't see any real reason to move every site to separate process.\r\n\r\nProcess isolation. It can help a lot in threat mitigation. The ZeroNet security model is currently to pray that the browser sandbox works and that there is no security vulnerability in ZeroNet itself. A site that figures out how to break out of the sandbox and run in the `127.0.0.1:43110` origin essentially has full control over the ZeroNet instance at that point. You don't need a multiprocess model for this but can instead run each site on its own port, but then you run into the python-is-singlethreaded problem again with multiple tcp listeners.", "added": 1575467544, "modified": 1575467544, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561733357", "source_type": "github"}, {"comment_id": 11, "body": "> That should be a badly designed site or a bug, as I running 50 sites with 70MB of memory usage.\r\n\r\nIt's 5.4 GB now. Not sure what you mean by \"badly designed site\". A Site should not have control over what ZeroNet keeps in memory or not. This sounds like an evil DoS possibility.\r\n\r\n> You need to keep connection open to be able to receive updates for the sites.\r\n\r\nYou don't need to be able to update a site you're not using instantly. If I'm not using a site I don't see the difference between it constantly updating or only every 5 minutes.\r\n\r\n> This used to be correct but it looks like almost all API calls are safe enough (i.e. there's no RCE possible and the best thing an attacker can get is private keys, but this can't be mitigated with any other sandbox).\r\n\r\nGetting private keys is usually an absolutely horrible thing.\r\n\r\nI've seen many people claim that their software is safe but unless it has been audited by an independent entity that does these kind of things professionally and regularly, I will not believe you on that one, especially since you said yourself that only \"almost all\" calls are \"safe enough\".", "added": 1575470288, "modified": 1575470288, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561758569", "source_type": "github"}, {"comment_id": 12, "body": "> You can't delay the updates as there is central server. So if don't accept updates any time then it means other people can't make new comment/etc. anytime.\r\n\r\nThis would mean that not a single person has the site open. If the update interval is 5 minutes and 10 people have the site, it would mean that on average, your update gets through after 30 seconds.\r\n\r\n> O_o That's wrong for sure. What OS and ZeroNet version are you using? What sites do you seed?\r\n\r\nWindows 7 x64, 0.7.1 rev4322.\r\n\r\nI used this to cause the memory issue: `1DdPHedr5Tz55EtQWxqvsbEXPdc4uCVi9D`\r\n\r\nWhile it syncs, the memory jumps up to about 1.5 GB after a while and stays there most of the time. I'm not exactly sure what causes the problem of it jumping up. I've seen it happen when I blacklist stuff (click the name of a board to get the menu) while it still has 20k files to sync. In that case you might find that blacklisting might lock up the site (and really most of ZeroNet) for minutes.", "added": 1575471755, "modified": 1575471755, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561768027", "source_type": "github"}, {"comment_id": 13, "body": "> ...to a single peer that will shut down soon. Now what?\r\n\r\nThe next peer can get the update when it polls for it. The chance for a peer shutting down is the same as for a new peer to appear so it's statistically irrelevant. If all possible peers go away you can't publish your update, but there is nobody around to looking at it anyways so nothing is lost.", "added": 1575478642, "modified": 1575478642, "source_link": "https://github.com/HelloZeroNet/ZeroNet/issues/2312#issuecomment-561813909", "source_type": "github"}]}}