{
	"next_topic_id": 1,
	"topic": [],
	"topic_vote": {
		"1514877972_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": 1,
		"1515376361_1P7ZF1U5tMb3BKzp6jxWzz37VqGg2XiMhs": 1,
		"1516070614_19WW28sn46rz9h15xf6NCELPdzxZ2YhbyB": 1,
		"1519956238_12gAes6NzDS9E2q6Q1UXrpUdbPS6nvuBPu": 1
	},
	"next_comment_id": 27,
	"comment": {
		"1514711433_164x9q7LJPLX4SMbhqLPraKAGZRPa2gTFe": [
			{
				"comment_id": 1,
				"body": "There can be many use cases of private zeronet sites but using it as paywall is something I am not very excited about.\n\nMy best wishes for the donation campaign 👍",
				"added": 1514749103
			},
			{
				"comment_id": 2,
				"body": "> [krixano](#comment_275_12gAes6NzDS9E2q6Q1UXrpUdbPS6nvuBPu): Why? Do you imagine a different way of monetizing things that you think would be better?\n\nDirect payment like donations, patreon are some good ways (though I admit, maybe not very successful till now). I think more people should start funding artists, programmers directly.\n\nAds are worst because instead of directly paying, we pay more indirectly like facebook makes on average $20 user/month (https://adexchanger.com/platforms/facebook-made-almost-20-average-revenue-per-user-q4-big-jump/). Guess how much benefit $$$ advertisers would be getting (from us) by spending those $20 and how much user's data collection and exploitation facebook must do to earn those $20.\n\nPaywalls are OK (but no DRM). My concern with paywalls is that not everyone can pay for everything. Perhaps early access to paid subscribers and free access to all after some time could be a balanced compromise. Time duration can depend on content like a film can be released for free a year after release and periodic content sooner.",
				"added": 1514752060
			},
			{
				"comment_id": 4,
				"body": "> [herp](#comment_70_164x9q7LJPLX4SMbhqLPraKAGZRPa2gTFe): Patreon or similar service (Liberapay anyone?) would be required as we currently don't have any payment providers on ZeroNet, but if you wanted to host the payable content on ZeroNet, then you'd need private site functionality.\n\nI think with wider adoption of cryptocurrencies, we may not need to be that reliant on third party payment processors. In fact, this was the whole purpose of creating bitcoin - to send/recive money directly without any middlemen/gatekeepers.\n\nI see ZeroNet also as fulfilling the same purpose - to connect people without any (powerful) third-party like youtube or twitter. Obviously, being p2p means it's uncensorable, 'cause, like even the GFW would have to block all IPs including within own country 👻 and it can scale to any size. We have16 years of torrent legacy to prove this. Even with private sites, these purposes will be fulfilled, provided there is no such thing as private trackers as it provides a single point of failure. As you rightly said, to bring most artists (programmers are cool people) we need private zites feature, as it would give assurance of a revenue model. After this feature implementation, I think this would be the best possible alternative for artists, far better than YouTube or Vimeo. We want artists to be on our side.\n\nBy the way, Liberapay seems good. Thanks for letting known.",
				"added": 1514921241
			},
			{
				"comment_id": 6,
				"body": "> [mrsrobotzerocat](#comment_27_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa): I believe the problem with private, restricted, monetized content .. on the clearnet, p2p platforms, etc .. is that, all it takes is one individual to make the first purchase, or get the first ticket - gain access to everything, and then leak said content to the public. It's extremely hard ...\n\nYes! with implementation of mrsrobotzerocat's ideas. We can:\n\n1. Have revenue stream for everything and not just paywall content, including blogs\n2. Everyone can have access to content and not have to worry about paid content being shared elsewhere as gratis\n3. Free mining and thus, cryptocurrencies from the control of few (giant) mining farms to, in the hands of everyone\n\nI see adblock or tracking protection as very important for protecting our browsing from being tracked by few monopolies like google, facebook and amazon. Besides, ads are either intrusive like flashy banners or deceptive like sponsored content and serves no purpose.\n\nNonetheless, as you clearly pointed out - private zites have many use cases. So we still need private zites feature. Whether someone wants to distribute content as paywall behind private zites, crowd sourcing or through mining revenue is their choice. I hope most would opt for later two options and people would support them.\n\nAbout patents (intellectual property is misleading, out of copyright and patents, I think you meant patents primarily) and post-scarcity, I also think the same. Patent system favors the large, already established corporations. One of the features of computer is creating infinite copies at zero-marginal cost, with this everyone can have access to anything that can be digitized. Over-time we need to gravitate towards it. For less and less things we need to pay for, less we need to earn $$$. But in between that and now, everyone need a source of livelihood.",
				"added": 1514962005
			}
		],
		"1510946916_1BrGZNxToC56H726jqXuUNuR6M8nn9kfBN": [],
		"1514877972_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": [
			{
				"comment_id": 5,
				"body": "distributing according to square root of people's contribution will ensure higher share for contributing more but also decrease exponentially. So it can be total revenue divided by (sum of square root of everyone's contribution).\n\nmrsrobotzerocat, I don't think zeronet zites data is stored on blockchain. Its simple torrent like. In torrent we have a hash of torrent file and torrent file contain hashes of all the pieces of files (which is kind of like blockchain but read on 👻). In ZeroNet, we have content.json instead of torrent file which is digitally signed with zite owner's private key, this way it is not static (unlike torrent) but can change, since signature of content.json need to be valid and not the hash. Inside content.json are hashes of other files. This way zite owner can add, remove and modify files but not others.",
				"added": 1514923273
			}
		],
		"1_1KkYwbYeCF7s17AmUF9Tc7ckzCgt3UoaaC": [
			{
				"comment_id": 7,
				"body": "magnet:?xt=urn:btih:A9B07B9B3CE47304B8B23150C9607BA4B9D9A1FE&dn=Safehold+1&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Fopen.demonii.com%3A1337&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce\n\nIt has all six audio books of David Weber and have 3 seeders. From https://btdb.to/torrent/v5KOjVNZlMCAO4x9vpaPSvvbPEX2DPt5jj0.html",
				"added": 1516202810
			},
			{
				"comment_id": 8,
				"body": "> [drone621](#comment_13_1KkYwbYeCF7s17AmUF9Tc7ckzCgt3UoaaC): Thank you very  much, all. I had never heard of BTDB and I've favorited it now...\n\nHere are list of many more torrent websites if you need - http://127.0.0.1:43110/17Tzm1HYknrRiiHZ5xLJqGNooebmoLqit7/?Post:117:Torrent+sites:+Index+Search+engine+Tools+[2017-05-05]\nhttp://127.0.0.1:43110/15qQZydzmaw6KpW8zWXz3XRmxQf4tQEqxX/?Post:3:DHT+Search+engine+Apps\nhttp://127.0.0.1:43110/15qQZydzmaw6KpW8zWXz3XRmxQf4tQEqxX/?Post:5:Torrent+search+engine",
				"added": 1516336590
			}
		],
		"1516541399_1FFFPdGUQvWPFf4cxmqrGwdKgra3Eu4UhM": [
			{
				"comment_id": 9,
				"body": "You can create ZeroID using bitmessage if you are concerned about ip address being associated with your ZeroID. And if you want to completely hide your ip address from ZeroNet network than you can additionally enable tor always mode. Though than ZeroNet would be slow.",
				"added": 1516554125
			},
			{
				"comment_id": 10,
				"body": "Ok, I understood what you are saying. Maybe @nofish can only answer these questions. I saw some certs zite which is completely within ZeroNet network. Given the improvement in ZN, ZeroID should also now be updated to work completely within ZN network. In any case, nofish and other zites which use clearnet should clearly state it on their zites.",
				"added": 1516625677
			},
			{
				"comment_id": 14,
				"body": "> [nofish](#comment_1096_1J3rJ8ecnwH2EPYa6MrgZttBNc61ACFiCj): If you use Tor/VPN/etc to register your ID and publish your content, then you are safe. Otherwise there is a possibility to connect it with your IP address.\n\nThanks! for clarification. I think specifying this on ZeroID also would a good idea so that everyone can know.",
				"added": 1516633566
			}
		],
		"1516596886_1KNSf9nhRFUc52aeX947CRwWTUGPPnZdyQ": [
			{
				"comment_id": 11,
				"body": "Some ideas which maybe helpful:\nWe can split database or static pages into pieces like 50 or 100 mb as merger zite or big files. And every visitor to main zite can start seeding the least seeded merger zite (or big file) or choose to host merger zite of their choice. And one can choose to host as many merger zites as many they want. This besides articles they access and therefore start seeding. \n\nFor search feature, we need to look at other distributed database related projects and how they perform search.\n\nFeature proposal:\nAnother thing that can be done is to have feature of allocating some percentage of zite size to host rarest pieces so that they never get lost. Like by default have 50% zite size (better option) or zite's size limit allocated to host rarest pieces. Users can modify this size limit to increase or decrease relatively like 200% or absolute number like 100mb for all zites (default) and per-zite also.\nBy zite size (instead of zite size limit), I mean if a user access a zite more than it would have bigger size on users storage space like 43mb than which access less like 13mb. Also some zites are small and some are big. If every zite starts filling up 10 mb zite size limit, after few zites only users storage space will start getting filled up. And a small zite's pieces will be replicated many times compared to large zite pieces. In case of zite's size, it will be based on zite's popularity (as already is) plus zite size (large size zite or small size).\n\nThis way, we need not split Wikipedia's static mirror zite into several merger zites but we still need to find a good search functionality across many peers.\nedit: for static content (which do not change), still using merger zites can be beneficial as one can add some content to merger zite (or big file) and remove it from own computer after some seeding. So it still gets seeded and zite owner need not keep the entire zite like 50 or 250gb on its computer (again provided the content do not change, wikipedia artcles are changebale by nature. maybe git can help?)\n\nA privacy note:\nI do not think sending search queries to several peers would be that big privacy issue since when we access a page and load their pages and images, the seeder can already figure out who is accessing which image or page. Being distributed, no single peer would known search queries of a particular user and most hopefully would be good peers who don't log anything. We may also try to find ways to obstruct who is searching what and who is just relaying search queries with some method. In this case only final seeder who actually serve the content like a wikipedia page (not those who served search queries) would known would actually requested.\n\nAlso using nofish's idea, if we can store and access offline data from zip/tar files than space requirement would significantly be reduced though I do not known if we can access individual pages from zip/tar files and if we can update zip/tar files without archive entire data from the start. However I think it should be possible. right?\n\nCombining the benefits of fetching and serving rarest pieces and zip/tar archive support would further increase capabilities of ZeroNet like big files and merger zites feature did.",
				"added": 1516629539
			},
			{
				"comment_id": 13,
				"body": "> [gitcenter](#comment_242_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): Implementation ideas [...]\n\nThis is a great idea and immediately implementable! I say you try it gitcenter, we would come to known how much it can scale to big website.s Even if we fail, we would known where the bottlenecks are and try to fix them. And given your excellent experience, you can be a very good leader in bringing mirror of big websites to ZeroNet.\n\nRandom grabbing of optional files can serve the purpose of fetching and seeding rare articles which no one is reading for now. But implementing 50% (configurable) rarest first files inside ZeroNet core would implement this feature in all ZeroNet zites. Though user and zite owners can disable it per zites like on ZeroMe or 0chan (you never known who might post what)\n\nIf we want to keep entire history of pages than versioning pages by date updated can be good but may make size grow very large if entire new page is created/updated instead of only differences. Otherwise (if keeping entire history is not important), just change in hash of page would be enough.\nI think we should implement page version that would be better.\n\nAllowing users to add and update pages is very good idea and it would make it not just Wikipedia mirror but entire wikipedia (on which users can contribute) with using wikipedia database as a base. And with this, we need not burden zite owner to do all the hard work of adding pages. A tutorial should be on the main page on how to add pages. We may in future need moderators to block and remove content from users who vandalize. Requiring removing action by 2-3 moderators can distribute moderators power and democratize the process. Like multi-signature maybe?\n\nI could not understand how seed index would work. Can't users find out from trackers and pex? and how would index be updated? Can you please explain it? Maybe feature proposal I am going to post on ZeroNet's git center issues might help in it.",
				"added": 1516633072
			},
			{
				"comment_id": 15,
				"body": "> [gitcenter](#comment_243_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): We don't even need additional files for index. We can use content.jsons. For example: [...]\n\nOk, i meant, finding out who is seeding which optional file which I guess we can find out from trackers and pex or do we request optional files from all zite's peers and than find out who has that file (which can be slow and flood the network with requests for large zites where each peer has only few optional files). You were talking about optional files index. I get it.",
				"added": 1516634173
			}
		],
		"1519627708_1M8ZpJwd63Vb8KdVaSb5EbqKxoRTVDmqTf": [
			{
				"comment_id": 16,
				"body": "> [innocentchild](#comment_1_1Gsi9eT29B2uDs5bgE8av17QJe4hZotADa): I have been tinkering with http://127.0.0.1:43110/1MgHVPCE1ve6QfKrgsqCURzRj72HrRWioz/?1Gsi9eT29B2uDs5bgE8av17QJe4hZotADa_4 to get the names of each magnet link, but I have yet to actually find a single name using this script.\n\nedit line 37 to:\n    line = file.readline().upper()[:-1]\n\nthere will be trailing newline '\\n' otherwise which may be the reason why script is not working.",
				"added": 1519831723
			},
			{
				"comment_id": 17,
				"body": "> [glightstar](#comment_58_14K7EydgyeP84L1NKaAHBZTPQCev8BbqCy): I haven't yet got to download the complete listing txt from the official source (as that seems to be offline or overloaded a lot), so would anybody mind making a tiny zite containing that file (maybe even the other files from the source :O)?I know that might be hard at first, to seed the files because of how large they are, but I think we should at least put the complete listing here on ZeroNet! [...]\n\nhttps://web.archive.org/web/20180221222547/http://46.166.187.105:8080/complete_listing.txt.gz\n\nIt would take a long time to crawl entire list by a single person. Task should be split among people like 50,000 per person. Create a separate topic and voluntarily pick list numbers like 1-50,000 or 50,0001 to 1,00,000 etc. and inform others which magnet links batch you're working on. Share generated list of titles with others using bigfile feature. Also inform users on other places like ZeroMe and Thunderwave who might volunteer.\n\ninnocentchild could you write code to crawl not just titles but size and date also? It only takes a little more space but provide a lot of useful information.",
				"added": 1519932452
			},
			{
				"comment_id": 18,
				"body": "> [glightstar](#comment_59_14K7EydgyeP84L1NKaAHBZTPQCev8BbqCy): I guess just some additions to L#51 are needed (filenew.write(\"magnet:?xt=urn:btih:%s&dn=%s\\n\" % (handle[1], urllib.quote_plus(handle[0].get_torrent_info().name())))), to add size and date stuff.., but I have no idea about the libtorrent-python-module (though it looks really neat!).. [...]\n\nWe can simply read torrent files in binary form and extract relevant information for our purpose. eg:\ntorrent_file = open(params['save_path'], 'rb')\n\ndef find_name(torrent_file):\n    name_after = torrent_file.find('name'.encode())\n    colon = ':'.encode()\n    start_at = name_after + torrent_file[name_after:].find(colon) + 1\n    end_at = start_at + torrent_file[start_at:].find(colon)\n    name = torrent_file[start_at:end_at].decode('utf-8')\n    return name\n\nname = find_name(torrent_file)\n\nAlternatively:\nNeat method would be to read libtorrent documentation to find how to parse name, date, size etc. because above code will not take care of special cases like escape characters like ':' in the name itself. eg: name 'ZeroNet : It's Awesome' will be parsed as just 'ZeroNet '. Also I do not know what it will do when there are multiple files. We need to try it with some torrents to improve code.\n\nMaybe you and innocentchild can help us in figuring out proper way from libtorrent documentation. Than we will start crawling to generate database of magnet links, names (just title or individual file names also?) , size and dates.\n\nedit:\ninnocentchild i will reply to you tomorrow.",
				"added": 1520022739
			},
			{
				"comment_id": 19,
				"body": "> [innocentchild](#comment_7_1Gsi9eT29B2uDs5bgE8av17QJe4hZotADa): libtorrent can fetch data such as creator, creation_date and comment (see full list below) from a fetched magnet URI. Since it is originally a c++ library, creation_date uses a datatype incompatible with python (but name and  size are fetched perfectly (see previous post)). The reason we are going for magnet links, and not torrent files, is simply because I don't have enough storage space for the 444GB dump. [...]\n\nI read your code and some libtorrent documentation. What the code is doing is downloading entire torrent at temp directory and than reading metadata from it. Which is problematic.\n\nWe can patch this code or write entirely new. Like add torrent in paused state and only fetch metadata. I will try to write if I have some time. Meanwhile you can also try and in the process will become better in programming by working on a real project. \n\nOtherwise, I think we should first bring torrent databases to 0net which are readily available like of thepiratebay http://uj3wazyk5u4hnvtk.onion/static/dump/.",
				"added": 1520094444
			}
		],
		"1520435778_18YsrRv5LsoqhKgirUbKNy8tXCRN1HqnDw": [
			{
				"comment_id": 20,
				"body": "> [leftside](#comment_53_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): I see!  [...]\n\nOptional files are of two types:\n* classic optional files\n* Big File optional files\n\nclassic optional files are downloaded entirely when yo browser them like like an image.\nBig File optional files are split into pieces like torrents and you do not download entire file but only what you requested. if you play first 2 minutes only, you will download only first 2 minutes of video If after that you seek to 7 minute, you will start downloading from 7th minute of video and you do not have video from 2-7 minutes. Instead, you should click '+seed' button to download entire file or play entire video in background and mute it.\n\n'+seed' button also has some caveat. It will stop downloading half-way if it do not find seeders at the time and will not retry later like torrent managers. I think we should have option of retrying but its currently not implemented. Instead, while you play video, if you stuck at some point, you keep on trying until you find some seeders or you close the tab. Therefore, playing video in background would be better.\n\nBig File optional files are not downloaded even if you click \"download and help distribute all files\". You have to manually download them.",
				"added": 1520500533
			}
		],
		"1522317542_1FenK9SJK21haGBvxM7vZNCLp1Z9hsCWUc": [
			{
				"comment_id": 21,
				"body": "DHT (not yet implemented) and pex (which I think already exist in ZN?) are effective against censorship provided there is no unique way ZN traffic can be distinguished from other traffic, which I think is [the case now](http://127.0.0.1:43110/Blog.ZeroNetwork.bit/?Post:128:Changelog:+March+21,+2018). Also users should be able to input ip/onion:port list to ZN as alternative bootstrapping method if bootstrapping nodes are blocked. After first run, ZN should not use bootstrapping nodes anymore and use previously cached ip/onion:port list.",
				"added": 1522595256
			},
			{
				"comment_id": 22,
				"body": "> [nullbock](#comment_9_18ewrh7YMwuSTBcYAqvsEgSAQp9DsBNhW6): Wait, it doesn't use a DHT?\n\n[Not yet](https://github.com/HelloZeroNet/ZeroNet/issues/57). Its not that easy.",
				"added": 1522597693
			}
		],
		"13_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ": [
			{
				"comment_id": 23,
				"body": "Please make a F-Droid repository or add to official F-Droid repository. F-Droid is an app store like Google Play but unlike Google Play, its official repository only host FLOSS apps and unlike Google, F-Droid cannot censor and dictate apps as anyone can make own F-Droid repository, hosting it on own server. User can add that F-Droid repository url and than can install any app on that repository. So its like downloading apps from the web with added benefits of app repositories like auto updates.",
				"added": 1523120312
			}
		],
		"1525030360_1AHEQxyRG9s6owyJHShB4U4rg9GL5FMX5K": [
			{
				"comment_id": 24,
				"body": "How about a merger zite? User data stay on merger zite and you *forget* its private key. You can update front-end but not user data.\n\nAnd if you moderate or censor user data on your front-end, users can clone and create new moderation-free front-end.",
				"added": 1525148743
			}
		],
		"1528116214_16CWdkXYYRnpct8GYS7GgD6yFcJHPXfMy8": [
			{
				"comment_id": 25,
				"body": "Check if your torrc file has been modified with update. Follow the process in FAQ to make it work with zeronet.\n\nIf no problem there than try if this works. In terminal:\n>python3\n>>import binascii\n>>binascii.b2a_hex(open('/var/lib/tor/control.authcookie', 'rb').read())\nCopy output, it would be something like:\nb'289jrensjanajnjnwnddnjfe'\n\nGo to zeronet_directory/src/Tor/TorManager.py\n\ncomment out line 188 and replace it with:\nauth_hex = b'that_output_above289jrensjanajnjnwnddnjfe'",
				"added": 1528124676
			}
		],
		"1528080237_1PMuHrGm4GFBBNbvRgr8zvyyRTc3rHpGYz": [
			{
				"comment_id": 26,
				"body": "> [a7oiz](#comment_3_1PMuHrGm4GFBBNbvRgr8zvyyRTc3rHpGYz): The problem with ZeroNet is that there is no sound economic system, and nodes have only two options: distribute content for free or not.This may not be a problem for the average website, but imagine sharing a 100 gigabyte Blu-ray movie file. At this point, if the seeder has no interest, the user deletes the file immediately after watching.\n\nHave you heard about bittorrent network? Successfully working for 16+ years without any financial incentive. Financialization of file sharing protocols may have unforeseeable disastrous consequences. From creating some sort of monopolies to calling out as criminal racket with financial incentives. I support nofish stance here, sharing should be done out of caring of community and not for some financial motives.",
				"added": 1528125799
			}
		]
	},
	"comment_vote": {
		"25_15TYJCh3i9W5dsoSUvchPwAuaB3BABXS6o": 1,
		"1086_1J3rJ8ecnwH2EPYa6MrgZttBNc61ACFiCj": 1,
		"70_164x9q7LJPLX4SMbhqLPraKAGZRPa2gTFe": 1,
		"21_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"25_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": 1,
		"26_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": 1,
		"24_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"27_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": 1,
		"29_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": 1,
		"30_1DpN91N84piuzYn2iJKM9RK3Ek8JVNjewa": 1,
		"2_1P7ZF1U5tMb3BKzp6jxWzz37VqGg2XiMhs": 1,
		"416_1F8omDpXyPsBfd9CBJ91o4Wv1AA2KxqMEY": 1,
		"97_19WW28sn46rz9h15xf6NCELPdzxZ2YhbyB": 1,
		"2_1Mr2r33XUTykaCR9yXLhqdkx2DaNa9EALD": 1,
		"422_1F8omDpXyPsBfd9CBJ91o4Wv1AA2KxqMEY": 1,
		"62_1AWwhg4EiWAVttfQboJZ4wJfX3WawfJT3h": 1,
		"13_1KkYwbYeCF7s17AmUF9Tc7ckzCgt3UoaaC": 1,
		"3_1FFFPdGUQvWPFf4cxmqrGwdKgra3Eu4UhM": 1,
		"5_1KNSf9nhRFUc52aeX947CRwWTUGPPnZdyQ": 1,
		"242_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di": 1,
		"1096_1J3rJ8ecnwH2EPYa6MrgZttBNc61ACFiCj": 1,
		"243_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di": 1,
		"244_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di": 1,
		"40_1AnFGL613gYqFWm4Lhwdk6x5gCRe1tPFxE": 1,
		"8_1Gsi9eT29B2uDs5bgE8av17QJe4hZotADa": 1,
		"10_18ewrh7YMwuSTBcYAqvsEgSAQp9DsBNhW6": 1,
		"1_19vYZ3YDA3LLxrhk3kSo58p2EppqkJVDMZ": 1,
		"110_1AHEQxyRG9s6owyJHShB4U4rg9GL5FMX5K": 1
	}
}