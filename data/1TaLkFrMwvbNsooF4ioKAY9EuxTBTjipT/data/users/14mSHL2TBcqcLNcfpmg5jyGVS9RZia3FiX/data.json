{
	"next_topic_id": 8,
	"topic": [
		{
			"topic_id": 1535667676,
			"title": "JS Framework for ZeroNet sites creation: what do you think is a needed/good feature?",
			"body": "As per title, what things would you like to have in a Framework for ZeroNet Sites?",
			"added": 1535667675
		},
		{
			"topic_id": 1535753075,
			"title": "Automatically generated ZeroNet Documentation for Developers",
			"body": "I've written a bash script that \"parses\" ZeroNet source code and writes function definitions separated per file, extrapolating some comments and with possibility so see source code.\n\nYou can visit [this site](/1DevDocJA8WBSVLPdHvDw8S1Zs9ihR3tBW) which is a bit ugly at present but exposes what the result of the script is. It can be useful to understand more in-depth the ZeroNet internals.\n\nAny help regarding improving the script and the site is welcome.\nFor example, at present it does not support interlinking and has an awful navigation.",
			"added": 1535753073
		},
		{
			"topic_id": 1535916228,
			"title": "What is the right `dbschema.json` for this use case?",
			"body": "I'd like to have a way to have, other than the tables that I need (topic and comment), the table json where I can consult the cert_auth_id of the user that posted such a file.\n\nUnfortunately I cannot get it to work. This is what I have now:\n```\n{\n    \"db_name\": \"Test\",\n    \"db_file\": \"data/users/test.db\",\n    \"version\": 2,\n    \"maps\": {\n\t\".*/data.json\": {\n\t    \"to_table\": [\n\t\t{ \"node\": \"topic\", \"table\": \"topic\" },\n\t\t{ \"node\": \"comment\", \"table\": \"comment\" }\n\t    ]\n\t},\n\t\".*/content.json\": {\n\t    \"to_json_table\": [ \"cert_user_id\" ]\n\t}\n    },\n    \"tables\": {\n\t\"topic\": {\n\t    \"cols\": [\n\t\t[\"topic_id\", \"INTEGER\"],\n\t\t[\"title\", \"TEXT\"],\n\t\t[\"body\", \"TEXT\"],\n\t\t[\"json_id\", \"INTEGER REFERENCES json (json_id)\"]\n\t    ],\n\t    \"schema_changed\": 1\n\t},\n\t\"comment\": {\n\t    \"cols\": [\n\t\t[\"comment_id\", \"INTEGER\"],\n\t\t[\"topic_id\", \"INTEGER\"],\n\t\t[\"body\", \"TEXT\"],\n\t\t[\"json_id\", \"INTEGER REFERENCES json (json_id)\"]\n\t    ],\n\t    \"schema_changed\": 2\n\t},\n\t\"json\": {\n\t    \"cols\": [\n\t\t[\"json_id\", \"INTEGER PRIMARY KEY AUTOINCREMENT\"],\n\t\t[\"directory\", \"TEXT\"],\n\t\t[\"file_name\", \"TEXT\"],\n\t\t[\"cert_user_id\", \"TEXT\"]\n\t    ],\n\t    \"schema_changed\": 5\n\t}\n    }\n}\n```\n\nLooking at other code (for `ZeroTalk`, `ZeroSites`, etc.) I see \"strange\" methods use, that I tried and it didn't work.\n\nCurrently my problem is that I get two lines for each user in json table: one for `data.json` with no `cert_user_id` and another one for `content.json` with `cert_user_id`. \n\nIs there any solution?",
			"added": 1535916225
		},
		{
			"topic_id": 1535994338,
			"title": "ZeroTalk sort-of written in Mavo",
			"body": "[Example of ZeroTalk written in Mavo](http://127.0.0.1:43110/1HV5GZ8ZkopZP8hS42rvqneFY2GgJNeHGq/)\n\nCan someone see if you can post comments?\n\nI've added a backend on the lines of [This other Mavo Example](http://127.0.0.1:43110/1F6Rt2QNMA6uJTSTZkDNBfXTd3unrtgcdJ/) that allows one to not only write apps for single user, but something along the lines of many of the sites that we have now by just writing the database tables and the html + css code for showing them.\n\nIt still has some bugs that I need to trace, but can someone try to post to it to see if it works? Thank you.",
			"added": 1535994334
		},
		{
			"topic_id": 1536955600,
			"title": "Watch out for the links you click on!",
			"body": "I just realized that using the basic markdown formatting used on this site one can trick other users to click on links thinking they are another site.\n\nFor example, you might be interested in the issue [https://github.com/HelloZeroNet/ZeroNet/issues/404](/1MgHVPCE1ve6QfKrgsqCURzRj72HrRWioz) but if you click on it you get redirected to NullPaste (but could be an \"evil\" site that after loading redirects you to the location you wanted to visit, gaining peers in this way).",
			"added": 1536955595
		},
		{
			"topic_id": 1543247109,
			"title": "ZN Wiki for site developers and maintainers",
			"body": "Hi all, I've recently started a [Documentation Wiki](/18hFUweFE3AJwZWnaMeq3fJMobrq8jevfd) for site developers and maintaners since nothing similar was available AFAIK.\nYou are all invited to use it (and to link it in the common answers on ZeroTalk) and to enlarge it by adding the missing pages.\n\nI think that the very scarce documentation for ZeroNet should be collected in one place and a collaborative wiki is the best way to do it.\n\nThe wiki already answers very quick questions about how to do something in ZeroNet, and should be enlarged with some tutorials and link to available documentation on other places.",
			"added": 1543247103
		}
	],
	"topic_vote": {
		"1530020784_1AHEQxyRG9s6owyJHShB4U4rg9GL5FMX5K": 1,
		"1536913855_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": 1,
		"1538505571_15hL2gR4oCSgKBbndn3MxY8Hc5xoy1vqCe": 1,
		"1540136796_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"1541169956_17WCGhYYYbjznWvdnjAoGS74aK9AtjfdU7": 1,
		"1542021379_139zVkd4AsZqs9qSYk6ypPDAqDcjPCmRm4": 1,
		"1542488436_191jbK3GNmVritcMymKkFuxyYFXxqv2xpC": 1,
		"1542515456_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"1540051200_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di": 1,
		"1546726166_13CFz29UuNbUx54HmLqQBfxPQNN2FmfdAq": 1,
		"1547669338_131Seivv6aH8vnohPmYPcPwPfZ1a5ubMMs": 1,
		"1550266119_131Seivv6aH8vnohPmYPcPwPfZ1a5ubMMs": 1,
		"1550134287_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"1552823531_12gAes6NzDS9E2q6Q1UXrpUdbPS6nvuBPu": 1
	},
	"next_comment_id": 93,
	"comment": {
		"1535667676_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX": [
			{
				"comment_id": 1,
				"body": "@gitcenter :) What I would like is:\n* file-backed-storage as an object to be able to write something like\n  ```\n  storage = getFileBackedStorage(\"data/users/1___/data.json\")\n  storage[\"comments\"].push({ body: \"Something\" })\n  storage.signAndPublish()\n  ```\n  which should now be possible to do with ES6 Proxy objects\n* Easy redirect command (the source code for 1Repo... (GitCenter Simple Repo) does it to redirect to main site and it is not really that simple, though not complicated either)\n* Wrapping all ZeroNet API commands in proper functions that return promises (to use async/await) instead of having to use `this.cmdp` from ZeroFrame.\n  This should make the code more clear and also could allow for static type checking of the code.\n* Code to create a custom CA and for each user to be able to have a certificate without having to do any action (useful for sites where we want users to be all anonymous), look at http://127.0.0.1:43110/Talk.ZeroNetwork.bit/?Topic:3_168qV1Xxp2EyszrzdECLUwXrn9wGYjDdar/NEW+WEBSITE+share+it+here#comment_17_16Szsy1UtjiXpEevWVdUDrNfNTAZ9PLbcY",
				"added": 1535706955
			},
			{
				"comment_id": 2,
				"body": "@gitcenter I casually saw it a couple of minutes before you replied. I think it would benefit a lot if a page is added where the methods from each File are enumerated with a brief esplanation and what do they return.\n\nThe code is easy to read, so something like this is not really necessary but I think would stimulate adoptance.\n\nDo you mind if I try to implement what I would like to have and then do a PR to your code?",
				"added": 1535710792
			},
			{
				"comment_id": 11,
				"body": "> [xianc78](#comment_27_141zS75MupoDVjN4p7ivG9pXUXjAm9jLBQ):  [...] how about using a paradigm other than object-oriented programming [...]\n\nCheck [Purescript](http://purescript.org) if you like functional programming: it is an Haskell-like language (pure and statically typed, with very similar syntax) that compiles to Javascript.",
				"added": 1535986849
			}
		],
		"1535753075_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX": [
			{
				"comment_id": 4,
				"body": "> [gitcenter](#comment_328_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): Though it has awful navigation ( :) ), its design is very nice. [...]\n@gitcenter Thank you! I've forgotten to link to the [repository with site code](/1MpJAtUTw4f7YjXXQfW2BovkmUmnZGT5AA).",
				"added": 1535790687
			},
			{
				"comment_id": 5,
				"body": "> [dankman](#comment_3_14YYM4xgA29hW4PXRdqJgSTKnD4fKKPqEH): This line has a bug:\"echo \"${BASH_REMATCH[1]}.${BASH_REMATCH[2]}\"    elif [[ $line =~ ^([A-Z_]+)\\ +\\=\\ +([^#]+)(#(.*))? ]]; then [...]\n\n@dankman Could you please be more specific? i.e. writing what should be the expected line and why should it have one less parenthesis? They seem balanced to me.",
				"added": 1535915673
			}
		],
		"1535916228_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX": [
			{
				"comment_id": 6,
				"body": "I don't get it. What I have now is:\n```\nsqlite> select * from json;\njson_id|directory|file_name|cert_user_id\n1|14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX|data.json|\n2|14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX|content.json|trenta3@zeroid.bit\n\nsqlite> select * from topic;\ntopic_id|title|body|json_id\n1|Post title|Post body|1\n```\n\nThe problem is the two `data.json` and `content.json` are different and repeat for each entry.\nLEFT JOIN using (json_in) does join the topic row with the row on `data.json` non with the one with `content.json`",
				"added": 1535918406
			},
			{
				"comment_id": 7,
				"body": "```\nsqlite> select * from topic LEFT JOIN json USING (json_id);\ntopic_id|title|body|json_id|directory|file_name|cert_user_id\n1|Post title|Post body|1|14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX|data.json|\n```\nNote `cert_user_id` is missing",
				"added": 1535918515
			},
			{
				"comment_id": 8,
				"body": "I did reload my database, the problem is I want `cert_user_id` not `cert_auth_user`, so I'd like to see `trenta3@zeroid.bit` written",
				"added": 1535918796
			},
			{
				"comment_id": 9,
				"body": "Yes, I did rebuild but it wasn't successful;\n\nI've nonetheless resolved, I just forgot to set `\"file_path\": \"data.json\"` under `\"maps\".\".*/content.json\"`",
				"added": 1535918935
			},
			{
				"comment_id": 10,
				"body": "> [krixano](#comment_507_12gAes6NzDS9E2q6Q1UXrpUdbPS6nvuBPu): Ok, well glad you got it figured out. Sorry if I may have sounded rude, patience running low right now, but not from anything you did.\n\n@krixano Nothing to worry about, I was also a little upset since I lost a couple of hours on this.\nThank you anyhow",
				"added": 1535919174
			}
		],
		"1535994338_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX": [
			{
				"comment_id": 12,
				"body": "@kaffie Could someone retry it now? Thanks",
				"added": 1536095778
			}
		],
		"1536836748_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 13,
				"body": "After defining a decent way to specify a document unit (see [my issue](https://github.com/HelloZeroNet/ZeroNet/issues/1513)), one could create a plugin that from each site the user seeds extracts and indexes all documents.\nThe same plugin would be used to do searches by sending requests to other peers and returning the document unit URL.\n\nIdeally one should prevent other peers from learning user queries and to return invalid documents (they could also send original signatures to see that they were sent to the same site), and also ensure that all results for a query are returned by imposing a structure on the resulting net.",
				"added": 1536860475
			},
			{
				"comment_id": 14,
				"body": "> [blurhy](#comment_99_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK): Another issue is how to ensure the search result or the site tags/maps are not faked.\n\nThe data.json signign process could be changed to sign it as a merkle tree (I don't know if this already happens). Then verifying a document unit is simply a matter of providing the merkle chain to the node inside the data.json for each record in such document.",
				"added": 1536921333
			},
			{
				"comment_id": 15,
				"body": "@blurhy That URL gives me 403 Forbidden",
				"added": 1536922076
			},
			{
				"comment_id": 16,
				"body": "@blurhy Exactly\n\nWe could bisect each \"JSON type\" (lists with natural order and objects alphabetically sorting by key) to generate a merkle tree:\n\nSuppose we have the list [obj1, obj2, obj3, obj4, obj5] then we could say the hash of such list is the composition of the hash of [obj1, obj2, obj3] and of [obj4, obj5] and so on. For primitive types (integers, booleans and strings) we could resort to standard hash of their string representation (with \"\" for strings)",
				"added": 1536942162
			}
		],
		"1526283454_17d3S7saDQP5phkkWS9F7tMJTymqLCQmig": [
			{
				"comment_id": 17,
				"body": "Title was \"Is it possible to port sci-hub on Zeronet?\"\n\nAFAIK Every article that sci-hub gets from the various sources, is then uploaded to [libgen](http://libgen.io/), which exposes database dumps. This means one could get the dumps and reduce them to minimal fields (Author, Title, ISBN/doi) and download links (file sha1) should be enough to construct a [magnet URI](https://en.wikipedia.org/wiki/Magnet_URI_scheme) and using [WebTorrent](https://webtorrent.io/) in browser or basing on a plugin with a torrent client one can create a decentralized libgen/sci-hub and at the same time contribute to the main one by seeding torrents.",
				"added": 1536942756
			}
		],
		"1536913855_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 18,
				"body": "Could just implement something simply like [this paper suggests](http://www.shibbo.ethz.ch/CDstore/www2004/docs/1p403.pdf).\n\nBasic ideas:\n* Each user can either trust or distrust any other (i.e. rating +1, 0 or -1).\n* These ratings are used to compute your level of trust of each other user using different kinds of propagations such as: *transitivity* (i trusts j and j trusts k implies i trusts k), *common interests* (i trusts j1 and j2, k trusts j1, maybe he should also trust j2) and others.\n* The final level of trust is a real number that is converted back to +1, 0, -1.\n\nNote that it does not give a global reputation (that could be easily obtained with a PageRank analogue), but rather a user-to-user reputation.",
				"added": 1536953391
			},
			{
				"comment_id": 19,
				"body": "@thunder33345 No time to do it now, but it should be fairly easy since it uses only sparse matrix multiplication.\n\nWe should just think about how to really integrate this into sites:\n* Should a user reject updates for distrusted users or simply do not show them on the interface?\n* Is there a flaw in these techniques? Is it possible to discard other users data by creating a lots of false accounts that distrust such user and at the same time trust other very trusted users in order to use the *common interests* rule to apply censorship?",
				"added": 1536963956
			},
			{
				"comment_id": 21,
				"body": "> [kaffie](#comment_679_1NWh3WAty57FH8at1WtrZigMrdhrDkuPzh): I'm against censorship, personally. That's why I support ZeroNet.\n\n@kaffie Even of bot generated data? What if one creates 1000 bots accounts and posts every day lots of messages about selling viagra (maybe the same message), making it difficult to find valuable posts on ZeroTalk and also growing the amount of data to download of a great amount (about 50M, which is a lot for the current size of it)?\n\nMoreover, such a trusting system could be also used to change the current restriction on how much data one is allowed to post: we could have a limit on **daily** data for each user and then a trust score for each one given by their reputation that increases or decreases such a limit down to zero.\n\nThis way even if one has a legit account that gets \"cracked\" and becomes a bot, the other users would simply distrust him and after little time he would not be able to post there anymore.\n\n@blurhy I think user blacklists really do not delete data nor do they have any effect on what data do you transmit, so the issue about how much data you have to download remains...",
				"added": 1536997789
			},
			{
				"comment_id": 26,
				"body": "@toyata Concrete examples of trust-based systems that can be subverted or have been?\n\n@thunder33345 Sorry I can't understand what you are saying, except that even if you are \"for freedom\", you have a blocklist for spam. The current topic is about automatically create a different blocklist for each user based on their trust/distrust scores and maybe even not to seed content of blocklisted users (if others thinks they are not spam they will continue to seed it).\n\nThe paper I suggested gives different trust score to *couples* of users, so that the trusting really depends on the people you choose to trust and who do they trust. This means that if you do not trust the \"CIA subverting agents\" their votes will have very little to no effect to what you see.\n\nThat apart, this whole discussion really encorages me to create thausands of bots and spam here for the whole week. After that I think we should re-have such a discussion.",
				"added": 1537001153
			}
		],
		"1536955600_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX": [
			{
				"comment_id": 20,
				"body": "> [skwerlman](#comment_79_1JupR9ExhM4aiVd1Jyy2iuS12eukDnQ54X): hovering over a link should replace the link text with the actual link\n\nIt is true with many of the most common browser, but I suspect almost all of us have clicked on links (especially those that show as full urls) without reading the actual link.",
				"added": 1536964221
			},
			{
				"comment_id": 22,
				"body": "> [kaffie](#comment_677_1NWh3WAty57FH8at1WtrZigMrdhrDkuPzh): In the vast majority of cases, people aren't malicious and make it clear what they're linking to. Pedos included. [...]\n\nGood to know.",
				"added": 1536997979
			},
			{
				"comment_id": 29,
				"body": "> [nekololi](#comment_259_1FzZsDQdh8wk71bDLxURPzmAt2FXehoNAB): this has been true about hyperlinks since hyperlinks were created.... [...]\n@nekololi Yes, but in some legislations distributing illegal content (e.g. clicking on a zeronet link that downloads and seeds a zite) is really more severely punished than to only view it (e.g. clicking on a link on the clearnet that only downloads some files and does not seed them).\nSo hyperlinks on zeronet are \"legally more dangerous\" than on the clearnet.",
				"added": 1538155287
			}
		],
		"1536994186_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 23,
				"body": "If I type something and press Enter I get a JS exception:\n```\nUncaught TypeError: Cannot read property 'length' of undefined\n    at next (horizon.js:240)\n    at Page.searchFollowedCORS (horizon.js:268)\n    at Object.<anonymous> (horizon.js:201)\n    at Page.onMessage (ZeroFrame.js:38)\n    at window.addEventListener.e (ZeroFrame.js:29)\n```\n@blurhy How should it be used?",
				"added": 1536998157
			},
			{
				"comment_id": 25,
				"body": "Good. It works now.\nI have a couple of questions:\n\n1. What kind of data it indices? I noticed that for many common sites requests read access. Apart from these, the others only have site title + description?\n2. How is the search done? Complete word matching and complete search (a loop over the whole data) or using stemming plus a term-documents index?",
				"added": 1536999879
			}
		],
		"1537397559_1b9urVTLtAAZAidg6hchWqhrVtYXUaoj7": [
			{
				"comment_id": 27,
				"body": "My guess would be that to be premature optimization: most files on zeronet are user content (very distinct files) or very small (javascripts / css / html). The only thing that would be deduplicated are videos, which I guess not a lots of sites have in common...\nWe'll wait to see the reports!",
				"added": 1537399410
			},
			{
				"comment_id": 28,
				"body": "@swastika It would be good to open an issue on [ZeroNet on GitHub](https://github.com/HelloZeroNet/ZeroNet/issues) with the results you obtained.",
				"added": 1537511578
			}
		],
		"1539131791_1KuwwnkAtVrfnyhv8pyGRxLqzNZbEkJt2u": [
			{
				"comment_id": 30,
				"body": "> [homulilly](#comment_79_1JXekkVYJNdozXPFXZEA1AJXKjkWyFciTD): Interesting, but why the need for a plugin? [...]\n\n@homulilly Inbrowser Webtorrent can't connect to standard BitTorrent client so hybrid clients are needed.\n\nFrom WebTorrent [FAQ page](https://webtorrent.io/faq): \n> The WebTorrent protocol works just like BitTorrent protocol, except it uses WebRTC instead of TCP/uTP as the transport protocol.\n> In order to support WebRTC's connection model, we made a few changes to the tracker protocol. Therefore, a browser-based WebTorrent client or \"web peer\" can only connect to other clients that support WebTorrent/WebRTC.",
				"added": 1539162833
			}
		],
		"1541169956_17WCGhYYYbjznWvdnjAoGS74aK9AtjfdU7": [
			{
				"comment_id": 31,
				"body": "@emiliano @styromaniac Can someone please invite me? I don't know how to join it. If there is a link or something a ZeroMail would be very much appreciated.",
				"added": 1541181582
			},
			{
				"comment_id": 32,
				"body": "> [styromaniac](#comment_275_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd): FYI, I can see your real names you enter when you sign up from the invite link, though I'll neither contact nor refer to you with these names.\n\n@styromaniac No problem for me, I use the same username on clearnet (for example on Github).\n\nIf someone other wants to join: https://initiativeq.com/invite/SBTt5f537 (a single people may only invite up to 5 other).",
				"added": 1541184475
			}
		],
		"1541253539_1J3LZRHNNaGgZYmB65S4aknyYbM5ar2Bzb": [
			{
				"comment_id": 33,
				"body": "I don't think there are other ways apart from the one of kai0. What should be your use case?\nThe user is a spammer or just don't want to spread its words on that particular topic?\nCause there are currently some proposal about possible ways to prevent spamming with a web-of-trust-like score, where if you don't trust someone you don't spread all of it's posts.",
				"added": 1541254049
			}
		],
		"1542021379_139zVkd4AsZqs9qSYk6ypPDAqDcjPCmRm4": [
			{
				"comment_id": 34,
				"body": "This is definitely useful. Maybe a link to it can be put in the left border, below \"Zero Talk\", called \"my topics\" which then links to something similar to the mail ZeroTalk page, but listing only user's posts.\n\nI would also propose to implement some kind of post filtering in the main page selecting by initial poster, date, etc. so that this one would be just a use of it.",
				"added": 1542029576
			},
			{
				"comment_id": 36,
				"body": "> [ornataweaver](#comment_75_139zVkd4AsZqs9qSYk6ypPDAqDcjPCmRm4): Would it become too crowded?\n\nI don't think I understand: what would became too crowded? ZeroTalk main page?",
				"added": 1542383211
			}
		],
		"1542120449_191jbK3GNmVritcMymKkFuxyYFXxqv2xpC": [
			{
				"comment_id": 35,
				"body": "> [caryoscelus](#comment_83_13oRBYqNeUr6Tvgt4KkAT9FT4XRiKFBjnE): In case of this original 0talk instance, you'll have to contact nofish to increase your limit.\n@caryoscelus Actually I understood from some conversation here that nofish automatically does it on a biweekly basis. Therefore you shouldn't have any need to contact him if you are not an \"aggressive\" writer.",
				"added": 1542212872
			}
		],
		"1542488436_191jbK3GNmVritcMymKkFuxyYFXxqv2xpC": [
			{
				"comment_id": 37,
				"body": "Concerning the splitting of `content.json` I think you could use the \"import\" directive: look at some example with user data, these usually import another file named content.json that is in the folder `data/users`. You could use the same syntax to import files in the same directory with different names e.g. `optional-1.json`, `optional-2.json`, etc.\n\nBe aware that I'm not sure if optional files are searched in all the imported jsons or just in the content.json.\nOnly someone that knows very well the code like @nofish could answer.",
				"added": 1542489956
			}
		],
		"1542531928_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": [
			{
				"comment_id": 38,
				"body": "@leftside Preventing to serve other peers' requests is currently not implemented I think, to avoid having lots of people that just updates their own version without helping the site spread. If you are interested in a site, you should help the creator and sustain such a site.\n\nThe only thing you can do about it is to put the site on Pause (click on the three buttons right to it in ZeroHello): it will stop receiving and propagating updates to others.\n\nNote through that is unlikely that the CPU burden is caused by serving other peers' requests for files, and if you are following only text-based sites (where people only sends small amount of text) the updates should be in the kilobyte range so that shouldn't be troublesome for any existing electronic device.",
				"added": 1542533456
			},
			{
				"comment_id": 39,
				"body": "> [leftside](#comment_339_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): I think this can be achieved by making the same use of ZeroHello's user-focus oriented screen updating mechanism. Based on the user's focus on ZeroNet tab or window, zeronet.py process would be halted or resumed.\n> This doesn't hurt the ZeroNet's data share spirit much isn't it? :)\n\nWell, in this way you are sharing ZeroNet content only when you are looking at it. This has the exact same problem as before, it is only a little subtle: let's pretend you only watch ZeroNet tabs for about twenty minutes per day (I think it is an abundant estimate, given how much content ZeroNet currently has), so you only help spread other's messages twenty minutes per day.\n\nWhen you send a message, look at the top right border of the screen: you will see to how many peers you are sending you update (usually up to five). Each of these peers then further broadcasts your message to other five peers that may not have it and so on...\n\nIf someone sends a message when you are in listen-only-mode (so you are not rebroadcasting the message) and you are among the five peers he selects, his message will only be rebroadcasted when you look at a ZeroNet page which may happen many hours later.\n\nSo there are two downsides:\n- Messages spread slower in the network, because they are sent to five more peers only when you look at a page.\n  This is a problem only when many users chose to do the same as you.\n- There is another problem that would arise, even if single persons would chose such an option:\n  Until you look at a page, *you* are not sending the update to other peers, so that you get only benefits in part-taking in the network since you are not consuming any more bandwith than what is really necessary for you to get the updates. In other words, when you wake up and try to send updates to other peers, they would already have their copy because the spreading process went on while your computer was \"sleeping\".\n\nLet me add just add that the zeronet program on my pc consumes from 0% to 0.1% of CPU time on a lowest-end computer, so that probably the cause to you high CPU time is another one.",
				"added": 1542546766
			},
			{
				"comment_id": 40,
				"body": "> [leftside](#comment_341_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): Good point. Thank u for ur kind explanation.But actually I'm rather concerning about SDD wear-leveling while turning on the zeronet.py .What do u think about this?\n\nWell, sending files to others obviously does not wears you SSD since it only needs to read files.\nOn the other hand having content distributed in lots of little files, each one in its own folder, isn't very hard-drive friendly. On each update you have to rewrite the actual file containing data plus the content.json for that user, and then import such changes in the database file, so the hard disk does indeed some work.\n\nI think that this point (wear-leveling) is something to be addressed by development of ZeroNet.\nUnfortunately at this time there is only one real developer for the whole ZeroNet, so that development in general focuses on correcting bugs or on other urgent topics, and is not concerned with mid-term or long-term problems IMHO.\n\nThese are the kind of problems that are better solved when designing the software and so they require very careful thinking and planning at the beginning, because changing such basic functionality when it is already started requires much greater efforts.\n\nNothing to blame nofish for, because had he thought about it for as long as it takes to address such mid-term problems, ZeroNet would be in the planning phase yet. I guess such improvements will be realized when the codebase gets more easier to read and get a proper documentation as to attract possible \"code donors\".\n\nAs of now I don't know of any practical / fast / tricky way to circumvent wear-leveling.",
				"added": 1542555369
			},
			{
				"comment_id": 42,
				"body": "> [leftside](#comment_342_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): [...] I'm thinking of continuous tailing structure rather than the current in-the-middle-updating structure. For the tailing structure, it would behave like the old read-only CD-ROM's continuous tail writing mechanism by flagging DELETE for the old files if the user wants to assume it's deleted or updated. Of course this is also like mbox structure. [...]\n\nThis is indeed an idea. I think you should open an issue on ZeroNet github to let others interest in it and maybe someone will code a similar idea.",
				"added": 1542624343
			}
		],
		"1542598456_1LFfcgghoJPCrsJNw4rseWS1kSBthNKtaj": [
			{
				"comment_id": 41,
				"body": "In tor mode = always you are only talking to tor peers. Some of them will have tor mode = enabled which allows them to talk with non-tor peers, so if some of those people download you site, they can then allow clearnet peers to get it.",
				"added": 1542624098
			},
			{
				"comment_id": 43,
				"body": "> [caryoscelus](#comment_87_13oRBYqNeUr6Tvgt4KkAT9FT4XRiKFBjnE): [...] I think what happens is that tor nodes can initiate connection with tor-supporting nodes and nodes with open port, while non-tor nodes can't initiate connection to tor-only nodes. [...]\n\nI don't think I understand: if I have tor=always I can nonetheless speak to nodes with tor=no but open port?",
				"added": 1542635127
			}
		],
		"1542675872_13fF8ReM7zvX4ytjPkwEaRtBbtFKeho7XS": [
			{
				"comment_id": 44,
				"body": "@shibeotp Actually on this ZeroTalk, nofish checks if someone is close to their limit every two weeks and raises its limit accordingly.",
				"added": 1542710002
			},
			{
				"comment_id": 45,
				"body": "> [shibeotp](#comment_12_13fF8ReM7zvX4ytjPkwEaRtBbtFKeho7XS): i've seen some threads with people who complain about this limit...and 50k as max limit, is very low for who write alot and want keep the thread and the comments.\n> so... i want ask if can i modify the source code of the Forum and made a unlimited space version.\n\nNot sure if there was a misunderstanding: there is no max limit in this forum: it is simply raised when you are near your curren maximum allowed. You will always keep the thread and the comments.\n\nThe problem with having an unlimited space for each user at the start is that someone could spam 100 Megabytes of random words the first time it connects to your forum, making it unavailable for the others (since they will have to start downloading the 100 Megabytes to have your site). This would in turn require your manual intervention to drop spammers' posts.",
				"added": 1542722944
			},
			{
				"comment_id": 46,
				"body": "> [shibeotp](#comment_13_13fF8ReM7zvX4ytjPkwEaRtBbtFKeho7XS): it's a spam protection?\n\nI think so, since there is nothing else preventing someone from posting a very large blob of content just for ruining the site for others.",
				"added": 1542798415
			},
			{
				"comment_id": 47,
				"body": "> [leftside](#comment_347_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): Indicating that notation on somewhere in this page easy to find for the new users is very important.People got easily pissed off when they face the situation. And turn their back to ZeroNet.\n\nActually I also thought so for long time before seeing some other topic in which nofish himself said such a thing. I hope I could find it again to link it here, but I failed to get it. If you want some clue of that, in the `data/users/content.json` of this forum you should find that some accounts have their limit raised (like to 100k or 150k).",
				"added": 1542798648
			}
		],
		"1543247109_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX": [
			{
				"comment_id": 48,
				"body": "Thanks for all the comments. While I have done quite a bit of work (particularly in aggregating already available informations), support from the whole community is needed to keep it going, so **please link to relevant informations in there when answering questions**.",
				"added": 1543315806
			},
			{
				"comment_id": 50,
				"body": "> [glightstar](#comment_100_14K7EydgyeP84L1NKaAHBZTPQCev8BbqCy): @anoa has also started re-making the official ZeroNet-Docs page here: https://zeronet.io/docs/ (he also added a lot of \"new\" zeroframe-commands, that were not documented before)\n\nVery useful to know and the Zero API documentation seems really well done now, it even has more than a single example.\n\n@anoa How did you acquire such informations? Looking at the source code or by talking to someone?\nHaving looked at the source code myself, I think that some plugins are just unreadable.",
				"added": 1543510200
			}
		],
		"1543348094_16VrCKQQiVZpJ6S5xJASR8LCFLv24M5bfn": [
			{
				"comment_id": 49,
				"body": "> [caryoscelus](#comment_90_13oRBYqNeUr6Tvgt4KkAT9FT4XRiKFBjnE): Yeah, i also agree that's a big issue. Is there any list of id providers, by the way? Back in the day i added kaffie's and perhaps going to add kxoid, but that's about all alternative providers i can think of.\n\nThere is now the possibility to add [Proof of work based ID (clearnet)](https://github.com/HelloZeroNet/ZeroNet/issues/1258).",
				"added": 1543399218
			}
		],
		"1543527038_1AdWXZdPUimpXoGNXUBde2CHR7gLf4onxE": [
			{
				"comment_id": 51,
				"body": "@ssdifnskdjfnsdjk @quantumworld @timelinehyperbot I think the nodes you can connect to per site are capped at 1000, so you won't see more than that. AFAIK it is not possible to know how many people are using it, but some estimates can be done basing on:\n- Number of registered IDs on ZeroID (about 90.000, so there are probably fewer than this number of people, and many may have left)\n- Number of people who ever posted on ZeroTalk (about 9.500: a more finer analysis would be who posted in the last year or so). I think we should add who posted on Chinese ZeroTalk since they may not know english and are probably  a majority of users of ZN, given their country laws.\n- Number of update downloads from Github (note this should be done considering the period between a new version release and a couple of weeks later, to give the opportunity to whoever uses it regularly to download it), but this could be an estimate from below since many peers may update basing on 1UpdateD\n",
				"added": 1543583198
			}
		],
		"1543597469_1G2W1r5Eh1M81v8sUanDq2Cj4xnaRJurgs": [
			{
				"comment_id": 52,
				"body": "What do you mean? You should only receive notification for the messages posted in the topics you are following.",
				"added": 1543599469
			}
		],
		"1543615742_1AHEQxyRG9s6owyJHShB4U4rg9GL5FMX5K": [
			{
				"comment_id": 53,
				"body": "Also the more complete [configuration page](http://127.0.0.1:43110/Config) in `/Config`.",
				"added": 1543617627
			}
		],
		"1543656691_1HbvuN84epzW1vjpRjgemxPADB6UqdZpvy": [
			{
				"comment_id": 54,
				"body": "@bitcoder You may be interested in [ZeroTuto](http://127.0.0.1:43110/1Jtjb5CU9aod4jtz8hF5NPPqFKofRSsb4x/) a wiki for newcomers that want to get started and know more about ZeroNet.\n\n[ZeroWiki - Search Engine list](http://127.0.0.1:43110/138R53t3ZW7KDfSfxVpWUsMXgwUnsDNXLP/?Page:zearch).\nBe aware that most search engines can only search by site title and description and not by page.\nWith those you can search chats, forums and file sharing.\n\nAlso you may want to see [ZeroSites](http://127.0.0.1:43110/Sites.ZeroNetwork.bit/) where a list of many sites is given.",
				"added": 1543658508
			},
			{
				"comment_id": 55,
				"body": "> [bitcoder](#comment_1_1HbvuN84epzW1vjpRjgemxPADB6UqdZpvy): I tried ZeroSites just after creating this thread. Unfortunately, for some reason I can't download the page.\n\nThis is very strange, ZeroSites is one of the most seeded site.\nMaybe you just have to wait a little bit to get all the content.\nHave you tried visiting it now?",
				"added": 1543659117
			},
			{
				"comment_id": 56,
				"body": "> [bitcoder](#comment_2_1HbvuN84epzW1vjpRjgemxPADB6UqdZpvy): Maybe it fails to download the page because I haven't port forward my router? It says \"Tracker connection error detected\" and \"Content.json download faild\" continuously.\n\nNo, port forward helps you to contact more peers and is good to download sites with few peers and to help the zeronet network.\nFor sites with lots of peers you don't need it, since you can contact one of the peers that has an open port.\n\nRegarding the tracker connection error, I don't really know what that means.\nIn [ZeroHello (the mainpage)](/) how many trackers do you have in the top right corner?\n\nAlso do other sites load fine? (Such as those I've posted)",
				"added": 1543659463
			},
			{
				"comment_id": 57,
				"body": "> [bitcoder](#comment_5_1HbvuN84epzW1vjpRjgemxPADB6UqdZpvy): Everything looks to work OK, after all I  can post here and changes are being sent to the network. In the main page it shows me 9/20 trackers.\n\nThis is really strange, try to look at the execution log when you visit Sites.",
				"added": 1543662111
			},
			{
				"comment_id": 58,
				"body": "> [bitcoder](#comment_6_1HbvuN84epzW1vjpRjgemxPADB6UqdZpvy): Hey, for some weird reason, ZeroTuto is empty. Hmm..\n\nThe only thing that comes to my mind is that you could be behind some kind of firewall that blocks your connections to some peers (the majority since many sites are not loading for you) but not others (from which you downloaded e.g. ZeroTalk).",
				"added": 1543662329
			},
			{
				"comment_id": 59,
				"body": "> [trenta3](#comment_58_14mSHL2TBcqcLNcfpmg5jyGVS9RZia3FiX): The only thing that comes to my mind is that you could be behind some kind of firewall that blocks your connections to some peers (the majority since many sites are not loading for you) but not others (from which you downloaded e.g. ZeroTalk).\n\n@bitcoder Then some scenario that comes to my mind (think before saying if you is in one of those since it could reveal something more about you):\n- You are in a corporate environment and have Tor enabled: the corportation firewall blocks your tor traffic.\n- You are in a corporate environment and the firewall is blocking IPs that are geolocalized in some other countries (and maybe that does not resolve reverse DNS) or that are not in your area.\n- ZeroNet thinks your port is open but you don't have it enabled on the router (maybe because you disabled Upnp after starting ZeroNet): many peers are trying to contact you on your router port, but obviously you get no messages from those.\n\nMy advice is: __try to restart ZeroNet__ since as any other software / hardware component resetting it to a known state may help you resolve some random bugs.",
				"added": 1543662916
			},
			{
				"comment_id": 60,
				"body": "@bitcoder If you use Linux starting a script the log is on its stdout, otherwise as @ssdifnskdjfnsdjk noted, you should find it in the ZeroBundle folder under `ZeroNet/log/debug.log`.",
				"added": 1543668332
			},
			{
				"comment_id": 61,
				"body": "I recently found [this list of sites](http://127.0.0.1:43110/siteofinteresting.bit/).",
				"added": 1543677576
			},
			{
				"comment_id": 62,
				"body": "> [vorticalbox](#comment_12_13Dnn8CFKa8NJLf4gxnJTp8sXqYqrA9U7L): Why are you using an av or an av firewall on linux?\n> The av software will likely introduce more holes for you to be attacked from and you can use iptables, there is even a gui for it, to control access.\n\nI think it is because AV are made to stop virus programs which, even if are less widespread, exist even for Linux.\nAlso there is some AV that is opensource so that it is scrutinized by many.\nUsing a firewall on own computer and router is nonetheless strongly advised if not mandatory to have, but AV can also protect you from other threats, such as for example:\n- Have you ever downloaded and executed code from a GitHub repository without caring too much to read it or control others opinions about its security?\n- Have you ever installed a minor node / python package (which in the install phase may execute arbitrary code)?\n- Have you ever downloaded a video from the internet and played it (some past threats from bug in codecs)?\n- Do you update your software daily?\n\nAnd many others... so I think it is really important to have firewall, AV and possibly also an intrusion detection system.",
				"added": 1543714195
			}
		],
		"1543862269_1AdWXZdPUimpXoGNXUBde2CHR7gLf4onxE": [
			{
				"comment_id": 63,
				"body": "I only know of [ZeroVerse Mail](http://127.0.0.1:43110/mail.zeroverse.bit/): the interface is the same as ZeroMail.\nUnfortunately as of yet there is no way to exchange mails between different services, so if you have a ZeroVerse account you won't be able to write to someone who only has ZeroMail.",
				"added": 1543863662
			}
		],
		"1540051200_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di": [
			{
				"comment_id": 64,
				"body": "@gitcenter Shouldn't then ZeroNet come up with something to cope with the last part of that bug?\n\nFor example:\n- When serving files to the user, always first check they have the same sha as what is currently displayed by the content.json (and also verify the content.json sign). This would prevent future local modifications of the 1Update site.\n- Add a permission that allows a site to get the sha of the python files inside the ZeroNet folder.\n  This way a site could be created that allows one to check if he has untampered sources of a specific ZeroNet revision or if some of its files got corrupted / modified.\n\nIt does not seem enough to me to simply check the \"html\" extension in all cases: since many vulnerabilities were found, shouldn't each one of them be \"patched\" in some way or made more complicated to exploit?",
				"added": 1543953529
			}
		],
		"1543914606_139zVkd4AsZqs9qSYk6ypPDAqDcjPCmRm4": [
			{
				"comment_id": 65,
				"body": "> [ornataweaver](#comment_99_139zVkd4AsZqs9qSYk6ypPDAqDcjPCmRm4): For example I hadn't heard of this wonderful site before:http://127.0.0.1:43110/14pM9huTYzJdyQyHRj6v2kfhMe8DrxwpGt/It's \"ZeroNet Dev Center\"\"Tutorials, Questions, Collaboration\"\n\nThat site only has three tutorials and they were the same since I first visited it some month ago.\nThe site really is well done and looks good also, but has virtually no content.\nMoreover, one cannot add its own.",
				"added": 1544173153
			}
		],
		"1544189569_13pMUCyHCSA16YYxki54Zh8FCv6qL18pcP": [
			{
				"comment_id": 66,
				"body": "> [leftside](#comment_405_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): That's a clever solution!But how can we know a user is the same user who visited yesterday without signed in?\n\nYou could have a javascript that autogenerates a certificate for your same site as [Nanasi](http://127.0.0.1:43110/16KzwuSAjFnivNimSHuRdPrYd1pNPhuHqN/) does and then have they post (or update) their data.json each time they visit your site, so you can also know how many users are online.\n\nNote tough that this comes at a very high espense for peers seeding your site, since you have them update their disks every time someone of them come to visit you, so please don't do it.\n\nIf the PeerMessage plugin will ever be merged into ZeroNet, one could use it to let the site peers message themselves when they visit your site and remember the statistics that could be sent back to you when you connect.",
				"added": 1544198350
			},
			{
				"comment_id": 67,
				"body": "> [caryoscelus](#comment_93_13oRBYqNeUr6Tvgt4KkAT9FT4XRiKFBjnE): That's ineffective. Most effective is just ignoring such sites, next best thing is reading it through raw db view interface ;)\n\nIndeed it would be nice if ZeroNet allows to access informations also in other ways (e.g. reading blogs with a command line program or something like that).",
				"added": 1544225689
			},
			{
				"comment_id": 68,
				"body": "> [caryoscelus](#comment_94_13oRBYqNeUr6Tvgt4KkAT9FT4XRiKFBjnE): That's actually easy.\n\nYeah, what I meant is something from inside ZeroNet itself, so that it would be easy to post and to also see comments of the posts easily. Maybe some kind of sandbox to execute some python code that only is allowed to use ZeroNet API commands and to write / read to stdout / stdin or something similar, but is obviously utopic.\n\nEdit: I'm thinking something using `urwid` (a python package) to create a command line UI and be able to do actions.\nIt could be that each site points to the main python script that gets executed in a sandboxed environment when using the command line interface.",
				"added": 1544262128
			},
			{
				"comment_id": 69,
				"body": "> [caryoscelus](#comment_95_13oRBYqNeUr6Tvgt4KkAT9FT4XRiKFBjnE): Python cannot into sandboxing. And if you're willing to just run some code in os-level sandbox instead, it doesn't have to be python. [...]\n\nYou can always build an [interpreter for python written in python itself](http://www.aosabook.org/en/500L/a-python-interpreter-written-in-python.html). It seems not so hard to do and there is the byterun project from where to start.\nBy interpreting it you are not just changing the behaviour of some globals, you are effectively deciding which external functions the program is able to call.\n\nThen you could:\n- Get the python code source\n- Compile it into bytecode\n- Execute such bytecode into your virtual machine\n- If the code calls some instruction that he is not allowed to do, raise an Exception / ask the user if he consent\n- If the code tries to load other libraries, simply get their python source and start executing them in the VM.",
				"added": 1544265461
			}
		],
		"1544245110_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 70,
				"body": "Instead of crawling zites, I think we should focus on creating a Plugin that enables distributed search (in all websites). There are some design that do not need to send the query to every peer, but just scale logarithmically with the size of the network.",
				"added": 1544265615
			}
		],
		"1544353154_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 71,
				"body": "I've thought about such search plugin a lot and I have a fairly detailed way to do it BUT it is really long and I have no time to implement it, so I will update the description below when I have more time to do it.\n\nFirst some thing one would likely want:\n1. Be able to search results in all available sites\n2. Be served only valid results for its search (i.e. no fake content)\n3. A system that scales well with the size of the network (so no query flooding)\n4. Other peers should not be able to gain informations about which sites we have locally or which search are we performing\n\nNote that in order to get (2) it is necessary to change the way (user data) json files are signed. A possible solution would be to sign them as merkle trees, so that with the global sign and the hash-path to the required node, an authentication of the content could be provided to some other peer.\nThis is not completely safe, since a peer could create some fake content and sign it with its private key and then it could say to you that that data is actually published on some site, while it is not.\n*This issue should be further investigated, in particular we should see how does ZeroNet be sure that some content is published under some site*.\n\n## The system in brief\n- Each node (or group of nodes) is responsible for some keys / search terms (similar to a DHT).\n- In a search we issue multiple queries for our search terms to the peers and we ask them to return the best 50 results for our query (more on that later) (note that intersection of results must be carried out in the peers themselves to minimize amount of information transferred).\n- When we have the best 50 results (for now just references to some piece of data) we ask other peers to return to us the whole pieces of data (e.g. actual comment / topic) and we then validate their authenticity.\n  If someone served us some fake data we add some ban point to it and we ask him to do the same to the peer that served that result to him.\n- The search should be done on database rows and not by crawling web pages, because it is waay more faster and we don't have to store redundant information (more on that later).\n\n### How to ensure a good scaling for most searched terms / results?\n- Each query result should be cached locally to all peers through which it passes, with a sort of LRU cache.\n  This way we ensure that:\n  - In case someone wants to localize the peer that has some terms to shut it down, by making a query (which is necessary to localize it) it just helps to spread the information further.\n  - In the case of a term that is very common in queries we are caching it in every peer that gets the request.\n\nPlease note that query routing should be deterministic to ensure that caching is really relevant.\n\n### How to ensure point 4 (No information gain)?\n- The protocol must ensure that in a search the exact same message is sent to another peer both if you are the initial searcher or if you are just propagating / routing the search of another peer.\n- The protocol must also ensure that the data insertion has the same command as its forwarding.\n\n### Getting data from the database\nSome sort of [Document Unit specification](https://github.com/HelloZeroNet/ZeroNet/issues/1513) should be enabled on the sites that we want to be able to search. I'll add some details on that issue later.\n\nThis enables a site to specify exactly which data should be indexed (imagine in NullPaste the encrypted content should *not* be indexed)\n\n### Actual processing for insertion\n- Get the document to be inserted\n- Recognize the language in which it is written (for example basing on trigram analysis)\n- Use a tokenizer and a stemmer for such language to get the stream of base-words\n- Calculate the tf-idf index score for each word and get the most relevant (for example the most relevant hundred) of terms\n- Issue an insert query for each of those terms\n\n## Some Problems\n### What to do when data is changed?\nFor example, if a ZeroTalk comment is changed, we should invalidate the old data.\nIt is not yet clear to me how to do this. One could let the data expire after some time, but this will increase burden on data reinsertion which uses lots of resources.\n\n### Who is responsible for the data to be inserted into the index?\nSome options:\n- The peer who post the comment (but he could not do it)\n- Every peer at random reposts every bit of data if he can't find it (very resource expensive)\n- Every viewer of the site when the comment is first posted (this seems the most doable).\n\n@blurHY Let me know if it is understandable and feel free to ask more about details. I'll be very glad if someone actually implements it.",
				"added": 1544391037
			},
			{
				"comment_id": 72,
				"body": "> [blurhy](#comment_201_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK): We can ensure the data is not modified with merkle tree.How can we know the order of datas are not faked.I searched on wiki.None of the p2p search engines (clearnet) could prevent faked content.Maybe It's possible on zeronet that the content has signs\n\nOne should look at the data that ZeroNet stores in Torrent Trackers and probably download the content.json of that data to be able to validate it...",
				"added": 1544437329
			},
			{
				"comment_id": 73,
				"body": "> [ssdifnskdjfnsdjk](#comment_543_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS): Maybe the proposed way (to gather search results from any peer) is too ambitious. (because of no effective way to handle fake results, bad peers - Human user manually moderated blacklist (single point of failure) is very weak tool against b0tnet)\n\nAs evident from the current status of ZeroNet itself we are not caring much about spamming / fake result and we don't have any sort of spam mitigation (except **MANUAL** blocklist), so I think this is a not really an argument.\nWe should certainly do something with this regard, but we may postpone it when we have some working and useful search.\n\n> [ssdifnskdjfnsdjk](#comment_543_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS): And better would be improving existing ZeroHello searcher to return the results (based on local files) faster?\n\nFaster than that is not really possible since ZeroNet is just asking the sqlite databases with the data inside to do the search. After the transition to Python3 it should be possible to enable Full Text Search on local database that should speed that up a lot.",
				"added": 1544452555
			}
		],
		"1544639635_1Dd32hiiWqzaa3mzA21af2VneXPJdzEbRr": [
			{
				"comment_id": 74,
				"body": "Each of the people who have downloaded (accessed) ZeroHello has its own local copy (even you), so it cannot happen, because from now on you will have that exact site in your computer folder. Other peers are only necessary when updating the site in order for the update to propagate, but they do not influence if the site works or the content that are already stored in it.",
				"added": 1544641002
			},
			{
				"comment_id": 75,
				"body": "> [ranx](#comment_1_1Dd32hiiWqzaa3mzA21af2VneXPJdzEbRr): Yeah, that works if you already downloaded the site. But if you just installed ZeroNet and there is no peers?\n\nAh, in that case I think that yes, you won't be able to manage your sites...",
				"added": 1544641704
			},
			{
				"comment_id": 76,
				"body": "> [quantumworld](#comment_59_1AdWXZdPUimpXoGNXUBde2CHR7gLf4onxE): So what is the best solution to run your own site if you half to do a computer shut down or go off line for a short time. Ask for people to clone your site? (Mirror ) Clone does mean mirror correct?\n\nNo, if you want your site to be always online even if you shut the computer the best solution is to ask to other people to **visit it**. By visiting they will have a local copy of *your own site* and will help to spread it.\n\nIf they clone it on the other hand, they will have their own copy (i.e. no sync between the two sites) and won't contribute to spread yours (i.e. they are not necessarily peers of your own site).",
				"added": 1544648357
			}
		],
		"1544706846_1GHXQ3vMtadfHFjTwS3roqYe8LUsyDqN8f": [
			{
				"comment_id": 78,
				"body": "Yes, it can happen. You have to be careful which sites you do visit. If you recognize you have been on a site that you didn't want to, you can go to ZeroHello page and remove its content (through traces may and will remain in your hard disk).\n\nOn the other hand many pedo sites (as i've been told) strongly advertise themselves as to contain CP. Personally I've seen such advertise only a couple of times here in ZeroTalk.\n\nIf you want to move on a safe ground then you can visit only sites that are well known and enable some of the porno and pedo blocklists (there is one by @styromaniac but I don't remember the sitename) which will warn you when you visit some of these potentially illegal sites and will prevent you from downloading them.",
				"added": 1544709506
			},
			{
				"comment_id": 79,
				"body": "> [0cat](#comment_1_144YkJsD7Ruj19po58bF8uSo8cJChYTeat): Yes, it is possible. sorry bro :(You cant create a virtual Disk with Veracrypt and execute ZeroNet inside this disk. No one can access to your veracrypt container without password. Also veracrypt can create hidden volumes inside normal volumes and you can enter with 2 different paswords accessing 2 different files (normal and hidden)\n\n@hypercode Before doing that please inform yourself if it is legal in your country to not disclose the keys: [Key disclosure law, on wikipedia](https://en.wikipedia.org/wiki/Key_disclosure_law).",
				"added": 1544710642
			},
			{
				"comment_id": 80,
				"body": "@hypercode In some countries (even if very few) also encrypting content is a crime. In many more encrypting content and not providing the encryption keys when asked to by a public officer also is a crime. In this last case you can have all the site content encrypted and then when your disk will be seized, you will be asked to provide the keys to decrypt all the content, so the effect is (legally) **the same** as not having any kind of encryption, with the added trouble that, should something require a password that you don't have, you incur in more legal penalties.",
				"added": 1544739004
			},
			{
				"comment_id": 81,
				"body": "> [hypercode](#comment_7_1GHXQ3vMtadfHFjTwS3roqYe8LUsyDqN8f): without mentioning that having a handful of data that you do not have access to (and because zeronet is opensource it's easy to prove that the person does not have access) it's much less serious that having child pornography or piracy recorded on your disk\n\nThe problem with this is that if you would still like to access a single site without entering any password (like you do today on ZeroTalk), then no matter how securely the content is encrypted on disk, there has to be on disk also the key to decrypt it, and with that key you can also decrypt the CP content stored in it, so I don't see this as solving the question: maybe it will make it harder to find for the police, but when they see that the content is stored in a folder called ZeroNet, it is a very simple job (after searching a bit online) to see where the key is stored to decrypt the content and then to see the files.",
				"added": 1544776110
			},
			{
				"comment_id": 82,
				"body": "> [hypercode](#comment_8_1GHXQ3vMtadfHFjTwS3roqYe8LUsyDqN8f): I think you have to have a password to start the service, I see it as an optional native option.\n\nOk, now it makes sense to me: each one of us has a password with which all the content on his disk is encrypted, and he enters such a password at the start of ZeroNet. That makes sense, but having a global password per site is not, since this could allow everyone to decrypt that content.",
				"added": 1544884227
			}
		],
		"1545035786_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS": [
			{
				"comment_id": 83,
				"body": "You can read something more in the [ZeroNet for developers Wiki](http://127.0.0.1:43110/18hFUweFE3AJwZWnaMeq3fJMobrq8jevfd/?Page:write-your-own-zerotube) about how to use the BigFiles feature to allow users to upload big videos as optional files.\nIn your case (if you want just the owner to upload videos) it only adds little: the bigfile treatment. In this way the video data will be split in many 1MB chunks that would allow the other users to download it more fastly and start distributing the pieces they already have to other peers.",
				"added": 1545037428
			}
		],
		"1545423165_13CFz29UuNbUx54HmLqQBfxPQNN2FmfdAq": [
			{
				"comment_id": 84,
				"body": "Just a comment: @zeronetisshit SQLite already supports Full-text search using virtual tables: [https://sqlite.org/fts5.html](). It is not built-in in the SQLite that ships with ZeroBundle, but once moved to Python3, it should be doable.",
				"added": 1545491163
			}
		],
		"1545454147_19AR8ndYAaE6Q2yRAcpTKmkwtyJ5ETPhMC": [
			{
				"comment_id": 85,
				"body": "Also remember to delete your current `data` folder and create a new ZeroID, since the old one can be traced back to you. Also remember to create it using BitMessage, to not send data to the clearnet.\nYou might want to see [this short article on DeveloperWiki](/18hFUweFE3AJwZWnaMeq3fJMobrq8jevfd/?Page:be-completely-anonymous) to make sure you are doing things right.",
				"added": 1545506992
			}
		],
		"1546939398_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 86,
				"body": "> [blurhy](#comment_223_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK): OK.It implements bittorent itself\n\nNo, it only uses bittorrent trackers to trace which peers have which sites, and then [it uses a custom protocol](https://zeronet.io/docs/help_zeronet/network_protocol/) to transmit files. Bittorrent is a protocol of its own and it is different from that of ZeroNet.",
				"added": 1546943445
			},
			{
				"comment_id": 87,
				"body": "> [randomshit](#comment_8_18wj1JzTXc2gNiG4MnUvToPyJkB4e6P79r): Bittorrent technology is the protocol concept of grabbing data from a swarm of peers.Zeronet technology was custom built to function on the concept of grabbing data from a swarm of peers. [...]\n\nOf course, I wrote it just not to create other confusion: [Bittorrent is a specific protocol](http://www.bittorrent.org/beps/bep_0003.html) and is different from ZeroNet.\nThe idea is of course the same (to share files between peers), but if someone asks if ZeroNet is based on Bittorrent the answer is that it is *not*.",
				"added": 1546977111
			}
		],
		"1547669338_131Seivv6aH8vnohPmYPcPwPfZ1a5ubMMs": [
			{
				"comment_id": 88,
				"body": "Ok, I will list some possible names, even if it is indeed difficult to name such things.\n\n5: Feed Types Bar (?)\n6: ZeroHello Search Bar (I think this is obvious)\n7: NewsFeed / Feeds\n8: Home (?) Button (since it is there in every site and brings you to your main page)\n13: ZeroHello Menu (?)\n14: Site Lateral Bar / Site Statistics Bar (?)\n\n*Edit*: sorry I noticed that Markdown formatting had changed my list's numbers. ",
				"added": 1547722046
			}
		],
		"1549527151_1GXJzsEWCBdesJY5BtKgJXFQLuMoVPD6ed": [
			{
				"comment_id": 90,
				"body": "I think you just got a permanent block for spam from whoever reads this and all the following messages. Bye!",
				"added": 1549533399
			}
		],
		"1549554952_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": [
			{
				"comment_id": 91,
				"body": "@leftside Does this just count words / unique words?\n\nA spammer could employ other techniques to generate high volume of spam (i.e. nonsense text generated by randomly extracting words from some dictionary and with posted times set every 10 minutes would overcome both this defence and flood the newsfeed with unwanted random good-looking threads) and I guess that ultimately only some sort of Web of Trust (i.e. amplified human intervention) can adequately solve such spamming issues, maybe along with CAPTCHA-based certification providers.",
				"added": 1549626686
			}
		],
		"1553362552_1AdWXZdPUimpXoGNXUBde2CHR7gLf4onxE": [
			{
				"comment_id": 92,
				"body": "For developers I'm taking care of a Wiki with some code snippets and short tutorial about common things: http://127.0.0.1:43110/18hFUweFE3AJwZWnaMeq3fJMobrq8jevfd/",
				"added": 1553455042
			}
		]
	},
	"comment_vote": {
		"339_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS": 1,
		"330_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ": 1,
		"331_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ": 1,
		"255_1Pv5CdXMDJm5k5iAp4SVbutnMg2ue8e58p": 1,
		"88_1KuwwnkAtVrfnyhv8pyGRxLqzNZbEkJt2u": 1,
		"274_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"292_1FzZsDQdh8wk71bDLxURPzmAt2FXehoNAB": 1,
		"1_1N76dkP8zdrNRETTYcs4CtxYdCNGcp6iqK": 1,
		"8_1Q2noE3qu7ccvEspqWWv4xkMktiqp6mhdK": 1,
		"73_139zVkd4AsZqs9qSYk6ypPDAqDcjPCmRm4": 1,
		"314_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"27_191jbK3GNmVritcMymKkFuxyYFXxqv2xpC": 1,
		"11_13Dnn8CFKa8NJLf4gxnJTp8sXqYqrA9U7L": 1,
		"516_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS": 1,
		"2_186JDoRvixr4EUx2EUc8dGW5opZbWi9SzK": 1,
		"167_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS": 1,
		"347_1AHEQxyRG9s6owyJHShB4U4rg9GL5FMX5K": 1,
		"8_19r2tS5GqeMQnxmyPq9ufD79LXoMVDszDU": 1,
		"643_1L4dZcDF2maSKHDy788yhxpYnBWnXadUtS": 1,
		"464_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd": 1,
		"86_131Seivv6aH8vnohPmYPcPwPfZ1a5ubMMs": 1
	}
}