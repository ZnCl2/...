{
	"next_topic_id": 3,
	"topic": [
		{
			"topic_id": 1537592624,
			"title": "Linux developers threaten to pull “kill switch”",
			"body": "https://lulz.com/linux-devs-threaten-killswitch-coc-controversy-1252/",
			"added": 1537592623
		},
		{
			"topic_id": 1537594142,
			"title": "Quick Bash Scripts",
			"body": "Simple and Quick Bash scripts to make things easier",
			"added": 1537594140
		}
	],
	"topic_vote": {
		"1534470841_1Kh3qLdTH2QKewUb6cWZa2dMveMT5EcVer": 1,
		"1537594142_1CthCNo2eADyWkShCTBTVKGHy3WNXcj6Fh": 1
	},
	"next_comment_id": 15,
	"comment": {
		"1536480860_1DoVPxFrQCDb3grC8Nsn6aACed9o2oiumX": [
			{
				"comment_id": 1,
				"body": "> [styromaniac](#comment_82_16NS3rBdW9zpLmLSQoD8nLTtNVsRFtVBhd): User submissions aren't uploaded on any ZeroNet site. The site only lists the file. ZeroNet points to the IP addresses and port numbers of users who have the file chunks where a connection is possible and sends copies of the chunks to peers requesting the file while both are online. The peer who clicked to download the file isn't a seeder until they have all chunks of the file. [...]\n\nthe link given by @robione appears to be a file that i seeded.  my connection is not always on, but it shows to have 12 peers at the moment.  i don't know how much of the file they have, but it's not a big file.",
				"added": 1536712429
			}
		],
		"1537594142_1CthCNo2eADyWkShCTBTVKGHy3WNXcj6Fh": [
			{
				"comment_id": 2,
				"body": "This script finds an mp3 modified on the system in the last 24 hours and converts it to a new codec\n\n#!/bin/bash\nfind . -mtime -1 -type f -iname \"*partialfilename.mp3\" -printf \"%f\\n\" | cat > read1\n/usr/bin/avconv -y -i ~/$(cat read1) -acodec libmp3lame   -b:a 64k -ac 2 -ar 44100   ~/path/to/$(cat read1)",
				"added": 1537594241
			},
			{
				"comment_id": 3,
				"body": "Remove spaces from filenames in a folder: (i used this for automation)\n\n#!/bin/bash\ncd ~/path/to/folder\nrename \"s/ //g\" *",
				"added": 1537594857
			},
			{
				"comment_id": 4,
				"body": "this script used to work for me.  yes it's crude, but i was young.  lets make it better",
				"added": 1537626942
			},
			{
				"comment_id": 5,
				"body": "> [stretchedpup](#comment_6_12H7vawHrBcyhkGXZYTJKPiboSKaAXyKw6): Why not use a semicolon, run it all in one line and never create a file for this script. Well, I suppose someone might find this script useful enough to save it as a file.\n\ni had this running from cron.  it would go into a specified folder and remove the spaces from the filenames",
				"added": 1537627162
			},
			{
				"comment_id": 6,
				"body": "This is an example of the crontab i had set up to download a show from blogtalkradio and dropbox, then find the new files, remove spaces in filename so as to not cause errors, then re-encode them to reduce mp3 file size.  Then re-uploading to my own podcasting server, then update the feed with cronjobs.  i know it's crude and sloppy, but it was my first go and it worked.\n\n#!/bin/bash\n30 2 25 2 * wget -O - http://www.blogtalkradio.com/user/YYYY/MM/DD/filenameonsite.mp3 > whateverfilenameyouwant.mp3\n.* 3 * * Sat ~/bin/dropboxuploader.sh download /Observations (dropboxuploader provided by Andrea Fabrizi <andrea.fabrizi@gmail.com)\n.15 3 * * Sat ~/bin/avconvautomator.sh\n.* 4 * * Sat ~/bin/removespaces.sh\n.05 4 * * Mon lftp username:password@mysite.com  (for this i had all my specific config in .lftprc)\n.05 5 * * Mon wget --spider http://mysite.com/btr/podgen/pg-cron.php?key=q1yfun0x\n.01 5 * * Mon wget -- spider http://mysite.com/ofot/podgen/pg-cron.php?key=7dttzxiw\n",
				"added": 1537635879
			},
			{
				"comment_id": 7,
				"body": "> [leftside](#comment_285_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): $ find without -maxdepth 1 option  could lead to avconv error like could not find the folder ***/***/ if there's *partialfilename.mp3 in a subfolder\n\nfind seems to work well with wildcards.  If there were many mp3's in the folder, but i wanted ti to find the ones in a certain time period with certain characteristics in their filename, it seems to work.  Should it not have?",
				"added": 1537636420
			},
			{
				"comment_id": 8,
				"body": "> [leftside](#comment_286_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm): @thornranger , If your recently downloaded .mp3's to be located in a restricted folder, it's OK without -maxdepth 1 option. But if u can choose the folder to be downloaded, why do you need to limit the timestamps of the .mp3's? Every .mp3 in the very folder is the one which needs to be converted and moved to the other folder, isn't it?\n\nnot real sure why i did some of the stuff i did.  seems redundant now that i look at it.",
				"added": 1537636925
			},
			{
				"comment_id": 9,
				"body": "what about this jewel :)?  This was a context menu i put together.  should have just used gpg.  anything i upload to a cloud, i want encrypted.  my goal was to right click the file, tar it, encrypt it, upload it to g..gle drive or some such then delete the original tar, all in one job. looks like i'm missing the gdrive upload command.\n\ntar -czvf %n.tar.gz %N && openssl enc -k yourpasswordhere -aes-256-cbc -salt -in %n.tar.gz -out %n.tar.gz.enc && rm -f %n.tar.gz",
				"added": 1537637736
			},
			{
				"comment_id": 10,
				"body": "I havent used any of these scripts in a while, but now that you mention it....it does seem that i had problems when it started trying to run avconv.  i may have fixed the error in a later version",
				"added": 1537637757
			},
			{
				"comment_id": 11,
				"body": "I would like to use this to save man page as pdf. can someone give me some context?\nman -Tpdf man >man.pdf\nfor example to output the manual for openssl to a pdf",
				"added": 1537643613
			},
			{
				"comment_id": 14,
				"body": "> [robione](#comment_53_1DoVPxFrQCDb3grC8Nsn6aACed9o2oiumX): you might like commandlinefu.com\n\ni used to have them on my twitter feed when i was on twitter",
				"added": 1538234100
			}
		],
		"1537592624_1CthCNo2eADyWkShCTBTVKGHy3WNXcj6Fh": [
			{
				"comment_id": 12,
				"body": "[A Plea to Unfuck our Codes of Conduct](https://lkml.org/lkml/2018/9/20/413)",
				"added": 1537666448
			}
		],
		"1537649547_1b9urVTLtAAZAidg6hchWqhrVtYXUaoj7": [
			{
				"comment_id": 13,
				"body": "Couldn't we just avoid having someone else lord over our content by cloning a zite?  Also, I thought all of our content was hosted on our own computers and those sharing/and/or seeding our site?",
				"added": 1537667617
			}
		]
	},
	"comment_vote": {
		"282_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"281_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"283_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"287_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"288_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"289_16dEowtNbV89NxPcDrAKkmnuXtiJVoVcDm": 1,
		"53_1DoVPxFrQCDb3grC8Nsn6aACed9o2oiumX": 1
	}
}