{
	"next_topic_id": 1,
	"topic": [],
	"topic_vote": {},
	"next_comment_id": 8,
	"comment": {
		"1569600536_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di": [
			{
				"comment_id": 1,
				"body": "Something tells me whatever you're planning to do is a terrible idea and the only reason you aren't ready to explain it is you need to compile your little list before your thread gets a hundred replies telling you how terrible your idea is.\nI mean, lets be honest. What is the reason you would need to build a list of names that you consider trustworthy? It obviously involves giving those people some kind of power. We don't need more people to wield power over ZN. What we need is true decentralization, not federation. Just look at Mastodon to see what the\n>This is a decentralized project!!!11 (except it's only decentralized among my friends teehee)\ndoes.\nNo subset of users should have the power to fuck it up for everyone else. Not nofish, not the developers, and not the people in your list. Stop it with the bandaids and push for true decentralization. Everything should be automatically forked by default, so that if some instances of something stop working, it's as easy as possible for the remaining users to only consider those that keep working. And the software should be as flexible, modularized and simple as possible to change how it works without having to make major changes to the code. Just how Firefox can be turned into Tor Browser by changing a few settings. I realize this is vague, but if I don't what you're trying to do I can't offer any specific criticism.\nIn any case your list obviously doesn't do anything to make this a reality, since if your project involved true decentralization you wouldn't need to make the list in the first place.\nAll of this is in part a cultural problem. ZeroNet should be the brand for the system, not for the client. Think of how you can't download \"Linux\" from linux.org. You can download Debian or Arch, but Linux is just the codebase, not the distribution.\nFor CAs, the trustworthiness of peers should be determined automatically by the clients (for example, through seed ratios or PoWs) or manually by the user, or through subscribable whitelists provided by other users. But it is also a code problem. The owners of multi-user sites shouldn't be able to determine what CAs can post on their sites. Users should be the ones making the call on what posts/data they want to download and see from what peers according to their preferences for the CAs who generated the peer's certificates. The certification system should be based on web of trust and not plain old centralized (or federated with manually picked peers) CAs. Users should be the ones making the call on whether they want to see posts from old users who've built up trust, new users who might be spammers, or anything in between. New IDs should be able to be easily and always successfully generated from behind Tor as often as needed to maintain a certain level of anonymity, and trust should be built up from then on, not a-priori (within reasonable parameters, but a single person should be able to generate multiple IDs per day no problem). Truly distributed CAs, rate limiting, duplicate name prevention and captchas aren't mutually exclusive things like you seem to think. You just need a web of trust system in place and to have some criteria for people being able to become a peer automatically when those are met.\nAnd again as I said, development truly decentralized with multiple forks/branches/distros being actively used by people.\nAs ZeroNet currently stands, it's almost unusable, at least for me. I hate people being able to link all my opinions back to a single identity, partially because I want my opinions about different topics to stand on their own, but also because it'd be very easy to deduce my real life identity after knowing every little thing about my life, every obscure hobby, opinion, thing I own etc. that I'd like to talk about. I realize it's hard to maintain strict anonymity and fight spam at the same time, but I think a better job could be done while still maintaining some balance, and I think people should be able to decide for themselves what do they want this balance to be, not the ZeroNet developers (which all seem not to care about anonymity at all and are happy to be pseudonymous or even reveal their real life names).\nOh, and another thing. If pseudonimity is going to be mandatory, at least make it easier to switch between multiple IDs without having to fiddle with files and restart things and so on just to switch IDs. That would make it much easier to compartmentalize whatever information is disclosed about oneself.",
				"added": 1569662264
			},
			{
				"comment_id": 2,
				"body": "> [glightstar](#comment_165_14K7EydgyeP84L1NKaAHBZTPQCev8BbqCy):\n>Also, I disagree that a single person should just be able to generate multiple IDs per day\nWhy? In the system I'm proposing, if you don't consider data from people who create multiple IDs per day worth downloading or seeing, you could set your client to not accept certificates from the CAs who allow you to create multiple IDs per day, or only download data from users that have been around for X number of days, etc. There are many ways to implement this. The \"likelihood of seeing spam vs anonymity\" balance doesn't have to be the same for all users.\nOf course it isn't easy, and I'm not suggesting all of this can be done in a week, but as far as I know, there aren't any plans or publicly stated intentions to do all of this in the short term OR in the long term. The prevailing attitude (at least from gitcenter) seems to be that these things are just plain impossible to implement in a practical way, and nofish just seems to be happy with the way things currently are.",
				"added": 1569672335
			},
			{
				"comment_id": 3,
				"body": "> [gitcenter](#comment_1112_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): \n>This looks like a troublesome idea to me -- mostly because of its implementation. Not all peers now host the whole site, they only host some parts of it. And if the fractions are going to be large, finding the peers hosting what you want will become difficult.\nWith that logic, muting shouldn't be a thing either. And on sites that host media like Millchan, by default users only download content that they click on, yet things work just fine.\n>As for your Linux comparison: who develops Linux?\nThe Linux Foundation, the X.Org foundation, GNU, distro maintainers and a bunch of other people.\nOf course I'm referring to what people normally call \"Linux\". The OS, not the kernel. But even if I was referring to the kernel, the analogy still works. Distros package their own kernel build after applying whatever source patches they deemed useful. Nobody actually downloads the kernel from linux.org except people who want to distribute to other people.",
				"added": 1569672762
			},
			{
				"comment_id": 4,
				"body": ">[gitcenter](#comment_1113_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): Nope, you didn't get it. Not everyone, and even not most peers block users.\nIt still makes it slightly harder to find peers to download things from even if most users don't block peers. What didn't I get, that CA whitelisting would make it harder to a large enough degree as to cause issues? Well, it's not that I didn't get it, it's just that I don't agree with it. And by pointing out muting I'm implying that the issue is not whether it would make it harder, the issue is whether it would make it harder to an extent that  the system would perform worse from the user's perspective. Which is an unproven assertion.\n>Say, if at least one fifth of the peers don't block duende, it'll be easy to download his posts. But if the count of people who mute a CA grows (or those don't allow it, whatever) it'll become harder to download content.\nSo? Yes, things that aren't seeded a lot are harder to download. But we don't force users to download all of ZeroNet's sites just to use one site. And if you don't allow people to post with a certain level of anonymity, a lot of these posts won't be made in the first place, so that content is not harder to download, it's impossible to download because it doesn't come into existence.\n>As for optional files, you can't download an optional file from a gray IP if the only peer is gray as well, so we get more centralization points.\nI disagree. A torrent with just a few seeders isn't centralized, it's just not very well seeded. Centralization is not just about how many nodes are serving something, but about how easy it is to serve a node. And again, if you forced people to become trustworthy to some degree so you could reasonably force everybody to download that content, a lot of that badly seeded content wouldn't get created in the first place because people might not want to have to become trustworthy for various reasons.\n\n> [gitcenter](#comment_1114_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): See, it's not about controlling the network -- it's about controlling the development of the network.\nIf you control development, you de facto control the network, because most people will download the official client unless there is a culture to do otherwise. As I've mentioned, decentralization is in part a cultural issue. Both how technically and how culturally feasible it is to fork and to get people to use a fork come into place here.\n>ZeroNet is currently too based on the reliability model -- many working PRs aren't meregd, many problems aren't addressed, huh, many issues aren't even responded to. \nWhich is a consequence of centralization. If everybody used different distributions you could patch in support for certain CAs directly into the client, or even give the user the option of choosing himself what CAs to use, or add whatever new you features you wanted without having to ask for permission. And what's better, you'd get a reasonable number of users to switch to your client, certainly a lot more people than how things currently are, where people just go to zeronet.io and download whatever nofish has decided to distribute and call it a day.\nTrying to keep the same model except now a few more people can weigh in is a bandaid solution to this. Sure, for a short time it might solve the issue, but even committees can make bad decisions which are hard for everyone else to prevent or correct when they aren't part of it, which is mitigated if there are many different groups controlling different versions of the client, just like there are many bittorrent clients all implementing different features and levels of cancer but all sharing the same common core protocol. Just imagine if people hadn't moved away from the official client. We'd all be using the adware proprietary uTorrent rebrand that is the first ever BitTorrent client right now.",
				"added": 1569674880
			},
			{
				"comment_id": 5,
				"body": "> [gitcenter](#comment_1117_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): There's still a question though: if I create my own provider, no one would use it so I won't be able to publish a post. I could use another account to tell people of my provider though but that looks hacky, especially when you have to trust that other provider... idk\nYou could always use self signed certs.",
				"added": 1569676363
			},
			{
				"comment_id": 6,
				"body": "> [gitcenter](#comment_1119_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): Than what's the point of using CAs, lol?\n\nThe point of using CAs is providing a way for people to be able to decrease the amount of spam they might see. Are you suggesting because CAs exist, the option to use self-signed certs should be removed from ZeroNet?\nIn any case, taking self signed certs out of the picture, the bootstrapping problem you mention ALREADY exists. There's no way to tell site owners to enable your CA without already having registered with another CA (besides self signed certs).",
				"added": 1569677095
			},
			{
				"comment_id": 7,
				"body": "> [gitcenter](#comment_1120_1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di): No, the other way round. What's the point of making new CAs if you have to have a self-signed certificate and that's enough to post? [...]\n\nAs things currently are? You might only be able to publicize your CA on sites that allow it, and some of these site's users might badger the owners of the sites that don't allow it to allow it.\nIn the system I'm describing, then the same mechanism of convincing by proxy, and also maybe not all users use the same settings throughout time. Maybe some users coast with self-signed certs enabled and turn on their trusted CA whitelist during times of heavy spam. In that case those users might pick up your recommendation during the times they have self signed certs enabled. Actually this also might apply with the current system, as long as the site owner in question is willing to fiddle with his site's settings depending on how much spam is going around.",
				"added": 1569677677
			}
		]
	},
	"comment_vote": {}
}