{
    "article": [
        {
            "title": "high_availability",
            "text": "{{redirect|Always-on|the software restriction|Always-on DRM}}\n{{Multiple issues|\n{{refimprove |date=January 2009}}\n{{tone |date=February 2010}}\n{{cleanup reorganize|date=October 2014}}\n}}\n'''High availability''' is a characteristic of a system, which aims to ensure an agreed level of operational performance, usually [[uptime]], for a higher than normal period.\n\nModernization has resulted in an increased reliance on these systems.  For example, hospitals and data centers require high availability of their systems to perform routine daily activities. [[Availability]] refers to the ability of the user community to obtain a service or good, access the system, whether to submit new work, update or alter existing work, or collect the results of previous work. If a user cannot access the system, it is - from the users point of view - ''unavailable''.<ref>{{cite book|author=Floyd Piedad, Michael Hawkins|title=High Availability: Design, Techniques, and Processes|url=https://books.google.com/books?id=kHB0HdQ98qYC&dq=high+availability+floyd+piedad+book&printsec=frontcover&source=bn&hl=en&ei=gs0LSrLvBKjm6gOT3ISPCA&sa=X&oi=book_result&ct=result&resnum=7|isbn=9780130962881|publisher=Prentice Hall|year=2001}}</ref> Generally, the term ''[[downtime]]'' is used to refer to periods when a system is unavailable.\n\n==Principles==\nThere are three principles of [[systems design]] in [[reliability engineering]] which can help achieve high availability.\n# Elimination of single points of failure.  This means adding redundancy to the system so that failure of a component does not mean failure of the entire system.\n# Reliable crossover.  In redundant systems, the crossover point itself tends to become a single point of failure.  Reliable systems must provide for reliable crossover.\n# Detection of failures as they occur.  If the two principles above are observed, then a user may never see a failure.  But the maintenance activity must.\n\n==Scheduled and unscheduled downtime==\n{{unreferenced section|date=June 2008}}\nA distinction can be made between scheduled and unscheduled downtime. Typically, scheduled downtime is a result of maintenance that is disruptive to system operation and usually cannot be avoided with a currently installed system design. Scheduled downtime events might include patches to [[system software]] that require a [[booting|reboot]] or system configuration changes that only take effect upon a reboot. In general, scheduled downtime is usually the result of some logical, management-initiated event. Unscheduled downtime events typically arise from some physical event, such as a hardware or software failure or environmental anomaly. Examples of unscheduled downtime events include power outages, failed [[CPU]] or [[RAM]] components (or possibly other failed hardware components), an over-temperature related shutdown, logically or physically severed network connections, security breaches, or various [[Application software|application]], [[middleware]], and [[operating system]] failures.\n\nIf users can be warned away from scheduled downtimes, then the distinction is useful.  But if the\nrequirement is for true high availability, then downtime is downtime whether or not it is scheduled.\n\nMany computing sites exclude scheduled downtime from availability calculations, assuming that it has little or no impact upon the computing user community. By doing this, they can claim to have phenomenally high availability, which might give the illusion of [[continuous availability]]. Systems that exhibit truly continuous availability are comparatively rare and higher priced, and most have carefully implemented specialty designs that eliminate any [[single point of failure]] and allow online hardware, network, operating system, middleware, and application upgrades, patches, and replacements. For certain systems, scheduled downtime does not matter, for example system downtime at an office building after everybody has gone home for the night.\n\n==Percentage calculation==\nAvailability is usually expressed as a percentage of uptime in a given year. The following table shows the downtime that will be allowed for a particular percentage of availability, presuming that the system is required to operate continuously. [[Service level agreement]]s often refer to monthly downtime or availability in order to calculate service credits to match monthly billing cycles. The following table shows the translation from a given availability percentage to the corresponding amount of time a system would be unavailable.\n\n<div align=\"center\">\n{| class=\"wikitable sortable\" style=\"text-align:right;\"\n!Availability %\n!Downtime per year\n!Downtime per month\n!Downtime per week\n!Downtime per day\n|- style=\"font-weight:bold;\"\n| align=\"left\" | 90% (\"one nine\")\n|36.5 days\n|72 hours\n|16.8 hours\n|2.4 hours\n|-\n| align=\"left\" |95% (\"one and a half nines\")\n|18.25 days\n|36 hours\n|8.4 hours\n|1.2 hours\n|-\n| align=\"left\" |97%\n|10.96 days\n|21.6 hours\n|5.04 hours\n|43.2 minutes\n|-\n| align=\"left\" |98%\n|7.30 days\n|14.4 hours\n|3.36 hours\n|28.8 minutes\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99% (\"two nines\")\n|3.65 days\n|7.20 hours\n|1.68 hours\n|14.4 minutes\n|-\n| align=\"left\" |99.5% (\"two and a half nines\")\n|1.83 days\n|3.60 hours\n|50.4 minutes\n|7.2 minutes\n|-\n| align=\"left\" |99.8%\n|17.52 hours\n|86.23 minutes\n|20.16 minutes\n|2.88 minutes\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.9% (\"three nines\")\n|8.76 hours\n|43.8 minutes\n|10.1 minutes\n|1.44 minutes\n|-\n| align=\"left\" |99.95% (\"three and a half nines\")\n|4.38 hours\n|21.56 minutes\n|5.04 minutes\n|43.2 seconds\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.99% (\"four nines\")\n|52.56 minutes\n|4.38 minutes\n|1.01 minutes\n|8.64 seconds\n|-\n| align=\"left\" |99.995% (\"four and a half nines\")\n|26.28 minutes\n|2.16 minutes\n|30.24 seconds\n|4.32 seconds\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.999% (\"five nines\")\n|5.26 minutes\n|25.9 seconds\n|6.05 seconds\n|864.3 milliseconds\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.9999% (\"six nines\")\n|31.5 seconds\n|2.59 seconds\n|604.8 milliseconds\n|86.4 milliseconds\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.99999% (\"seven nines\")\n|3.15 seconds\n|262.97 milliseconds\n|60.48 milliseconds\n|8.64 milliseconds\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.999999% (\"eight nines\")\n|315.569 milliseconds\n|26.297 milliseconds\n|6.048 milliseconds\n|0.864 milliseconds\n|- style=\"font-weight:bold;\"\n| align=\"left\" |99.9999999% (\"nine nines\")\n|31.5569 milliseconds\n|2.6297 milliseconds\n|0.6048 milliseconds\n|0.0864 milliseconds\n|}\n</div>\n\n[[Uptime]] and [[availability]] can be used synonymously, as long as the items being discussed are kept consistent.  That is, a system can be up, but its services are not available, as in the case of a [[network outage]].  This can also be viewed as, a system can be available to work on, but its services are not up from a functional perspective (as opposed to software service/process perspective). The perspective is important here, whether the item being discussed is the server hardware, server OS, functional service, software service/process...etc.  Keep the perspective consistent throughout a discussion, then uptime and availability can be used synonymously.\n\n===\"Nines\"===\n{{see also|List of unusual units of measurement#Nines}}\nPercentages of a particular order of magnitude are sometimes referred to by the [[List of unusual units of measurement#Nines|number of nines]] or \"class of nines\" in the digits.  For example, electricity that is delivered without interruptions ([[Power outage|blackout]]s, [[brownout (electricity)|brownout]]s or [[Voltage spike|surge]]s) 99.999% of the time would have 5 nines reliability, or class five.<ref>[http://www.cs.kent.edu/~walker/classes/aos.s00/lectures/L25.ps Lecture Notes] M. Nesterenko, Kent State University</ref>  In particular, the term is used in connection with [[mainframes]]<ref>[http://comet.lehman.cuny.edu/cocchi/CIS345/LargeComputing/05_Availability.ppt Introduction to the new mainframe: Large scale commercial computing Chapter 5 Availability] IBM (2006)</ref><ref>[https://www.youtube.com/watch?v=DPcM5UePTY0 IBM zEnterprise EC12 Business Value Video] at ''youtube.com''</ref> or enterprise computing, often as part of a [[service-level agreement]].\n\nSimilarly, percentages ending in a 5 have conventional names, traditionally the number of nines, then \"five\", so 99.95% is \"three nines five\", abbreviated 3N5.<ref>{{cite journal |journal=Precious metals |volume=4 |year=1981 |page=[https://books.google.com/books?id=KOMYAAAAIAAJ&q=%22four+nines+five%22 262]}}</ref><ref>{{cite book|title=PVD for Microelectronics: Sputter Desposition to Semiconductor Manufacturing|year=1998|page=[https://books.google.com/books?id=hmvtBE_7i00C&pg=PA387&lpg=PA387&dq=%22four+nines+five%22 387]}}<!-- NOTE: reference has typo, writing 99.9995% as \"four-nines-five\" while it is actually *five* nines five. --></ref> This is casually referred to as \"three and a half nines\",<ref>{{cite book |title=Site Reliability Engineering: How Google Runs Production Systems |authors=Niall Richard Murphy, Betsy Beyer, Jennifer Petoff, Chris Jones |year=2016 |page=[https://books.google.com/books?id=_4rPCwAAQBAJ&pg=PA38&dq=%22three+and+a+half+nines%22 38]}}</ref> but this is incorrect: a 5 is only a factor of 2, while a 9 is a factor of 10, so a 5 is 0.3 nines (per below formula: <math>\\log_{10} 2 \\approx 0.3</math>):{{efn|See [[Mathematical coincidence#Concerning base 2|mathematical coincidences concerning base 2]] for details on this approximation.}} 99.95% availability is 3.3 nines, not 3.5 nines.<ref>{{cite web |url=http://www.joshdeprez.com/post/67-nines-of-availability/ |title=Nines of Nines |author=Josh Deprez |date=2016-04-23}}</ref> More simply, going from 99.9% availability to 99.95% availability is a factor of 2 (0.1% to 0.05% unavailability), but going from 99.95% to 99.99% availability is a factor of 5 (0.05% to 0.01% unavailability), over twice as much.{{efn|\"Twice as much\" on a logarithmic scale, meaning two ''factors'' of 2: <math>\\times 2 \\times 2 < \\times 5</math>}}\n\nA formulation of the ''class of 9s''  <math>c</math>  based on a system's [[unavailability]] <math>x</math> would be\n\n:<math> c := \\lfloor  - \\log_{10} x  \\rfloor</math>\n\n(cf. [[Floor and ceiling functions]]).\n\nA [[Nine (purity)|similar measurement]] is sometimes used to describe the purity of substances.\n\nIn general, the number of nines is not often used by a network engineer when modeling and measuring availability because it is hard to apply in formula. More often, the unavailability expressed as a [[probability]] (like 0.00001), or a [[downtime]] per year is quoted. Availability specified as a number of nines is often seen in [[marketing]] documents.{{Citation needed|date=August 2008}} The use of the \"nines\" has been called into question, since it does not appropriately reflect that the impact of unavailability varies with its time of occurrence.<ref>[http://searchstorage.techtarget.com/tip/0,289483,sid5_gci921823,00.html Evan L. Marcus, ''The myth of the nines'']</ref> For large amounts of 9s, the \"unavailability\" index (measure of downtime rather than uptime) is easier to handle. For example, this is why an \"unavailability\" rather than availability metric is used in hard disk or data link [[bit error rate]]s.\n\n==Measurement and interpretation==\n\nAvailability measurement is subject to some degree of interpretation. A system that has been up for 365 days in a non-leap year might have been eclipsed by a network failure that lasted for 9 hours during a peak usage period; the user community will see the system as unavailable, whereas the system administrator will claim 100% [[uptime]]. However, given the true definition of availability, the system will be approximately 99.9% available, or three nines (8751 hours of available time out of 8760 hours per non-leap year). Also, systems experiencing performance problems are often deemed partially or entirely unavailable by users, even when the systems are continuing to function. Similarly, unavailability of select application functions might go unnoticed by administrators yet be devastating to users&nbsp;&mdash; a true availability measure is holistic.\n\nAvailability must be measured to be determined, ideally with comprehensive monitoring tools (\"instrumentation\") that are themselves highly available. If there is a lack of instrumentation, systems supporting high volume transaction processing throughout the day and night, such as credit card processing systems or telephone switches, are often inherently better monitored, at least by the users themselves, than systems which experience periodic lulls in demand.\n\nAn alternative metric is [[mean time between failures]] (MTBF).\n\n==Closely related concepts==\nRecovery time (or estimated time of repair (ETR), also known as [[recovery time objective]] (RTO) is closely related to availability, that is the total time required for a planned outage or the time required to fully recover from an unplanned outage. Another metric is [[mean time to recovery]] (MTTR).  Recovery time could be infinite with certain system designs and failures, i.e. full recovery is impossible. One such example is a fire or flood that destroys a data center and its systems when there is no secondary [[disaster recovery]] data center.\n\nAnother related concept is data availability, that is the degree to which databases and other information storage systems faithfully record and report system transactions. Information management specialists often focus separately on data availability in order to determine acceptable (or actual) data loss with various failure events. Some users can tolerate application service interruptions but cannot tolerate data loss.\n\nA [[service level agreement]] (\"SLA\") formalizes an organization's availability objectives and requirements.\n\n==System design==\nParadoxically, adding more components to an overall system design can undermine efforts to achieve high availability. That is because complex systems inherently have more potential failure points and are more difficult to implement correctly. While some analysts would put forth the theory that the most highly available systems adhere to a simple architecture (a single, high quality, multi-purpose physical system with comprehensive internal hardware redundancy), this architecture suffers from the requirement that the entire system must be brought down for patching and operating system upgrades. More advanced system designs allow for systems to be patched and upgraded without compromising service availability (see [[load balancing (computing)|load balancing]] and [[failover]]).\n\nHigh availability requires less human intervention to restore operation in complex systems; the reason for this being that the most common cause for outages is human error.<ref name=humanerror>{{cite web|date=October 27, 2010|url=http://www.gartner.com/id=1458131|title=Top Seven Considerations for Configuration Management for Virtual and Cloud Infrastructures |publisher=[[Gartner]]|accessdate=October 13, 2013}}</ref>\n\n[[Redundancy (engineering)|Redundancy]] is used to create systems with high levels of availability (e.g. aircraft flight computers). In this case it is required to have high levels of failure detectability and avoidance of common cause failures. Two kinds of redundancy are passive redundancy and active redundancy.\n\nPassive redundancy is used to achieve high availability by including enough excess capacity in the design to accommodate a performance decline. The simplest example is a boat with two separate engines driving two separate propellers. The boat continues toward its destination despite failure of a single engine or propeller. A more complex example is multiple redundant power generation facilities within a large system involving [[electric power transmission]]. Malfunction of single components is not considered to be a failure unless the resulting performance decline exceeds the specification limits for the entire system.\n\nActive redundancy is used in complex systems to achieve high availability with no performance decline. Multiple items of the same kind are incorporated into a design that includes a method to detect failure and automatically reconfigure the system to bypass failed items using a voting scheme. This is used with complex computing systems that are linked. Internet [[routing]] is derived from early work by Birman and Joseph in this area.<ref>{{IETF RFC|992}}</ref> Active redundancy may introduce more complex failure modes into a system, such as continuous system reconfiguration due to faulty voting logic.\n\nZero downtime system design means that modeling and simulation indicates [[mean time between failures]] significantly exceeds the period of time between [[planned maintenance]], [[upgrade]] events, or system lifetime. Zero downtime involves massive redundancy, which is needed for some types of aircraft and for most kinds of [[communications satellite|communications satellites]]. [[Global Positioning System]] is an example of a zero downtime system.\n\nFault [[instrumentation]] can be used in systems with limited redundancy to achieve high availability. Maintenance actions occur during brief periods of down-time only after a fault indicator activates. Failure is only significant if this occurs during a [[mission critical]] period.\n\n[[Modeling and simulation]] is used to evaluate the theoretical reliability for large systems. The outcome of this kind of model is used to evaluate different design options. A model of the entire system is created, and the model is stressed by removing components. Redundancy simulation involves the N-x criteria. N represents the total number of components in the system. x is the number of components used to stress the system. N-1 means the model is stressed by evaluating performance with all possible combinations where one component is faulted. N-2 means the model is stressed by evaluating performance with all possible combinations where two component are faulted simultaneously.\n\n==Reasons for unavailability==\nA survey among academic availability experts in 2010 ranked reasons for unavailability of enterprise IT systems. All reasons refer to '''not following best practice''' in each of the following areas (in order of importance):<ref>Ulrik Franke, Pontus Johnson, Johan König, Liv Marcks von Würtemberg: Availability of enterprise IT systems – an expert-based Bayesian model, ''Proc. Fourth International Workshop on Software Quality and Maintainability'' (WSQM 2010), Madrid, [http://www.kth.se/ees/forskning/publikationer/modules/publications_polopoly/reports/2010/IR-EE-ICS_2010_047.pdf?l=en_UK]</ref>\n\n# Monitoring of the relevant components\n# [[Requirements management|Requirements]] and procurement\n# Operations\n# Avoidance of network failures\n# Avoidance of internal application failures\n# Avoidance of external services that fail\n# Physical environment\n# Network redundancy\n# Technical solution of backup\n# Process solution of backup\n# Physical location\n# Infrastructure redundancy\n# Storage architecture redundancy\n\nThe factors themselves are based on the work of [[Evan Marcus]] and [[Hal Stern]].<ref>{{cite book |first=E. |last=Marcus |first2=H. |last2=Stern |title=Blueprints for high availability |edition=Second |location=Indianapolis, IN |publisher=John Wiley & Sons |year=2003 |isbn=0-471-43026-9 }}</ref>\n\n==Costs of unavailability==\nIn a 1998 report from IBM Global Services, unavailable systems were estimated to have cost American businesses $4.54 billion in 1996, due to lost productivity and revenues.<ref>IBM Global Services, ''Improving systems availability'', IBM Global Services, 1998, [http://www.dis.uniroma1.it/~irl/docs/availabilitytutorial.pdf]</ref>\n\nHigh availability is one of the primary requirements of the control systems in unmanned vehicles<ref>[[Ground Combat Vehicle]]</ref> and autonomous maritime vessels.<ref>[[ACTUV]]</ref> If the controlling system becomes unavailable, the Ground Combat Vehicle (GCV) or ASW Continuous Trail Unmanned Vessel (ACTUV) would be lost.\n\n==See also==\n* [[Fault-tolerance]]\n* [[Reliability, availability and serviceability (computing)]]\n* [[Reliability engineering]]\n* [[Resilience (network)]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.openclovis.com OpenClovis SAFplus: Open source High Availability Software tools for telecom, defense, aerospace, gaming, banking, etc]\n* [http://www.cisco.com/en/US/tech/tk869/tk769/technologies_white_paper09186a00800a998b.shtml Cisco IOS Management for High Availability Networking] – Best Practices White Paper\n* [http://jedi.informatik.uni-leipzig.de/de/index_en.html Homepage of the Dept. for Computer Science of the University of Leipzig]\n* [http://www-ti.informatik.uni-tuebingen.de/~spruth/ECvorles/index.html Lecture Notes on Enterprise Computing] University of Tübingen\n* [http://www.pixelbeat.org/docs/reliability_calculator/ Online reliability calculator]\n\n{{DEFAULTSORT:High Availability}}\n[[Category:System administration]]\n[[Category:Quality control]]\n[[Category:Applied probability]]\n[[Category:Reliability engineering]]\n[[Category:Measurement]]",
            "slug": "high-availability",
            "date_updated": 1519551397827,
            "imported": "https://en.wikipedia.org/wiki/high_availability"
        }
    ]
}