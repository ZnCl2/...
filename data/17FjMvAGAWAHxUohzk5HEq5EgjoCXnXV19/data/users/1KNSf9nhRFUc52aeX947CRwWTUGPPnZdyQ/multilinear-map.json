{
    "article": [
        {
            "title": "Multilinear map",
            "text": "In [[linear algebra]], a '''multilinear map''' is a [[function (mathematics)|function]] of several variables that is linear separately in each variable.  More precisely, a multilinear map is a function\n\n:<math>f\\colon V_1 \\times \\cdots \\times V_n \\to W\\text{,}</math>\n\nwhere <math>V_1,\\ldots,V_n</math> and <math>W</math> are [[vector space]]s (or [[module (mathematics)|module]]s over a [[commutative ring]]), with the following property: for each <math>i</math>, if all of the variables but <math>v_i</math> are held constant, then <math>f(v_1,\\ldots,v_n)</math> is a [[linear map|linear function]] of <math>v_i</math>.<ref>Lang. Algebra. Springer; 3rd edition (January 8, 2002)</ref>\n\nA multilinear map of one variable is a [[linear map]], and of two variables is a [[bilinear map]].  More generally, a multilinear map of ''k'' variables is called a '''''k''-linear map'''.  If the codomain of a multilinear map is the field of scalars, it is called a [[multilinear form]].  Multilinear maps and multilinear forms are fundamental objects of study in [[multilinear algebra]].\n\nIf all variables belong to the same space, one can consider [[symmetric function|symmetric]], [[Linear algebra|antisymmetric]] and [[alternating map|alternating]] ''k''-linear maps. The latter coincide if the underlying [[ring (mathematics)|ring]] (or field) has a characteristic different from two, else the former two coincide.\n\n==Examples==\n* Any [[bilinear map]] is a multilinear map.  For example, any [[inner product]] on a vector space is a multilinear map, as is the [[cross product]] of vectors in <math>\\mathbb{R}^3</math>.\n* The [[determinant]] of a matrix is an [[Antisymmetric matrix|antisymmetric]] multilinear function of the columns (or rows) of a [[square matrix]].\n* If <math>F\\colon \\mathbb{R}^m \\to \\mathbb{R}^n</math> is a [[smooth function|''C<sup>k</sup>'' function]], then the <math>k\\!</math>th derivative of <math>F\\!</math> at each point <math>p</math> in its domain can be viewed as a [[symmetric function|symmetric]] <math>k\\!</math>-linear function <math>D^k\\!f(p)\\colon \\mathbb{R}^m\\times\\cdots\\times\\mathbb{R}^m \\to \\mathbb{R}^n</math>.\n* The [[Multilinear subspace learning#Tensor-to-vector projection .28TVP.29|tensor-to-vector projection]] in [[multilinear subspace learning]] is a multilinear map as well.\n\n==Coordinate representation==\nLet\n\n:<math>f\\colon V_1 \\times \\cdots \\times V_n \\to W\\text{,}</math>\n\nbe a multilinear map between finite-dimensional vector spaces, where <math>V_i\\!</math> has dimension <math>d_i\\!</math>, and <math>W\\!</math> has dimension <math>d\\!</math>.  If we choose a [[basis (linear algebra)|basis]] <math>\\{\\textbf{e}_{i1},\\ldots,\\textbf{e}_{id_i}\\}</math> for each <math>V_i\\!</math> and a basis <math>\\{\\textbf{b}_1,\\ldots,\\textbf{b}_d\\}</math> for <math>W\\!</math> (using bold for vectors), then we can define a collection of scalars <math>A_{j_1\\cdots j_n}^k</math> by\n\n:<math>f(\\textbf{e}_{1j_1},\\ldots,\\textbf{e}_{nj_n}) = A_{j_1\\cdots j_n}^1\\,\\textbf{b}_1 + \\cdots +  A_{j_1\\cdots j_n}^d\\,\\textbf{b}_d.</math>\n\nThen the scalars <math>\\{A_{j_1\\cdots j_n}^k \\mid 1\\leq j_i\\leq d_i, 1 \\leq k \\leq d\\}</math> completely determine the multilinear function <math>f\\!</math>.  In particular, if\n\n:<math>\\textbf{v}_i = \\sum_{j=1}^{d_i} v_{ij} \\textbf{e}_{ij}\\!</math>\n\nfor <math>1 \\leq i \\leq n\\!</math>, then\n\n:<math>f(\\textbf{v}_1,\\ldots,\\textbf{v}_n) = \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_n=1}^{d_n} \\sum_{k=1}^{d} A_{j_1\\cdots j_n}^k v_{1j_1}\\cdots v_{nj_n} \\textbf{b}_k.</math>\n\n==Example==\nLet's take a trilinear function\n\n:<math>f\\colon R^2 \\times R^2 \\times R^2 \\to R, </math>\nwhere {{math|1=''V<sub>i</sub>'' = ''R''<sup>2</sup>, ''d<sub>i</sub>'' = 2, ''i'' = 1,2,3}}, and {{math|1=''W'' = ''R'', ''d'' = 1}}.\n\nA basis for each {{mvar|V<sub>i</sub>}} is <math>\\{\\textbf{e}_{i1},\\ldots,\\textbf{e}_{id_i}\\} = \\{\\textbf{e}_{1}, \\textbf{e}_{2}\\} = \\{(1,0), (0,1)\\}.</math> Let\n\n:<math>f(\\textbf{e}_{1i},\\textbf{e}_{2j},\\textbf{e}_{3k}) = f(\\textbf{e}_{i},\\textbf{e}_{j},\\textbf{e}_{k}) = A_{ijk},</math>\nwhere <math>i,j,k \\in \\{1,2\\}</math>. In other words, the constant <math>A_{i j k}</math> is a function value at one of the eight possible triples of basis vectors (since there are two choices for each of the three <math>V_i</math>), namely: \n:<math>\n\\{\\textbf{e}_1, \\textbf{e}_1, \\textbf{e}_1\\}, \n\\{\\textbf{e}_1, \\textbf{e}_1, \\textbf{e}_2\\}, \n\\{\\textbf{e}_1, \\textbf{e}_2, \\textbf{e}_1\\},\n\\{\\textbf{e}_1, \\textbf{e}_2, \\textbf{e}_2\\},\n\\{\\textbf{e}_2, \\textbf{e}_1, \\textbf{e}_1\\}, \n\\{\\textbf{e}_2, \\textbf{e}_1, \\textbf{e}_2\\}, \n\\{\\textbf{e}_2, \\textbf{e}_2, \\textbf{e}_1\\},\n\\{\\textbf{e}_2, \\textbf{e}_2, \\textbf{e}_2\\}.\n</math>\n\nEach vector <math>\\textbf{v}_i \\in V_i = R^2</math> can be expressed as a linear combination of the basis vectors\n\n:<math>\\textbf{v}_i = \\sum_{j=1}^{2} v_{ij} \\textbf{e}_{ij} = v_{i1} \\times \\textbf{e}_1 + v_{i2} \\times \\textbf{e}_2 = v_{i1} \\times (1, 0) + v_{i2} \\times (0, 1).</math>\n\nThe function value at an arbitrary collection of three vectors <math>\\textbf{v}_i \\in R^2</math> can be expressed as\n:<math>f(\\textbf{v}_1,\\textbf{v}_2, \\textbf{v}_3) = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\sum_{k=1}^{2} A_{i j k} v_{1i} v_{2j} v_{3k}.</math>\nOr, in expanded form as\n:<math> \\begin{align}\nf((a,b),(c,d)&, (e,f)) = ace \\times f(\\textbf{e}_1, \\textbf{e}_1, \\textbf{e}_1) + acf \\times f(\\textbf{e}_1, \\textbf{e}_1, \\textbf{e}_2) \\\\\n&+ ade \\times f(\\textbf{e}_1, \\textbf{e}_2, \\textbf{e}_1) +\nadf \\times f(\\textbf{e}_1, \\textbf{e}_2, \\textbf{e}_2) +\nbce \\times f(\\textbf{e}_2, \\textbf{e}_1, \\textbf{e}_1) +\nbcf \\times f(\\textbf{e}_2, \\textbf{e}_1, \\textbf{e}_2) \\\\ \n&+ bde \\times f(\\textbf{e}_2, \\textbf{e}_2, \\textbf{e}_1) +\nbdf \\times f(\\textbf{e}_2, \\textbf{e}_2, \\textbf{e}_2).\n\\end{align} \n</math>\n\n==Relation to tensor products==\nThere is a natural one-to-one correspondence between multilinear maps\n\n:<math>f\\colon V_1 \\times \\cdots \\times V_n \\to W\\text{,}</math>\n\nand linear maps\n\n:<math>F\\colon V_1 \\otimes \\cdots \\otimes V_n \\to W\\text{,}</math>\n\nwhere <math>V_1 \\otimes \\cdots \\otimes V_n\\!</math> denotes the [[tensor product]] of <math>V_1,\\ldots,V_n</math>.  The relation between the functions <math>f\\!</math> and <math>F\\!</math> is given by the formula\n\n:<math>F(v_1\\otimes \\cdots \\otimes v_n) = f(v_1,\\ldots,v_n).</math>\n\n==Multilinear functions on ''n''&times;''n'' matrices==\nOne can consider multilinear functions, on an {{math|''n''&times;''n''}} matrix over a [[commutative ring]] {{mvar|K}} with identity, as a function of the rows (or equivalently the columns) of the matrix.  Let {{math|''A''}} be such a matrix and {{math|''a<sub>i</sub>'', 1 ≤ ''i'' ≤ ''n''}}, be the rows of {{math|''A''}}.  Then the multilinear function {{math|''D''}} can be written as\n\n:<math>D(A) = D(a_{1},\\ldots,a_{n}),</math>\n\nsatisfying\n\n:<math>D(a_{1},\\ldots,c a_{i} + a_{i}',\\ldots,a_{n}) = c D(a_{1},\\ldots,a_{i},\\ldots,a_{n}) + D(a_{1},\\ldots,a_{i}',\\ldots,a_{n}).</math>\n\nIf we let <math>\\hat{e}_j</math> represent the {{mvar|j}}th row of the identity matrix, we can express each row {{math|''a<sub>i</sub>''}} as the sum\n\n:<math>a_{i} = \\sum_{j=1}^n A(i,j)\\hat{e}_{j}.</math>\n\nUsing the multilinearity of {{math|''D''}} we rewrite {{math|''D''(''A'')}} as\n\n:<math>\nD(A) = D\\left(\\sum_{j=1}^n A(1,j)\\hat{e}_{j}, a_2, \\ldots, a_n\\right)\n       = \\sum_{j=1}^n A(1,j) D(\\hat{e}_{j},a_2,\\ldots,a_n).\n</math>\n\nContinuing this substitution for each {{math|''a<sub>i</sub>''}} we get, for {{math|1 ≤ ''i'' ≤ ''n''}},\n\n:<math>\nD(A) = \\sum_{1\\le k_i\\le n} A(1,k_{1})A(2,k_{2})\\dots A(n,k_{n}) D(\\hat{e}_{k_{1}},\\dots,\\hat{e}_{k_{n}}),\n</math>\n\nwhere, since in our case {{math|1 ≤ ''i'' ≤ ''n''}},\n::<math>\n \\sum_{1\\le k_i \\le n} = \\sum_{1\\le k_1 \\le n} \\ldots \\sum_{1\\le k_i \\le n} \\ldots \\sum_{1\\le k_n \\le n} \n</math>\nis a series of nested summations.\n\nTherefore, {{math|''D''(''A'')}} is uniquely determined by how {{mvar|D}} operates on <math>\\hat{e}_{k_{1}},\\dots,\\hat{e}_{k_{n}}</math>.\n\n==Example==\nIn the case of 2&times;2 matrices we get\n\n:<math>\nD(A) = A_{1,1}A_{2,1}D(\\hat{e}_1,\\hat{e}_1) + A_{1,1}A_{2,2}D(\\hat{e}_1,\\hat{e}_2) + A_{1,2}A_{2,1}D(\\hat{e}_2,\\hat{e}_1) + A_{1,2}A_{2,2}D(\\hat{e}_2,\\hat{e}_2) \\,\n</math>\n\nWhere <math>\\hat{e}_1 = [1,0]</math> and <math>\\hat{e}_2 = [0,1]</math>.  If we restrict <math>D</math> to be an alternating function then <math>D(\\hat{e}_1,\\hat{e}_1) = D(\\hat{e}_2,\\hat{e}_2) = 0</math> and <math>D(\\hat{e}_2,\\hat{e}_1) = -D(\\hat{e}_1,\\hat{e}_2) = -D(I)</math>.  Letting <math>D(I) = 1</math> we get the determinant function on 2&times;2 matrices:\n\n:<math> D(A) = A_{1,1}A_{2,2} - A_{1,2}A_{2,1} .</math>\n\n==Properties==\n* A multilinear map has a value of zero whenever one of its arguments is zero.\n\n==See also==\n* [[Algebraic form]]\n* [[Multilinear form]]\n* [[Homogeneous polynomial]]\n* [[Homogeneous function]]\n* [[Tensor]]s\n* [[Multilinear subspace learning#Multilinear projection|Multilinear projection]]\n* [[Multilinear subspace learning]]\n\n==References==\n<references/>\n\n[[Category:Multilinear algebra]]",
            "slug": "multilinear-map",
            "date_updated": 1517596768080,
            "imported": "https://en.wikipedia.org/wiki/Multilinear map"
        }
    ]
}