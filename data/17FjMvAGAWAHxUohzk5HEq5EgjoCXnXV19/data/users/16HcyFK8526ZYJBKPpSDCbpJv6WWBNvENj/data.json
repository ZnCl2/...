{
    "article": [
        {
            "title": "",
            "text": "{{about|the branch of computer science and mathematics|the journal|Theoretical Computer Science (journal)}}\n[[Image:Maquina.png|thumb|An artistic representation of a [[Turing machine]]. Turing machines are used to model general computing devices.]]\n\n'''Theoretical computer science''', or TCS, is a subset of general [[computer science]] and [[mathematics]] that focuses on more mathematical topics \nof computing and includes the [[theory of computation]].\n\nIt is difficult to circumscribe the theoretical areas precisely. The [[Association for Computing Machinery|ACM]]'s [[ACM SIGACT|Special Interest Group on Algorithms and Computation Theory]] (SIGACT) provides the following description:<ref>{{cite web | title =  SIGACT | url = https://www.sigact.org/ | accessdate = 2017-01-19}}</ref>\n\n{{\"|TCS covers a wide variety of topics including [[algorithms]], [[data structure]]s, [[computational complexity theory|computational complexity]], [[parallel computation|parallel]] and [[distributed computation|distributed]] computation, [[probabilistic computation]], [[quantum computation]], [[automata theory]], [[information theory]], [[cryptography]], [[program semantics]] and [[Formal methods|verification]], [[machine learning]], [[computational biology]], [[computational economics]], [[computational geometry]], and [[computational number theory]] and [[Symbolic computation|algebra]]. Work in this field is often distinguished by its emphasis on mathematical technique and [[rigor#Mathematical rigour|rigor]].}}\n\nIn this list, the ACM's journal Transactions on Computation Theory includes [[coding theory]] and [[computational learning theory]], as well as theoretical computer science aspects of areas such as [[databases]], [[information retrieval]], economic models, and [[Computer network|networks]].<ref>{{cite web | title =  ToCT| url = http://toct.acm.org/journal.html | accessdate = 2010-06-09}}</ref> Despite this broad scope, the \"theory people\" in computer science self-identify as different from the \"applied people\"{{Citation needed|date=September 2017}}. Some characterize themselves as doing the \"(more fundamental) 'science(s)' underlying the field of computing.\"<ref>{{cite web | title = Challenges for Theoretical Computer Science: Theory as the Scientific Foundation of Computing | url = http://www.research.att.com/%7Edsj/nsflist.html#Intro | accessdate = 2009-03-29}}</ref> Other \"theory-applied people\" suggest that it is impossible to separate theory and application. This means that the so-called \"theory people\" regularly use experimental science(s) done in less-theoretical areas such as [[software system]] research{{Citation needed|date=September 2017}}. It also means that there is more cooperation than mutually exclusive competition between theory and application{{Citation needed|date=September 2017}}.\n\n== History ==\n{{Main|History of computer science}}\n\nWhile logical inference and mathematical proof had existed previously, in 1931 [[Kurt Gödel]] proved with his [[incompleteness theorem]] that there are fundamental limitations on what statements could be proved or disproved.\n\nThese developments have led to the modern study of logic and [[computability]], and indeed the field of theoretical computer science as a whole{{Citation needed|date=September 2017}}. [[Information theory]] was added to the field{{by whom|date=September 2017}} with a 1948 mathematical theory of communication by [[Claude Shannon]]. In the same decade, [[Donald Hebb]] introduced a mathematical model of [[Hebbian learning|learning]] in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of [[neural network]]s and [[parallel distributed processing]] were established. In 1971, [[Stephen Cook]] and, working independently, [[Leonid Levin]], proved that there exist practically relevant problems that are [[NP-complete]] &ndash; a landmark result in [[computational complexity theory]]{{Citation needed|date=September 2017}}.\n\nWith the development of [[quantum mechanics]] in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously. This led to the concept of a [[quantum computer]] in the latter half of the 20th century that took off in the 1990s when [[Peter Shor]] showed that such methods could be used to factor large numbers in [[polynomial time]], which, if implemented, would render most modern [[public key cryptography]] systems uselessly insecure.{{citation needed|reason=Your explanation here|date=October 2014}}\n\nModern theoretical computer science research is based on these basic developments, but includes many other mathematical and interdisciplinary problems that have been posed, as shown below:\n\n{| style=\"border:1px solid #ddd; text-align:center;  margin: 0 auto;\" cellspacing=\"15\"\n| <math> P \\rightarrow Q \\,</math>\n| [[File:DFAexample.svg|96px]]\n| [[File:Elliptic curve simple.png|96px]]\n| [[File:6n-graf.svg|96px]]\n| [[File:Wang tiles.svg|96px]]\n| '''P = NP''' ?\n|-\n| [[Mathematical logic]]\n| [[Automata theory]]\n| [[Number theory]]\n| [[Graph theory]]\n| [[Computability theory]]\n| [[Computational complexity theory]]\n|-\n| '''GNITIRW-TERCES'''\n| <math>\\Gamma\\vdash x: \\text{Int}</math>\n| [[File:Commutative diagram for morphism.svg|96px]]\n| [[File:SimplexRangeSearching.svg|96px]]\n| [[File:TSP Deutschland 3.png|96px]]\n| [[File:Blochsphere.svg|96px]]\n|-\n| [[Cryptography]]\n| [[Type theory]]\n| [[Category theory]]\n| [[Computational geometry]]\n| [[Combinatorial optimization]]\n| [[Quantum computer|Quantum computing theory]]\n|}\n\n== Topics ==\n\n===Algorithms===\n{{main|Algorithm}}\nAn [[algorithm]] is a step-by-step procedure for calculations.  Algorithms are used for [[calculation]], [[data processing]], and [[automated reasoning]].\n\nAn algorithm is an [[effective method]] expressed as a [[wikt:finite|finite]] list<ref>\"Any classical mathematical algorithm, for example, can be described in a finite number of English words\" (Rogers 1987:2).</ref> of well-defined instructions<ref>Well defined with respect to the agent that executes the algorithm: \"There is a computing agent, usually human, which can react to the instructions and carry out the computations\" (Rogers 1987:2).</ref> for calculating a [[Function (mathematics)|function]].<ref>\"an algorithm is a procedure for computing a ''function'' (with respect to some chosen notation for integers) ... this limitation (to numerical functions) results in no loss of generality\", (Rogers 1987:1).</ref>  Starting from an initial state and initial input (perhaps [[null string|empty]]),<ref>\"An algorithm has [[zero]] or more inputs, i.e., [[quantity|quantities]] which are given to it initially before the algorithm begins\" (Knuth 1973:5).</ref> the instructions describe a [[computation]] that, when [[Execution (computing)|executed]], proceeds through a finite<ref>\"A procedure which has all the characteristics of an algorithm except that it possibly lacks finiteness may be called a 'computational method'\" (Knuth 1973:5).</ref> number of well-defined successive states, eventually producing \"output\"<ref>\"An algorithm has one or more outputs, i.e. quantities which have a specified relation to the inputs\" (Knuth 1973:5).</ref> and terminating at a final ending state. The transition from one state to the next is not necessarily [[deterministic]]; some algorithms, known as [[randomized algorithms]], incorporate random input.<ref>Whether or not a process with random interior processes (not including the input) is an algorithm is debatable. Rogers opines that: \"a computation is carried out in a discrete stepwise fashion, without use of continuous methods or analogue devices . . . carried forward deterministically, without resort to random methods or devices, e.g., dice\" Rogers 1987:2.</ref>\n\n===Data structures===\n{{main|Data structure}}\nA [[data structure]] is a particular way of organizing [[data (computing)|data]] in a computer so that it can be used [[algorithmic efficiency|efficiently]].<ref>Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. U.S. [[National Institute of Standards and Technology]]. 15 December 2004. [http://xlinux.nist.gov/dads/HTML/datastructur.html Online version] Accessed May 21, 2009.''</ref><ref>Entry ''data structure'' in the [[Encyclopædia Britannica]] (2009) [http://www.britannica.com/EBchecked/topic/152190/data-structure Online entry] accessed on May 21, 2009.</ref>\n\nDifferent kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use [[B-tree]] indexes for small percentages of data retrieval and [[compiler]]s and databases use dynamic [[hash table]]s as look up tables.\n\nData structures provide a means to manage large amounts of data efficiently for uses such as large [[database]]s and [[web indexing|internet indexing services]]. Usually, efficient data structures are key to designing efficient [[algorithm]]s. Some formal design methods and [[programming language]]s emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both [[main memory]] and in [[secondary memory]].\n\n===Computational complexity theory===\n{{main|Computational complexity theory}}\n[[Computational complexity theory]] is a branch of the [[theory of computation]] that focuses on classifying [[computational problems]] according to their inherent difficulty, and relating those [[Complexity class|classes]] to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an [[algorithm]].\n\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the [[algorithm]] used. The theory formalizes this intuition, by introducing mathematical [[models of computation]] to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other [[complexity]] measures are also used, such as the amount of communication (used in [[communication complexity]]), the number of [[logic gate|gates]] in a circuit (used in [[circuit complexity]]) and the number of processors (used in [[parallel computing]]). One of the roles of computational complexity theory is to determine the practical limits on what [[computer]]s can and cannot do.\n\n===Distributed computation===\n{{main|Distributed computation}}\n[[Distributed computing]] studies distributed systems. A distributed system is a software system in which components located on [[computer network|networked computers]] communicate and coordinate their actions by [[message passing|passing messages]].<ref name=\"Coulouris\">{{cite book|last=Coulouris|first=George|author2=Jean Dollimore|author3=Tim Kindberg|author4=Gordon Blair|title=Distributed Systems: Concepts and Design (5th Edition)|publisher = Addison-Wesley|year=2011|location=Boston|isbn=0-132-14301-1}}</ref> The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.<ref name=\"Coulouris\"/> Examples of distributed systems vary from [[Service-oriented architecture|SOA-based systems]] to [[massively multiplayer online game]]s to [[Peer-to-peer| peer-to-peer applications]].\n\nA [[computer program]] that runs in a distributed system is called a '''distributed program''', and distributed programming is the process of writing such programs.<ref>{{harvtxt|Andrews|2000}}. {{harvtxt|Dolev|2000}}. {{harvtxt|Ghosh|2007}}, p. 10.</ref> There are many alternatives for the message passing mechanism, including [[Remote procedure call|RPC-like]] connectors and [[Message-oriented middleware|message queues]].  An important goal and challenge of distributed systems is [[location transparency]].\n\n===Parallel computation===\n{{main|Parallel computation}}\n[[Parallel computing]] is a form of [[computing|computation]] in which many calculations are carried out simultaneously,<ref>{{cite book|last=Gottlieb|first=Allan|title=Highly parallel computing|year=1989|publisher=Benjamin/Cummings|location=Redwood City, Calif.|isbn=0-8053-0177-1|url=http://dl.acm.org/citation.cfm?id=160438|author2=Almasi, George S.}}</ref> operating on the principle that large problems can often be divided into smaller ones, which are then solved [[Parallelism (computing)|\"in parallel\"]]. There are several different forms of parallel computing: [[bit-level parallelism|bit-level]], [[instruction level parallelism|instruction level]], [[data parallelism|data]], and [[task parallelism]]. Parallelism has been employed for many years, mainly in [[high performance computing|high-performance computing]], but interest in it has grown lately due to the physical constraints preventing [[frequency scaling]].<ref>S.V. Adve et al. (November 2008). [http://www.upcrc.illinois.edu/documents/UPCRC_Whitepaper.pdf \"Parallel Computing Research at Illinois: The UPCRC Agenda\"] (PDF). Parallel@Illinois, University of Illinois at Urbana-Champaign. \"The main techniques for these performance benefits&nbsp;– increased clock frequency and smarter but increasingly complex architectures&nbsp;– are now hitting the so-called power wall. The computer industry has accepted that future performance increases must largely come from increasing the number of processors (or cores) on a die, rather than making a single core go faster.\"</ref> As power consumption (and consequently heat generation) by computers has become a concern in recent years,<ref>Asanovic et al. Old [conventional wisdom]: Power is free, but transistors are expensive. New [conventional wisdom] is [that] power is expensive, but transistors are \"free\".</ref> parallel computing has become the dominant paradigm in [[computer architecture]], mainly in the form of [[multi-core processor]]s.<ref name=\"View-Power\">Asanovic, Krste et al. (December 18, 2006). [http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf \"The Landscape of Parallel Computing Research: A View from Berkeley\"] (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. \"Old [conventional wisdom]: Increasing clock frequency is the primary method of improving processor performance. New [conventional wisdom]: Increasing parallelism is the primary method of improving processor performance&nbsp;... Even representatives from Intel, a company generally associated with the 'higher clock-speed is better' position, warned that traditional approaches to maximizing performance through maximizing clock speed have been pushed to their limit.\"</ref>\n\n[[Parallel algorithm|Parallel computer programs]] are more difficult to write than sequential ones,<ref>{{cite book|last=Hennessy|first=John L.|title=Computer organization and design : the hardware/software interface|year=1999|publisher=Kaufmann|location=San Francisco|isbn=1-55860-428-6|edition=2. ed., 3rd print.|author2=Patterson, David A. |author3=Larus, James R. }}</ref> because concurrency introduces several new classes of potential [[software bug]]s, of which [[race condition]]s are the most common. [[Computer networking|Communication]] and [[Synchronization (computer science)|synchronization]] between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.\n\nThe maximum possible [[speedup|speed-up]] of a single program as a result of parallelization is known as [[Amdahl's law]].\n\n===Very-large-scale integration===\n{{main|VLSI}}\n[[Very-large-scale integration]] ('''VLSI''') is the process of creating an [[integrated circuit]] (IC) by combining thousands of [[transistors]] into a single chip. VLSI began in the 1970s when complex [[semiconductor]] and [[communication]] technologies were being developed. The [[microprocessor]] is a VLSI device. Before the introduction of VLSI technology most ICs had a limited set of functions they could perform. An [[electronic circuit]] might consist of a [[central processing unit|CPU]], [[Read-only memory|ROM]], [[Random Access Memory|RAM]] and other [[glue logic]]. VLSI allows IC makers to add all of these circuits into one chip.\n\n===Machine learning===\n{{main|Machine learning}}\n[[Machine learning]] is a [[academic disciplines|scientific discipline]] that deals with the construction and study of [[algorithm]]s that can [[learning|learn]] from data.<ref>{{cite journal |title=Glossary of terms |author1=Ron Kovahi |author2=Foster Provost |journal=[[Machine Learning (journal)|Machine Learning]] |volume=30 |pages=271–274 |year=1998 |url=http://ai.stanford.edu/~ronnyk/glossary.html}}</ref> Such algorithms operate by building a [[Statistical model|model]] based on inputs<ref name=\"bishop\">{{cite book |author=C. M. Bishop |authorlink=Christopher M. Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |isbn=0-387-31073-8}}</ref>{{rp|2}} and using that to make predictions or decisions, rather than following only explicitly programmed instructions.\n\nMachine learning can be considered a subfield of computer science and [[statistics]]. It has strong ties to [[artificial intelligence]] and [[mathematical optimization|optimization]], which deliver methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit, rule-based [[algorithm]]s is infeasible. Example applications include [[spam filter]]ing, [[optical character recognition]] (OCR),<ref name=Wernick-Signal-Proc-July-2010>Wernick, Yang, Brankov, Yourganov and Strother, Machine Learning in Medical Imaging, ''[[IEEE Signal Processing Society|IEEE Signal Processing Magazine]]'', vol. 27, no. 4, July 2010, pp. 25-38</ref> [[Learning to rank|search engines]] and [[computer vision]]. Machine learning is sometimes conflated with [[data mining]],<ref>{{cite conference |last=Mannila |first=Heikki |title=Data mining: machine learning, statistics, and databases |conference=Int'l Conf. Scientific and Statistical Database Management |publisher=IEEE Computer Society |year=1996}}</ref> although that focuses more on exploratory data analysis.<ref>{{cite journal |last=Friedman |first=Jerome H. |authorlink=Jerome H. Friedman |title=Data Mining and Statistics: What's the connection? |journal=Computing Science and Statistics |volume=29 |issue=1 |year=1998 |pages=3–9}}</ref> Machine learning and [[pattern recognition]] \"can be viewed as two facets of\nthe same field.\"<ref name=\"bishop\"/>{{rp|vii}}\n\n===Computational biology===\n{{main|Computational biology}}\n[[Computational biology]] involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.<ref name=\"nih\">\n{{cite web\n| url          = http://www.bisti.nih.gov/docs/compubiodef.pdf\n| title        = NIH working definition of bioinformatics and computational biology\n| date         = 17 July 2000\n| accessdate   = 18 August 2012\n| publisher    = Biomedical Information Science and Technology Initiative\n}}\n</ref> The field is broadly defined and includes foundations in computer science, [[applied mathematics]], [[animation]], [[statistics]], [[biochemistry]], [[chemistry]], [[biophysics]], [[molecular biology]], [[genetics]], [[genomics]], [[ecology]], [[evolution]], [[anatomy]], [[neuroscience]], and [[scientific visualization|visualization]].<ref name=\"brown\">\n{{cite web\n| url          = http://www.brown.edu/research/projects/computational-molecular-biology/\n| title        = About the CCMB\n| accessdate   = 18 August 2012\n| publisher    = Center for Computational Molecular Biology\n}}\n</ref>\n\nComputational biology is different from [[biological computation]], which is a subfield of computer science and [[computer engineering]] using [[bioengineering]] and [[biology]] to build [[computer]]s, but is similar to [[bioinformatics]], which is an interdisciplinary science using computers to store and process biological data.\n\n===Computational geometry===\n{{main|Computational geometry}}\n[[Computational geometry]] is a branch of computer science devoted to the study of algorithms that can be stated in terms of [[geometry]]. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. An ancient precursor is the [[Sanskrit]] treatise [[Shulba Sutras]], or \"Rules of the Chord\", that is a book of algorithms written in 800 BCE. The book prescribes step-by-step procedures for constructing geometric objects like altars using a peg and chord.\n\nThe main impetus for the development of computational geometry as a discipline was progress in [[computer graphics]] and computer-aided design and manufacturing ([[Computer-aided design|CAD]]/[[Computer-aided manufacturing|CAM]]), but many problems in computational geometry are classical in nature, and may come from [[mathematical visualization]].\n\nOther important applications of computational geometry include [[robotics]] (motion planning and visibility problems), [[geographic information system]]s (GIS) (geometrical location and search, route planning), [[integrated circuit]] design (IC geometry design and verification), [[computer-aided engineering]] (CAE) (mesh generation), [[computer vision]] (3D reconstruction).\n\n===Information theory===\n{{main|Information theory}}\n[[Information theory]] is a branch of [[applied mathematics]], [[electrical engineering]], and [[computer science]] involving the [[Quantification (science)|quantification]] of [[information]].  Information theory was developed by [[Claude E. Shannon]] to find fundamental limits on [[signal processing]] operations such as [[data compression|compressing data]] and on reliably [[Computer data storage|storing]] and [[Telecommunication|communicating]] data. Since its inception it has broadened to find applications in many other areas, including [[statistical inference]], [[natural language processing]], [[cryptography]], [[neurobiology]],<ref>{{cite book|author1=F. Rieke |author2=D. Warland |author3=R Ruyter van Steveninck |author4=W Bialek |title=Spikes: Exploring the Neural Code|publisher=The MIT press|year=1997|isbn=978-0262681087}}</ref> the evolution<ref>cf. Huelsenbeck, J. P., F. Ronquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny and its impact on evolutionary biology, ''Science'' '''294''':2310-2314</ref> and function<ref>Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, [http://alum.mit.edu/www/toms/ Thomas D. Schneider], Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, ''Gene'' '''215''':1, 111-122</ref> of molecular codes, [[model selection]] in statistics,<ref>Burnham, K. P. and Anderson D. R. (2002) ''Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition'' (Springer Science, New York) {{isbn|978-0-387-95364-9}}.</ref> thermal physics,<ref>Jaynes, E. T. (1957) [http://bayes.wustl.edu/ Information Theory and Statistical Mechanics], ''Phys. Rev.'' '''106''':620</ref> [[quantum computing]], [[linguistics]], plagiarism detection,<ref>Charles H. Bennett, Ming Li, and Bin Ma (2003) [http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 Chain Letters and Evolutionary Histories], ''Scientific American'' '''288''':6, 76-81</ref> [[pattern recognition]], [[anomaly detection]] and other forms of [[data analysis]].<ref>\n{{Cite web\n | author = David R. Anderson\n | title = Some background on why people in the empirical sciences may want to better understand the information-theoretic methods\n | date = November 1, 2003\n | url = http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf\n | format = pdf\n | accessdate = 2010-06-23}}\n</ref>\n\nApplications of fundamental topics of information theory include [[lossless data compression]] (e.g. [[ZIP (file format)|ZIP files]]), [[lossy data compression]] (e.g. [[MP3]]s and [[JPEG]]s), and [[channel capacity|channel coding]] (e.g. for [[DSL|Digital Subscriber Line (DSL)]]).  The field is at the intersection of [[mathematics]], [[statistics]], [[computer science]], [[physics]], [[neurobiology]], and [[electrical engineering]]. Its impact has been crucial to the success of the [[Voyager program|Voyager]] missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the [[Internet]], the study of [[linguistics]] and of human perception, the understanding of [[black hole]]s, and numerous other fields. Important sub-fields of information theory are [[source coding]], [[channel coding]], [[algorithmic complexity theory]], [[algorithmic information theory]], [[information-theoretic security]], and measures of information.\n\n===Cryptography===\n{{main|Cryptography}}\n[[Cryptography]]  is the practice and study of techniques for [[secure communication]] in the presence of third parties (called [[adversary (cryptography)|adversaries]]).<ref name=\"rivest90\">{{cite book|first=Ronald L.|last=Rivest|authorlink=Ron Rivest|editor=J. Van Leeuwen|title=Handbook of Theoretical Computer Science|chapter=Cryptology|volume=1|publisher=Elsevier|year=1990}}</ref> More generally, it is about constructing and analyzing [[communications protocol|protocol]]s that overcome the influence of adversaries<ref name=\"modern-crypto\">{{Cite book|first1=Mihir|last1=Bellare|first2=Phillip|last2=Rogaway|title=Introduction to Modern Cryptography|chapter=Introduction|page=10|date=21 September 2005}}</ref> and that are related to various aspects in [[information security]] such as data [[confidentiality]], [[data integrity]], [[authentication]], and [[non-repudiation]].<ref name=\"hac\">{{cite book |first=A. J. |last=Menezes |first2=P. C. |last2=van Oorschot |first3=S. A. |last3=Vanstone |url=https://web.archive.org/web/20050307081354/www.cacr.math.uwaterloo.ca/hac/ |title=Handbook of Applied Cryptography |publisher= |isbn=0-8493-8523-7}}</ref> Modern cryptography intersects the disciplines of [[mathematics]], [[computer science]], and [[electrical engineering]]. Applications of cryptography include [[automated teller machine|ATM cards]], [[password|computer passwords]], and [[electronic commerce]].\n\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around [[computational hardness assumption]]s, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in [[integer factorization]] algorithms, and faster computing technology require these solutions to be continually adapted. There exist [[Information theoretic security|information-theoretically secure]] schemes that {{not a typo|provably}} cannot be broken even with unlimited computing power—an example is the [[one-time pad]]—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.\n\n===Quantum computation===\n{{main|Quantum computation}}\nA [[quantum computer]] is a [[computation]] system that makes direct use of [[quantum mechanics|quantum-mechanical]] [[phenomena]], such as [[quantum superposition|superposition]] and [[quantum entanglement|entanglement]], to perform [[Instruction (computer science)|operations]] on [[data]].<ref>\"[http://cba.mit.edu/docs/papers/98.06.sciqc.pdf Quantum Computing with Molecules]\" article in ''[[Scientific American]]'' by [[Neil Gershenfeld]] and [[Isaac L. Chuang]]</ref> Quantum computers are different from digital computers based on [[transistor]]s. Whereas digital computers require data to be encoded into binary digits ([[bit]]s), each of which is always in one of two definite states (0 or 1), quantum computation uses [[qubits]] (quantum bits), which can be in [[quantum superposition|superpositions]] of states. A theoretical model is the [[quantum Turing machine]], also known as the universal quantum computer.  Quantum computers share theoretical similarities with [[Non-deterministic Turing machine|non-deterministic]] and [[probabilistic automaton|probabilistic computers]]; one example is the ability to be in more than one state simultaneously.  The field of quantum computing was first introduced by [[Yuri Manin]] in 1980<ref name=\"manin1980vychislimoe\">{{cite book| author=Manin, Yu. I.| title=Vychislimoe i nevychislimoe |trans-title=Computable and Noncomputable | year=1980| publisher=Sov.Radio| url=http://publ.lib.ru/ARCHIVES/M/MANIN_Yuriy_Ivanovich/Manin_Yu.I._Vychislimoe_i_nevychislimoe.(1980).%5Bdjv%5D.zip| pages=13–15| language=Russian| accessdate=4 March 2013}}</ref> and [[Richard Feynman]] in 1982.<ref name=\"Feynman82\">{{cite journal |last=Feynman |first=R. P. |title=Simulating physics with computers |journal=[[International Journal of Theoretical Physics]] |year=1982 |volume=21 |issue=6 |pages=467–488 |doi=10.1007/BF02650179 }}</ref><ref>{{cite journal |title=Quantum computation |authorlink=David Deutsch |first=David |last=Deutsch |journal=Physics World |date=1992-01-06 }}</ref> A quantum computer with spins as quantum bits was also formulated for use as a quantum [[space–time]] in 1968.<ref>{{cite book |first=David |last=Finkelstein |chapter=Space-Time Structure in High Energy Interactions |title=Fundamental Interactions at High Energy |editor1-first=T. |editor1-last=Gudehus |editor2-first=G. |editor2-last=Kaiser |location=New York |publisher=Gordon & Breach |year=1968 }}</ref>\n\n{{as of|2014}}, quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on a very small number of qubits.<ref>{{cite web|url=http://phys.org/news/2013-01-qubit-bodes-future-quantum.html|title=New qubit control bodes well for future of quantum computing|publisher=|accessdate=26 October 2014}}</ref> Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum [[computer]]s for both civilian and national security purposes, such as [[cryptanalysis]].<ref>[http://qist.lanl.gov/qcomp_map.shtml Quantum Information Science and Technology Roadmap] for a sense of where the research is heading.</ref>\n\n===Information-based complexity===\n{{main|Information-based complexity}}\nInformation-based complexity (IBC) studies optimal algorithms and computational complexity for continuous problems. IBC has studied continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration.\n\n===Computational number theory===\n{{main|Computational number theory}}\n[[Computational number theory]], also known as '''algorithmic number theory''', is the study of [[algorithm]]s for performing [[number theory|number theoretic]] [[computation]]s. The best known problem in the field is [[integer factorization]].\n\n===Symbolic computation===\n{{main|Symbolic computation}}\n[[Computer algebra]], also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of [[algorithm]]s and [[software]] for manipulating [[expression (mathematics)|mathematical expressions]] and other [[mathematical object]]s. Although, properly speaking, computer algebra should be a subfield of [[scientific computing]], they are generally considered as distinct fields because scientific computing is usually based on [[numerical computation]] with approximate [[floating point number]]s, while symbolic computation emphasizes ''exact'' computation with expressions containing [[variable (mathematics)|variable]]s that have not any given value and are thus manipulated as symbols (therefore the name of ''symbolic computation'').\n\n[[Software]] applications that perform symbolic calculations are called ''[[computer algebra system]]s'', with the term ''system'' alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a [[user interface]] for the input/output of mathematical expressions, a large set of [[function (computer science)|routines]] to perform usual operations, like simplification of expressions, [[differentiation (mathematics)|differentiation]] using [[chain rule]], [[polynomial factorization]], [[indefinite integration]], etc.\n\n===Program semantics===\n{{main|Program semantics}}\nIn [[programming language theory]], '''semantics''' is the field concerned with the rigorous mathematical study of the meaning of [[programming language]]s. It does so by evaluating the meaning of [[programming language syntax|syntactically]] legal [[String (computer science)|strings]] defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically illegal strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will execute on a certain [[computer platform|platform]], hence creating a [[model of computation]].\n\n===Formal methods===\n{{main|Formal methods}}\n[[Formal methods]] are a particular kind of [[mathematics]] based techniques for the [[formal specification|specification]], development and [[formal verification|verification]] of [[software]] and [[computer hardware|hardware]] systems.<ref name=\"butler\">{{cite web|author=R. W. Butler|title=What is Formal Methods?|url=http://shemesh.larc.nasa.gov/fm/fm-what.html|date=2001-08-06|accessdate=2006-11-16}}</ref> The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.<ref>{{cite journal|author=C. Michael Holloway|title=Why Engineers Should Consider Formal Methods|url=http://klabs.org/richcontent/verification/holloway/nasa-97-16dasc-cmh.pdf| publisher=16th Digital Avionics Systems Conference (27–30 October 1997)|accessdate=2006-11-16}}</ref>\n\nFormal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular [[logic in computer science|logic]] calculi, [[formal language]]s, [[automata theory]], and [[program semantics]], but also [[type systems]] and [[algebraic data types]] to problems in software and hardware specification and verification.<ref>Monin, pp.3-4</ref>\n\n===Automata theory===\n{{main|Automata theory}}\n[[Automata theory]] is the study of ''[[abstract machine]]s'' and ''[[automaton|automata]]'', as well as the computational problems that can be solved using them. It is a theory in theoretical computer science, under [[Discrete mathematics]] (a section of [[Mathematics]] and also of [[Computer Science]]). ''Automata'' comes from the Greek word αὐτόματα meaning \"self-acting\".\n\nAutomata Theory is the study of self-operating virtual machines to help in logical understanding of input and output process, without or with intermediate stage(s) of [[computation]] (or any [[Function (engineering)|function]] / process).\n\n===Coding theory===\n{{main|Coding theory}}\n[[Coding theory]] is the study of the properties of codes and their fitness for a specific application. Codes are used for [[data compression]], [[cryptography]], [[error-correction]] and more recently also for [[network coding]]. Codes are studied by various scientific disciplines—such as [[information theory]], [[electrical engineering]],  [[mathematics]], and [[computer science]]—for the purpose of designing efficient and reliable [[data transmission]] methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.\n\n===Computational learning theory===\n{{main|Computational learning theory}}\nTheoretical results in machine learning mainly deal with a type of\ninductive learning called supervised learning.  In supervised\nlearning, an algorithm is given samples that are labeled in some\nuseful way.  For example, the samples might be descriptions of\nmushrooms, and the labels could be whether or not the mushrooms are\nedible.  The algorithm takes these previously labeled samples and\nuses them to induce a classifier.  This classifier is a function that\nassigns labels to samples including the samples that have never been\npreviously seen by the algorithm.  The goal of the supervised learning\nalgorithm is to optimize some measure of performance such as\nminimizing the number of mistakes made on new samples.\n\n== Organizations ==\n* [[European Association for Theoretical Computer Science]]\n* [[SIGACT]]\n* [[Simons Institute for the Theory of Computing]]\n\n== Journals and newsletters ==\n* ''[[Information and Computation]]''\n* ''[[Theory of Computing (journal)|Theory of Computing]]'' ([[Open access (publishing)|open access]] journal)\n* ''[[Formal Aspects of Computing]]''\n* ''[[Journal of the ACM]]''\n* ''[[SIAM Journal on Computing]]'' (SICOMP)\n* ''[[SIGACT News]]''\n* ''[[Theoretical Computer Science (journal)|Theoretical Computer Science]]''\n* ''[[Theory of Computing Systems]]''\n* ''[[International Journal of Foundations of Computer Science]]''\n* ''[[Chicago Journal of Theoretical Computer Science]]'' ([[Open access (publishing)|open access]] journal)\n* ''[[Foundations and Trends in Theoretical Computer Science]]''\n* ''[[Journal of Automata, Languages and Combinatorics]]''\n* ''[[Acta Informatica]]''\n* ''[[Fundamenta Informaticae]]''\n* ''[[ACM Transactions on Computation Theory]]''\n* ''[[Computational Complexity (journal)|Computational Complexity]]''\n* ''[[Journal of Complexity (journal)|Journal of Complexity]]''\n* ACM Transactions on Algorithms\n* Information Processing Letters\n\n== Conferences ==\n* Annual ACM [[Symposium on Theory of Computing]] (STOC)<ref name=\"core-a-plus\">The [http://www.core.edu.au/rankings/Conference%20Ranking%20Main.html 2007 Australian Ranking of ICT Conferences]: tier A+.</ref>\n* Annual IEEE [[Symposium on Foundations of Computer Science]] (FOCS)<ref name=\"core-a-plus\"/>\n* [[Innovations in Theoretical Computer Science]] (ITCS)\n* [[Mathematical Foundations of Computer Science]] (MFCS)<ref>[http://mfcs2017.cs.aau.dk/ MFCS 2017]</ref>\n* [[International Computer Science Symposium in Russia]] (CSR)<ref>[https://logic.pdmi.ras.ru/csr2018/ CSR 2018]</ref>\n* ACM–SIAM [[Symposium on Discrete Algorithms]] (SODA)<ref name=\"core-a-plus\"/>\n* IEEE [[Symposium on Logic in Computer Science]] (LICS)<ref name=\"core-a-plus\"/>\n* [[Computational Complexity Conference]] (CCC)<ref name=\"core-a\"/>\n* [[International Colloquium on Automata, Languages and Programming]] (ICALP)<ref name=\"core-a\"/>\n* Annual [[Symposium on Computational Geometry]] (SoCG)<ref name=\"core-a\">The [http://www.core.edu.au/rankings/Conference%20Ranking%20Main.html 2007 Australian Ranking of ICT Conferences]: tier A.</ref>\n* ACM [[Symposium on Principles of Distributed Computing]] (PODC)<ref name=\"core-a-plus\"/>\n* ACM [[Symposium on Parallelism in Algorithms and Architectures]] (SPAA)<ref name=\"core-a\"/>\n* [[Annual Conference on Learning Theory]] (COLT)<ref name=\"core-a\"/>\n* [[Symposium on Theoretical Aspects of Computer Science]] (STACS)<ref name=\"core-a\"/>\n* [[European Symposium on Algorithms]] (ESA)<ref name=\"core-a\"/>\n* [[Workshop on Approximation Algorithms for Combinatorial Optimization Problems]] (APPROX)<ref name=\"core-a\"/>\n* [[Workshop on Randomization and Computation]] (RANDOM)<ref name=\"core-a\"/>\n* [[International Symposium on Algorithms and Computation]] (ISAAC)<ref name=\"core-a\"/>\n* [[International Symposium on Fundamentals of Computation Theory]] (FCT)<ref>[http://fct11.ifi.uio.no/ FCT 2011] (retrieved 2013-06-03)</ref>\n* [[International Workshop on Graph-Theoretic Concepts in Computer Science]] (WG)\n\n==See also==\n* [[Formal science]]\n* [[Unsolved problems in computer science]]\n* [[List of important publications in theoretical computer science]]\n\n== Notes ==\n<references/>\n\n== Further reading ==\n\n* [[Martin Davis]], Ron Sigal, Elaine J. Weyuker, ''Computability, complexity, and languages: fundamentals of theoretical computer science'', 2nd ed., Academic Press, 1994, {{isbn|0-12-206382-1}}. Covers [[theory of computation]], but also [[program semantics]] and [[quantification theory]]. Aimed at graduate students.\n\n== External links ==\n* [http://www.sigact.org/webpages.php SIGACT directory of additional theory links]\n* [http://theorymatters.org/ Theory Matters Wiki] Theoretical Computer Science (TCS) Advocacy Wiki\n* [news://comp.theory Usenet comp.theory]\n* [http://www.confsearch.org/confsearch/faces/pages/topic.jsp?topic=Theory&sortMode=1&graphicView=1 List of academic conferences in the area of theoretical computer science] at [http://www.confsearch.org confsearch]\n* [http://cstheory.stackexchange.com/ Theoretical Computer Science - StackExchange], a Question and Answer site for researchers in theoretical computer science\n* [http://www.csanimated.com/browse.php Computer Science Animated]\n* http://theory.csail.mit.edu/   @ [[Massachusetts Institute of Technology]]\n\n{{Computer science}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Theoretical Computer Science}}\n[[Category:Theoretical computer science|*]]\n[[Category:Formal sciences]]",
            "slug": "",
            "date_updated": 1517191241504,
            "imported": "https://en.wikipedia.org/wiki/Theoretical_computer_science"
        },
        {
            "title": "",
            "text": "{{about|the branch of computer science and mathematics|the journal|Theoretical Computer Science (journal)}}\n[[Image:Maquina.png|thumb|An artistic representation of a [[Turing machine]]. Turing machines are used to model general computing devices.]]\n\n'''Theoretical computer science''', or TCS, is a subset of general [[computer science]] and [[mathematics]] that focuses on more mathematical topics \nof computing and includes the [[theory of computation]].\n\nIt is difficult to circumscribe the theoretical areas precisely. The [[Association for Computing Machinery|ACM]]'s [[ACM SIGACT|Special Interest Group on Algorithms and Computation Theory]] (SIGACT) provides the following description:<ref>{{cite web | title =  SIGACT | url = https://www.sigact.org/ | accessdate = 2017-01-19}}</ref>\n\n{{\"|TCS covers a wide variety of topics including [[algorithms]], [[data structure]]s, [[computational complexity theory|computational complexity]], [[parallel computation|parallel]] and [[distributed computation|distributed]] computation, [[probabilistic computation]], [[quantum computation]], [[automata theory]], [[information theory]], [[cryptography]], [[program semantics]] and [[Formal methods|verification]], [[machine learning]], [[computational biology]], [[computational economics]], [[computational geometry]], and [[computational number theory]] and [[Symbolic computation|algebra]]. Work in this field is often distinguished by its emphasis on mathematical technique and [[rigor#Mathematical rigour|rigor]].}}\n\nIn this list, the ACM's journal Transactions on Computation Theory includes [[coding theory]] and [[computational learning theory]], as well as theoretical computer science aspects of areas such as [[databases]], [[information retrieval]], economic models, and [[Computer network|networks]].<ref>{{cite web | title =  ToCT| url = http://toct.acm.org/journal.html | accessdate = 2010-06-09}}</ref> Despite this broad scope, the \"theory people\" in computer science self-identify as different from the \"applied people\"{{Citation needed|date=September 2017}}. Some characterize themselves as doing the \"(more fundamental) 'science(s)' underlying the field of computing.\"<ref>{{cite web | title = Challenges for Theoretical Computer Science: Theory as the Scientific Foundation of Computing | url = http://www.research.att.com/%7Edsj/nsflist.html#Intro | accessdate = 2009-03-29}}</ref> Other \"theory-applied people\" suggest that it is impossible to separate theory and application. This means that the so-called \"theory people\" regularly use experimental science(s) done in less-theoretical areas such as [[software system]] research{{Citation needed|date=September 2017}}. It also means that there is more cooperation than mutually exclusive competition between theory and application{{Citation needed|date=September 2017}}.\n\n== History ==\n{{Main|History of computer science}}\n\nWhile logical inference and mathematical proof had existed previously, in 1931 [[Kurt Gödel]] proved with his [[incompleteness theorem]] that there are fundamental limitations on what statements could be proved or disproved.\n\nThese developments have led to the modern study of logic and [[computability]], and indeed the field of theoretical computer science as a whole{{Citation needed|date=September 2017}}. [[Information theory]] was added to the field{{by whom|date=September 2017}} with a 1948 mathematical theory of communication by [[Claude Shannon]]. In the same decade, [[Donald Hebb]] introduced a mathematical model of [[Hebbian learning|learning]] in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of [[neural network]]s and [[parallel distributed processing]] were established. In 1971, [[Stephen Cook]] and, working independently, [[Leonid Levin]], proved that there exist practically relevant problems that are [[NP-complete]] &ndash; a landmark result in [[computational complexity theory]]{{Citation needed|date=September 2017}}.\n\nWith the development of [[quantum mechanics]] in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously. This led to the concept of a [[quantum computer]] in the latter half of the 20th century that took off in the 1990s when [[Peter Shor]] showed that such methods could be used to factor large numbers in [[polynomial time]], which, if implemented, would render most modern [[public key cryptography]] systems uselessly insecure.{{citation needed|reason=Your explanation here|date=October 2014}}\n\nModern theoretical computer science research is based on these basic developments, but includes many other mathematical and interdisciplinary problems that have been posed, as shown below:\n\n{| style=\"border:1px solid #ddd; text-align:center;  margin: 0 auto;\" cellspacing=\"15\"\n| <math> P \\rightarrow Q \\,</math>\n| [[File:DFAexample.svg|96px]]\n| [[File:Elliptic curve simple.png|96px]]\n| [[File:6n-graf.svg|96px]]\n| [[File:Wang tiles.svg|96px]]\n| '''P = NP''' ?\n|-\n| [[Mathematical logic]]\n| [[Automata theory]]\n| [[Number theory]]\n| [[Graph theory]]\n| [[Computability theory]]\n| [[Computational complexity theory]]\n|-\n| '''GNITIRW-TERCES'''\n| <math>\\Gamma\\vdash x: \\text{Int}</math>\n| [[File:Commutative diagram for morphism.svg|96px]]\n| [[File:SimplexRangeSearching.svg|96px]]\n| [[File:TSP Deutschland 3.png|96px]]\n| [[File:Blochsphere.svg|96px]]\n|-\n| [[Cryptography]]\n| [[Type theory]]\n| [[Category theory]]\n| [[Computational geometry]]\n| [[Combinatorial optimization]]\n| [[Quantum computer|Quantum computing theory]]\n|}\n\n== Topics ==\n\n===Algorithms===\n{{main|Algorithm}}\nAn [[algorithm]] is a step-by-step procedure for calculations.  Algorithms are used for [[calculation]], [[data processing]], and [[automated reasoning]].\n\nAn algorithm is an [[effective method]] expressed as a [[wikt:finite|finite]] list<ref>\"Any classical mathematical algorithm, for example, can be described in a finite number of English words\" (Rogers 1987:2).</ref> of well-defined instructions<ref>Well defined with respect to the agent that executes the algorithm: \"There is a computing agent, usually human, which can react to the instructions and carry out the computations\" (Rogers 1987:2).</ref> for calculating a [[Function (mathematics)|function]].<ref>\"an algorithm is a procedure for computing a ''function'' (with respect to some chosen notation for integers) ... this limitation (to numerical functions) results in no loss of generality\", (Rogers 1987:1).</ref>  Starting from an initial state and initial input (perhaps [[null string|empty]]),<ref>\"An algorithm has [[zero]] or more inputs, i.e., [[quantity|quantities]] which are given to it initially before the algorithm begins\" (Knuth 1973:5).</ref> the instructions describe a [[computation]] that, when [[Execution (computing)|executed]], proceeds through a finite<ref>\"A procedure which has all the characteristics of an algorithm except that it possibly lacks finiteness may be called a 'computational method'\" (Knuth 1973:5).</ref> number of well-defined successive states, eventually producing \"output\"<ref>\"An algorithm has one or more outputs, i.e. quantities which have a specified relation to the inputs\" (Knuth 1973:5).</ref> and terminating at a final ending state. The transition from one state to the next is not necessarily [[deterministic]]; some algorithms, known as [[randomized algorithms]], incorporate random input.<ref>Whether or not a process with random interior processes (not including the input) is an algorithm is debatable. Rogers opines that: \"a computation is carried out in a discrete stepwise fashion, without use of continuous methods or analogue devices . . . carried forward deterministically, without resort to random methods or devices, e.g., dice\" Rogers 1987:2.</ref>\n\n===Data structures===\n{{main|Data structure}}\nA [[data structure]] is a particular way of organizing [[data (computing)|data]] in a computer so that it can be used [[algorithmic efficiency|efficiently]].<ref>Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. U.S. [[National Institute of Standards and Technology]]. 15 December 2004. [http://xlinux.nist.gov/dads/HTML/datastructur.html Online version] Accessed May 21, 2009.''</ref><ref>Entry ''data structure'' in the [[Encyclopædia Britannica]] (2009) [http://www.britannica.com/EBchecked/topic/152190/data-structure Online entry] accessed on May 21, 2009.</ref>\n\nDifferent kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use [[B-tree]] indexes for small percentages of data retrieval and [[compiler]]s and databases use dynamic [[hash table]]s as look up tables.\n\nData structures provide a means to manage large amounts of data efficiently for uses such as large [[database]]s and [[web indexing|internet indexing services]]. Usually, efficient data structures are key to designing efficient [[algorithm]]s. Some formal design methods and [[programming language]]s emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both [[main memory]] and in [[secondary memory]].\n\n===Computational complexity theory===\n{{main|Computational complexity theory}}\n[[Computational complexity theory]] is a branch of the [[theory of computation]] that focuses on classifying [[computational problems]] according to their inherent difficulty, and relating those [[Complexity class|classes]] to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an [[algorithm]].\n\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the [[algorithm]] used. The theory formalizes this intuition, by introducing mathematical [[models of computation]] to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other [[complexity]] measures are also used, such as the amount of communication (used in [[communication complexity]]), the number of [[logic gate|gates]] in a circuit (used in [[circuit complexity]]) and the number of processors (used in [[parallel computing]]). One of the roles of computational complexity theory is to determine the practical limits on what [[computer]]s can and cannot do.\n\n===Distributed computation===\n{{main|Distributed computation}}\n[[Distributed computing]] studies distributed systems. A distributed system is a software system in which components located on [[computer network|networked computers]] communicate and coordinate their actions by [[message passing|passing messages]].<ref name=\"Coulouris\">{{cite book|last=Coulouris|first=George|author2=Jean Dollimore|author3=Tim Kindberg|author4=Gordon Blair|title=Distributed Systems: Concepts and Design (5th Edition)|publisher = Addison-Wesley|year=2011|location=Boston|isbn=0-132-14301-1}}</ref> The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.<ref name=\"Coulouris\"/> Examples of distributed systems vary from [[Service-oriented architecture|SOA-based systems]] to [[massively multiplayer online game]]s to [[Peer-to-peer| peer-to-peer applications]].\n\nA [[computer program]] that runs in a distributed system is called a '''distributed program''', and distributed programming is the process of writing such programs.<ref>{{harvtxt|Andrews|2000}}. {{harvtxt|Dolev|2000}}. {{harvtxt|Ghosh|2007}}, p. 10.</ref> There are many alternatives for the message passing mechanism, including [[Remote procedure call|RPC-like]] connectors and [[Message-oriented middleware|message queues]].  An important goal and challenge of distributed systems is [[location transparency]].\n\n===Parallel computation===\n{{main|Parallel computation}}\n[[Parallel computing]] is a form of [[computing|computation]] in which many calculations are carried out simultaneously,<ref>{{cite book|last=Gottlieb|first=Allan|title=Highly parallel computing|year=1989|publisher=Benjamin/Cummings|location=Redwood City, Calif.|isbn=0-8053-0177-1|url=http://dl.acm.org/citation.cfm?id=160438|author2=Almasi, George S.}}</ref> operating on the principle that large problems can often be divided into smaller ones, which are then solved [[Parallelism (computing)|\"in parallel\"]]. There are several different forms of parallel computing: [[bit-level parallelism|bit-level]], [[instruction level parallelism|instruction level]], [[data parallelism|data]], and [[task parallelism]]. Parallelism has been employed for many years, mainly in [[high performance computing|high-performance computing]], but interest in it has grown lately due to the physical constraints preventing [[frequency scaling]].<ref>S.V. Adve et al. (November 2008). [http://www.upcrc.illinois.edu/documents/UPCRC_Whitepaper.pdf \"Parallel Computing Research at Illinois: The UPCRC Agenda\"] (PDF). Parallel@Illinois, University of Illinois at Urbana-Champaign. \"The main techniques for these performance benefits&nbsp;– increased clock frequency and smarter but increasingly complex architectures&nbsp;– are now hitting the so-called power wall. The computer industry has accepted that future performance increases must largely come from increasing the number of processors (or cores) on a die, rather than making a single core go faster.\"</ref> As power consumption (and consequently heat generation) by computers has become a concern in recent years,<ref>Asanovic et al. Old [conventional wisdom]: Power is free, but transistors are expensive. New [conventional wisdom] is [that] power is expensive, but transistors are \"free\".</ref> parallel computing has become the dominant paradigm in [[computer architecture]], mainly in the form of [[multi-core processor]]s.<ref name=\"View-Power\">Asanovic, Krste et al. (December 18, 2006). [http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf \"The Landscape of Parallel Computing Research: A View from Berkeley\"] (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. \"Old [conventional wisdom]: Increasing clock frequency is the primary method of improving processor performance. New [conventional wisdom]: Increasing parallelism is the primary method of improving processor performance&nbsp;... Even representatives from Intel, a company generally associated with the 'higher clock-speed is better' position, warned that traditional approaches to maximizing performance through maximizing clock speed have been pushed to their limit.\"</ref>\n\n[[Parallel algorithm|Parallel computer programs]] are more difficult to write than sequential ones,<ref>{{cite book|last=Hennessy|first=John L.|title=Computer organization and design : the hardware/software interface|year=1999|publisher=Kaufmann|location=San Francisco|isbn=1-55860-428-6|edition=2. ed., 3rd print.|author2=Patterson, David A. |author3=Larus, James R. }}</ref> because concurrency introduces several new classes of potential [[software bug]]s, of which [[race condition]]s are the most common. [[Computer networking|Communication]] and [[Synchronization (computer science)|synchronization]] between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.\n\nThe maximum possible [[speedup|speed-up]] of a single program as a result of parallelization is known as [[Amdahl's law]].\n\n===Very-large-scale integration===\n{{main|VLSI}}\n[[Very-large-scale integration]] ('''VLSI''') is the process of creating an [[integrated circuit]] (IC) by combining thousands of [[transistors]] into a single chip. VLSI began in the 1970s when complex [[semiconductor]] and [[communication]] technologies were being developed. The [[microprocessor]] is a VLSI device. Before the introduction of VLSI technology most ICs had a limited set of functions they could perform. An [[electronic circuit]] might consist of a [[central processing unit|CPU]], [[Read-only memory|ROM]], [[Random Access Memory|RAM]] and other [[glue logic]]. VLSI allows IC makers to add all of these circuits into one chip.\n\n===Machine learning===\n{{main|Machine learning}}\n[[Machine learning]] is a [[academic disciplines|scientific discipline]] that deals with the construction and study of [[algorithm]]s that can [[learning|learn]] from data.<ref>{{cite journal |title=Glossary of terms |author1=Ron Kovahi |author2=Foster Provost |journal=[[Machine Learning (journal)|Machine Learning]] |volume=30 |pages=271–274 |year=1998 |url=http://ai.stanford.edu/~ronnyk/glossary.html}}</ref> Such algorithms operate by building a [[Statistical model|model]] based on inputs<ref name=\"bishop\">{{cite book |author=C. M. Bishop |authorlink=Christopher M. Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |isbn=0-387-31073-8}}</ref>{{rp|2}} and using that to make predictions or decisions, rather than following only explicitly programmed instructions.\n\nMachine learning can be considered a subfield of computer science and [[statistics]]. It has strong ties to [[artificial intelligence]] and [[mathematical optimization|optimization]], which deliver methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit, rule-based [[algorithm]]s is infeasible. Example applications include [[spam filter]]ing, [[optical character recognition]] (OCR),<ref name=Wernick-Signal-Proc-July-2010>Wernick, Yang, Brankov, Yourganov and Strother, Machine Learning in Medical Imaging, ''[[IEEE Signal Processing Society|IEEE Signal Processing Magazine]]'', vol. 27, no. 4, July 2010, pp. 25-38</ref> [[Learning to rank|search engines]] and [[computer vision]]. Machine learning is sometimes conflated with [[data mining]],<ref>{{cite conference |last=Mannila |first=Heikki |title=Data mining: machine learning, statistics, and databases |conference=Int'l Conf. Scientific and Statistical Database Management |publisher=IEEE Computer Society |year=1996}}</ref> although that focuses more on exploratory data analysis.<ref>{{cite journal |last=Friedman |first=Jerome H. |authorlink=Jerome H. Friedman |title=Data Mining and Statistics: What's the connection? |journal=Computing Science and Statistics |volume=29 |issue=1 |year=1998 |pages=3–9}}</ref> Machine learning and [[pattern recognition]] \"can be viewed as two facets of\nthe same field.\"<ref name=\"bishop\"/>{{rp|vii}}\n\n===Computational biology===\n{{main|Computational biology}}\n[[Computational biology]] involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.<ref name=\"nih\">\n{{cite web\n| url          = http://www.bisti.nih.gov/docs/compubiodef.pdf\n| title        = NIH working definition of bioinformatics and computational biology\n| date         = 17 July 2000\n| accessdate   = 18 August 2012\n| publisher    = Biomedical Information Science and Technology Initiative\n}}\n</ref> The field is broadly defined and includes foundations in computer science, [[applied mathematics]], [[animation]], [[statistics]], [[biochemistry]], [[chemistry]], [[biophysics]], [[molecular biology]], [[genetics]], [[genomics]], [[ecology]], [[evolution]], [[anatomy]], [[neuroscience]], and [[scientific visualization|visualization]].<ref name=\"brown\">\n{{cite web\n| url          = http://www.brown.edu/research/projects/computational-molecular-biology/\n| title        = About the CCMB\n| accessdate   = 18 August 2012\n| publisher    = Center for Computational Molecular Biology\n}}\n</ref>\n\nComputational biology is different from [[biological computation]], which is a subfield of computer science and [[computer engineering]] using [[bioengineering]] and [[biology]] to build [[computer]]s, but is similar to [[bioinformatics]], which is an interdisciplinary science using computers to store and process biological data.\n\n===Computational geometry===\n{{main|Computational geometry}}\n[[Computational geometry]] is a branch of computer science devoted to the study of algorithms that can be stated in terms of [[geometry]]. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. An ancient precursor is the [[Sanskrit]] treatise [[Shulba Sutras]], or \"Rules of the Chord\", that is a book of algorithms written in 800 BCE. The book prescribes step-by-step procedures for constructing geometric objects like altars using a peg and chord.\n\nThe main impetus for the development of computational geometry as a discipline was progress in [[computer graphics]] and computer-aided design and manufacturing ([[Computer-aided design|CAD]]/[[Computer-aided manufacturing|CAM]]), but many problems in computational geometry are classical in nature, and may come from [[mathematical visualization]].\n\nOther important applications of computational geometry include [[robotics]] (motion planning and visibility problems), [[geographic information system]]s (GIS) (geometrical location and search, route planning), [[integrated circuit]] design (IC geometry design and verification), [[computer-aided engineering]] (CAE) (mesh generation), [[computer vision]] (3D reconstruction).\n\n===Information theory===\n{{main|Information theory}}\n[[Information theory]] is a branch of [[applied mathematics]], [[electrical engineering]], and [[computer science]] involving the [[Quantification (science)|quantification]] of [[information]].  Information theory was developed by [[Claude E. Shannon]] to find fundamental limits on [[signal processing]] operations such as [[data compression|compressing data]] and on reliably [[Computer data storage|storing]] and [[Telecommunication|communicating]] data. Since its inception it has broadened to find applications in many other areas, including [[statistical inference]], [[natural language processing]], [[cryptography]], [[neurobiology]],<ref>{{cite book|author1=F. Rieke |author2=D. Warland |author3=R Ruyter van Steveninck |author4=W Bialek |title=Spikes: Exploring the Neural Code|publisher=The MIT press|year=1997|isbn=978-0262681087}}</ref> the evolution<ref>cf. Huelsenbeck, J. P., F. Ronquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny and its impact on evolutionary biology, ''Science'' '''294''':2310-2314</ref> and function<ref>Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, [http://alum.mit.edu/www/toms/ Thomas D. Schneider], Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, ''Gene'' '''215''':1, 111-122</ref> of molecular codes, [[model selection]] in statistics,<ref>Burnham, K. P. and Anderson D. R. (2002) ''Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition'' (Springer Science, New York) {{isbn|978-0-387-95364-9}}.</ref> thermal physics,<ref>Jaynes, E. T. (1957) [http://bayes.wustl.edu/ Information Theory and Statistical Mechanics], ''Phys. Rev.'' '''106''':620</ref> [[quantum computing]], [[linguistics]], plagiarism detection,<ref>Charles H. Bennett, Ming Li, and Bin Ma (2003) [http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 Chain Letters and Evolutionary Histories], ''Scientific American'' '''288''':6, 76-81</ref> [[pattern recognition]], [[anomaly detection]] and other forms of [[data analysis]].<ref>\n{{Cite web\n | author = David R. Anderson\n | title = Some background on why people in the empirical sciences may want to better understand the information-theoretic methods\n | date = November 1, 2003\n | url = http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf\n | format = pdf\n | accessdate = 2010-06-23}}\n</ref>\n\nApplications of fundamental topics of information theory include [[lossless data compression]] (e.g. [[ZIP (file format)|ZIP files]]), [[lossy data compression]] (e.g. [[MP3]]s and [[JPEG]]s), and [[channel capacity|channel coding]] (e.g. for [[DSL|Digital Subscriber Line (DSL)]]).  The field is at the intersection of [[mathematics]], [[statistics]], [[computer science]], [[physics]], [[neurobiology]], and [[electrical engineering]]. Its impact has been crucial to the success of the [[Voyager program|Voyager]] missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the [[Internet]], the study of [[linguistics]] and of human perception, the understanding of [[black hole]]s, and numerous other fields. Important sub-fields of information theory are [[source coding]], [[channel coding]], [[algorithmic complexity theory]], [[algorithmic information theory]], [[information-theoretic security]], and measures of information.\n\n===Cryptography===\n{{main|Cryptography}}\n[[Cryptography]]  is the practice and study of techniques for [[secure communication]] in the presence of third parties (called [[adversary (cryptography)|adversaries]]).<ref name=\"rivest90\">{{cite book|first=Ronald L.|last=Rivest|authorlink=Ron Rivest|editor=J. Van Leeuwen|title=Handbook of Theoretical Computer Science|chapter=Cryptology|volume=1|publisher=Elsevier|year=1990}}</ref> More generally, it is about constructing and analyzing [[communications protocol|protocol]]s that overcome the influence of adversaries<ref name=\"modern-crypto\">{{Cite book|first1=Mihir|last1=Bellare|first2=Phillip|last2=Rogaway|title=Introduction to Modern Cryptography|chapter=Introduction|page=10|date=21 September 2005}}</ref> and that are related to various aspects in [[information security]] such as data [[confidentiality]], [[data integrity]], [[authentication]], and [[non-repudiation]].<ref name=\"hac\">{{cite book |first=A. J. |last=Menezes |first2=P. C. |last2=van Oorschot |first3=S. A. |last3=Vanstone |url=https://web.archive.org/web/20050307081354/www.cacr.math.uwaterloo.ca/hac/ |title=Handbook of Applied Cryptography |publisher= |isbn=0-8493-8523-7}}</ref> Modern cryptography intersects the disciplines of [[mathematics]], [[computer science]], and [[electrical engineering]]. Applications of cryptography include [[automated teller machine|ATM cards]], [[password|computer passwords]], and [[electronic commerce]].\n\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around [[computational hardness assumption]]s, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in [[integer factorization]] algorithms, and faster computing technology require these solutions to be continually adapted. There exist [[Information theoretic security|information-theoretically secure]] schemes that {{not a typo|provably}} cannot be broken even with unlimited computing power—an example is the [[one-time pad]]—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.\n\n===Quantum computation===\n{{main|Quantum computation}}\nA [[quantum computer]] is a [[computation]] system that makes direct use of [[quantum mechanics|quantum-mechanical]] [[phenomena]], such as [[quantum superposition|superposition]] and [[quantum entanglement|entanglement]], to perform [[Instruction (computer science)|operations]] on [[data]].<ref>\"[http://cba.mit.edu/docs/papers/98.06.sciqc.pdf Quantum Computing with Molecules]\" article in ''[[Scientific American]]'' by [[Neil Gershenfeld]] and [[Isaac L. Chuang]]</ref> Quantum computers are different from digital computers based on [[transistor]]s. Whereas digital computers require data to be encoded into binary digits ([[bit]]s), each of which is always in one of two definite states (0 or 1), quantum computation uses [[qubits]] (quantum bits), which can be in [[quantum superposition|superpositions]] of states. A theoretical model is the [[quantum Turing machine]], also known as the universal quantum computer.  Quantum computers share theoretical similarities with [[Non-deterministic Turing machine|non-deterministic]] and [[probabilistic automaton|probabilistic computers]]; one example is the ability to be in more than one state simultaneously.  The field of quantum computing was first introduced by [[Yuri Manin]] in 1980<ref name=\"manin1980vychislimoe\">{{cite book| author=Manin, Yu. I.| title=Vychislimoe i nevychislimoe |trans-title=Computable and Noncomputable | year=1980| publisher=Sov.Radio| url=http://publ.lib.ru/ARCHIVES/M/MANIN_Yuriy_Ivanovich/Manin_Yu.I._Vychislimoe_i_nevychislimoe.(1980).%5Bdjv%5D.zip| pages=13–15| language=Russian| accessdate=4 March 2013}}</ref> and [[Richard Feynman]] in 1982.<ref name=\"Feynman82\">{{cite journal |last=Feynman |first=R. P. |title=Simulating physics with computers |journal=[[International Journal of Theoretical Physics]] |year=1982 |volume=21 |issue=6 |pages=467–488 |doi=10.1007/BF02650179 }}</ref><ref>{{cite journal |title=Quantum computation |authorlink=David Deutsch |first=David |last=Deutsch |journal=Physics World |date=1992-01-06 }}</ref> A quantum computer with spins as quantum bits was also formulated for use as a quantum [[space–time]] in 1968.<ref>{{cite book |first=David |last=Finkelstein |chapter=Space-Time Structure in High Energy Interactions |title=Fundamental Interactions at High Energy |editor1-first=T. |editor1-last=Gudehus |editor2-first=G. |editor2-last=Kaiser |location=New York |publisher=Gordon & Breach |year=1968 }}</ref>\n\n{{as of|2014}}, quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on a very small number of qubits.<ref>{{cite web|url=http://phys.org/news/2013-01-qubit-bodes-future-quantum.html|title=New qubit control bodes well for future of quantum computing|publisher=|accessdate=26 October 2014}}</ref> Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum [[computer]]s for both civilian and national security purposes, such as [[cryptanalysis]].<ref>[http://qist.lanl.gov/qcomp_map.shtml Quantum Information Science and Technology Roadmap] for a sense of where the research is heading.</ref>\n\n===Information-based complexity===\n{{main|Information-based complexity}}\nInformation-based complexity (IBC) studies optimal algorithms and computational complexity for continuous problems. IBC has studied continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration.\n\n===Computational number theory===\n{{main|Computational number theory}}\n[[Computational number theory]], also known as '''algorithmic number theory''', is the study of [[algorithm]]s for performing [[number theory|number theoretic]] [[computation]]s. The best known problem in the field is [[integer factorization]].\n\n===Symbolic computation===\n{{main|Symbolic computation}}\n[[Computer algebra]], also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of [[algorithm]]s and [[software]] for manipulating [[expression (mathematics)|mathematical expressions]] and other [[mathematical object]]s. Although, properly speaking, computer algebra should be a subfield of [[scientific computing]], they are generally considered as distinct fields because scientific computing is usually based on [[numerical computation]] with approximate [[floating point number]]s, while symbolic computation emphasizes ''exact'' computation with expressions containing [[variable (mathematics)|variable]]s that have not any given value and are thus manipulated as symbols (therefore the name of ''symbolic computation'').\n\n[[Software]] applications that perform symbolic calculations are called ''[[computer algebra system]]s'', with the term ''system'' alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a [[user interface]] for the input/output of mathematical expressions, a large set of [[function (computer science)|routines]] to perform usual operations, like simplification of expressions, [[differentiation (mathematics)|differentiation]] using [[chain rule]], [[polynomial factorization]], [[indefinite integration]], etc.\n\n===Program semantics===\n{{main|Program semantics}}\nIn [[programming language theory]], '''semantics''' is the field concerned with the rigorous mathematical study of the meaning of [[programming language]]s. It does so by evaluating the meaning of [[programming language syntax|syntactically]] legal [[String (computer science)|strings]] defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically illegal strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will execute on a certain [[computer platform|platform]], hence creating a [[model of computation]].\n\n===Formal methods===\n{{main|Formal methods}}\n[[Formal methods]] are a particular kind of [[mathematics]] based techniques for the [[formal specification|specification]], development and [[formal verification|verification]] of [[software]] and [[computer hardware|hardware]] systems.<ref name=\"butler\">{{cite web|author=R. W. Butler|title=What is Formal Methods?|url=http://shemesh.larc.nasa.gov/fm/fm-what.html|date=2001-08-06|accessdate=2006-11-16}}</ref> The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.<ref>{{cite journal|author=C. Michael Holloway|title=Why Engineers Should Consider Formal Methods|url=http://klabs.org/richcontent/verification/holloway/nasa-97-16dasc-cmh.pdf| publisher=16th Digital Avionics Systems Conference (27–30 October 1997)|accessdate=2006-11-16}}</ref>\n\nFormal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular [[logic in computer science|logic]] calculi, [[formal language]]s, [[automata theory]], and [[program semantics]], but also [[type systems]] and [[algebraic data types]] to problems in software and hardware specification and verification.<ref>Monin, pp.3-4</ref>\n\n===Automata theory===\n{{main|Automata theory}}\n[[Automata theory]] is the study of ''[[abstract machine]]s'' and ''[[automaton|automata]]'', as well as the computational problems that can be solved using them. It is a theory in theoretical computer science, under [[Discrete mathematics]] (a section of [[Mathematics]] and also of [[Computer Science]]). ''Automata'' comes from the Greek word αὐτόματα meaning \"self-acting\".\n\nAutomata Theory is the study of self-operating virtual machines to help in logical understanding of input and output process, without or with intermediate stage(s) of [[computation]] (or any [[Function (engineering)|function]] / process).\n\n===Coding theory===\n{{main|Coding theory}}\n[[Coding theory]] is the study of the properties of codes and their fitness for a specific application. Codes are used for [[data compression]], [[cryptography]], [[error-correction]] and more recently also for [[network coding]]. Codes are studied by various scientific disciplines—such as [[information theory]], [[electrical engineering]],  [[mathematics]], and [[computer science]]—for the purpose of designing efficient and reliable [[data transmission]] methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.\n\n===Computational learning theory===\n{{main|Computational learning theory}}\nTheoretical results in machine learning mainly deal with a type of\ninductive learning called supervised learning.  In supervised\nlearning, an algorithm is given samples that are labeled in some\nuseful way.  For example, the samples might be descriptions of\nmushrooms, and the labels could be whether or not the mushrooms are\nedible.  The algorithm takes these previously labeled samples and\nuses them to induce a classifier.  This classifier is a function that\nassigns labels to samples including the samples that have never been\npreviously seen by the algorithm.  The goal of the supervised learning\nalgorithm is to optimize some measure of performance such as\nminimizing the number of mistakes made on new samples.\n\n== Organizations ==\n* [[European Association for Theoretical Computer Science]]\n* [[SIGACT]]\n* [[Simons Institute for the Theory of Computing]]\n\n== Journals and newsletters ==\n* ''[[Information and Computation]]''\n* ''[[Theory of Computing (journal)|Theory of Computing]]'' ([[Open access (publishing)|open access]] journal)\n* ''[[Formal Aspects of Computing]]''\n* ''[[Journal of the ACM]]''\n* ''[[SIAM Journal on Computing]]'' (SICOMP)\n* ''[[SIGACT News]]''\n* ''[[Theoretical Computer Science (journal)|Theoretical Computer Science]]''\n* ''[[Theory of Computing Systems]]''\n* ''[[International Journal of Foundations of Computer Science]]''\n* ''[[Chicago Journal of Theoretical Computer Science]]'' ([[Open access (publishing)|open access]] journal)\n* ''[[Foundations and Trends in Theoretical Computer Science]]''\n* ''[[Journal of Automata, Languages and Combinatorics]]''\n* ''[[Acta Informatica]]''\n* ''[[Fundamenta Informaticae]]''\n* ''[[ACM Transactions on Computation Theory]]''\n* ''[[Computational Complexity (journal)|Computational Complexity]]''\n* ''[[Journal of Complexity (journal)|Journal of Complexity]]''\n* ACM Transactions on Algorithms\n* Information Processing Letters\n\n== Conferences ==\n* Annual ACM [[Symposium on Theory of Computing]] (STOC)<ref name=\"core-a-plus\">The [http://www.core.edu.au/rankings/Conference%20Ranking%20Main.html 2007 Australian Ranking of ICT Conferences]: tier A+.</ref>\n* Annual IEEE [[Symposium on Foundations of Computer Science]] (FOCS)<ref name=\"core-a-plus\"/>\n* [[Innovations in Theoretical Computer Science]] (ITCS)\n* [[Mathematical Foundations of Computer Science]] (MFCS)<ref>[http://mfcs2017.cs.aau.dk/ MFCS 2017]</ref>\n* [[International Computer Science Symposium in Russia]] (CSR)<ref>[https://logic.pdmi.ras.ru/csr2018/ CSR 2018]</ref>\n* ACM–SIAM [[Symposium on Discrete Algorithms]] (SODA)<ref name=\"core-a-plus\"/>\n* IEEE [[Symposium on Logic in Computer Science]] (LICS)<ref name=\"core-a-plus\"/>\n* [[Computational Complexity Conference]] (CCC)<ref name=\"core-a\"/>\n* [[International Colloquium on Automata, Languages and Programming]] (ICALP)<ref name=\"core-a\"/>\n* Annual [[Symposium on Computational Geometry]] (SoCG)<ref name=\"core-a\">The [http://www.core.edu.au/rankings/Conference%20Ranking%20Main.html 2007 Australian Ranking of ICT Conferences]: tier A.</ref>\n* ACM [[Symposium on Principles of Distributed Computing]] (PODC)<ref name=\"core-a-plus\"/>\n* ACM [[Symposium on Parallelism in Algorithms and Architectures]] (SPAA)<ref name=\"core-a\"/>\n* [[Annual Conference on Learning Theory]] (COLT)<ref name=\"core-a\"/>\n* [[Symposium on Theoretical Aspects of Computer Science]] (STACS)<ref name=\"core-a\"/>\n* [[European Symposium on Algorithms]] (ESA)<ref name=\"core-a\"/>\n* [[Workshop on Approximation Algorithms for Combinatorial Optimization Problems]] (APPROX)<ref name=\"core-a\"/>\n* [[Workshop on Randomization and Computation]] (RANDOM)<ref name=\"core-a\"/>\n* [[International Symposium on Algorithms and Computation]] (ISAAC)<ref name=\"core-a\"/>\n* [[International Symposium on Fundamentals of Computation Theory]] (FCT)<ref>[http://fct11.ifi.uio.no/ FCT 2011] (retrieved 2013-06-03)</ref>\n* [[International Workshop on Graph-Theoretic Concepts in Computer Science]] (WG)\n\n==See also==\n* [[Formal science]]\n* [[Unsolved problems in computer science]]\n* [[List of important publications in theoretical computer science]]\n\n== Notes ==\n<references/>\n\n== Further reading ==\n\n* [[Martin Davis]], Ron Sigal, Elaine J. Weyuker, ''Computability, complexity, and languages: fundamentals of theoretical computer science'', 2nd ed., Academic Press, 1994, {{isbn|0-12-206382-1}}. Covers [[theory of computation]], but also [[program semantics]] and [[quantification theory]]. Aimed at graduate students.\n\n== External links ==\n* [http://www.sigact.org/webpages.php SIGACT directory of additional theory links]\n* [http://theorymatters.org/ Theory Matters Wiki] Theoretical Computer Science (TCS) Advocacy Wiki\n* [news://comp.theory Usenet comp.theory]\n* [http://www.confsearch.org/confsearch/faces/pages/topic.jsp?topic=Theory&sortMode=1&graphicView=1 List of academic conferences in the area of theoretical computer science] at [http://www.confsearch.org confsearch]\n* [http://cstheory.stackexchange.com/ Theoretical Computer Science - StackExchange], a Question and Answer site for researchers in theoretical computer science\n* [http://www.csanimated.com/browse.php Computer Science Animated]\n* http://theory.csail.mit.edu/   @ [[Massachusetts Institute of Technology]]\n\n{{Computer science}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Theoretical Computer Science}}\n[[Category:Theoretical computer science|*]]\n[[Category:Formal sciences]]",
            "slug": "",
            "date_updated": 1517191294672,
            "imported": "https://en.wikipedia.org/wiki/Theoretical_computer_science"
        },
        {
            "title": "Freenet",
            "text": "{{distinguish|Free-Net}}\n{{Other uses}}\n{{use dmy dates|date=January 2012}}\n{{Multiple issues|{{refimprove|date=July 2013}}{{More footnotes|article|date=August 2013}}}}\n{{Infobox software\n| title                  = \n| name                   = Freenet\n| logo                   = Freenet logo.svg\n| logo caption           = \n| screenshot             = Freenetscreenshot.png\n| caption                = FProxy index page (Freenet 0.7)\n| collapsible            = \n| author                 = \n| developer              = The Freenet Project<ref name=\"People\">{{cite web| title = People| url = https://freenetproject.org/people.html| date = 22 September 2008| publisher = ''Freenet: The Free Network official website''| accessdate = 31 May 2014| deadurl = yes| archiveurl = https://web.archive.org/web/20130921053414/https://freenetproject.org/people.html| archivedate = 21 September 2013| df = dmy-all}}</ref>\n| released               = {{start date and age|2000|3}} <!-- {{Start date|2000|03|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = {{Latest stable software release/Freenet}}\n| latest preview version = 0.7.5 (Build 1475-pre4)\n| latest preview date    = {{Start date and age|2016|06|23|df=yes/no}}<ref>https://github.com/freenet/fred/releases/tag/testing-build-1475-pre4</ref>\n| status                 = Active\n| programming language   = [[Java (programming language)|Java]]\n| operating system       = [[Cross-platform]]\n| platform               = [[Java (Sun)|Java]]\n| size                   = \n| language               = English, French, Italian, German, Dutch, Spanish, Portuguese, Swedish, Norwegian, Chinese<ref>[https://github.com/freenet/fred/tree/master/src/freenet/l10n Language specific versions of Freenet], ''GitHub: Freenet''.</ref>\n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = [[Anonymity]], [[Peer-to-peer]], [[Friend-to-friend]], [[overlay network]]\n| license                = [[GNU General Public License]]\n| alexa                  = \n| website                = {{URL|https://freenetproject.org}}\n| standard               = \n| AsOf                   = \n| logo_alt               = Logo of Freenet\n| screenshot_alt         = Screenshot of Freenet 0.7\n}}\n{{File sharing sidebar}}\n'''Freenet''' is a [[peer-to-peer]] platform for [[censorship]]-resistant communication. It uses a decentralized [[distributed data store]] to keep and deliver information, and has a suite of [[free software]] for publishing and communicating on the Web without fear of censorship.<ref name=\"What is Freenet?\">[https://freenetproject.org/whatis.html What is Freenet?], ''Freenet: The Free network official website''.</ref><ref name=\"Peers in a Client/Server World, 2005\">Taylor, Ian J. ''From P2P to Web Services and Grids: Peers in a Client/Server World''. London: Springer, 2005.</ref>{{rp|151}} Both Freenet and some of its associated tools were originally designed by [[Ian Clarke (computer scientist)|Ian Clarke]], who defined Freenet's goal as providing [[freedom of speech]] on the Internet with strong anonymity protection.<ref name=\"Time Magazine\">{{cite news|url=http://www.time.com/time/magazine/article/0,9171,997286,00.html|title=The Infoanarchist|last=Cohen|first=Adam|date=26 June 2000|work=[[Time (magazine)|''TIME'']] Magazine|accessdate=18 December 2011}}</ref><ref name=\"The Guardian\">{{cite news|url=https://www.theguardian.com/technology/2009/nov/26/dark-side-internet-freenet |title=The dark side of the internet |last=Beckett |first=Andy |date=26 November 2009 |work=[[The Guardian]] |accessdate=26 November 2009 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20130908073158/http://www.theguardian.com/technology/2009/nov/26/dark-side-internet-freenet |archivedate=8 September 2013 |df=dmy }}  [http://blog.locut.us/2009/11/26/the-guardian-writes-about-freenet/ The Guardian writes about Freenet (Ian Clarke's response)] [https://www.webcitation.org/6PzlbqMVu?url=http://blog.locut.us/2009/11/26/the-guardian-writes-about-freenet/ Archived at WebCite]</ref>\n\nThe distributed data store of Freenet (with same principles as a [[blockchain]]) is used by many third-party programs and plugins to provide [[microblogging]] and media sharing,<ref name=\"sone\">{{cite web |url = http://draketo.de/licht/freie-software/freenet/sone-pseudonymes-microblogging|title = Sone: Pseudonymes Microblogging über Freenet}}, german article, 2010</ref> anonymous and decentralised version tracking,<ref name=\"infocalypse\">{{cite web | url=http://mercurial.selenic.com/wiki/Infocalypse | title=Infoclypse: A Mercurial plugin for decentral, anonymous version tracking and code-sharing over freenet | deadurl=yes | archiveurl=https://web.archive.org/web/20111120194346/http://mercurial.selenic.com/wiki/Infocalypse | archivedate=20 November 2011 | df=dmy-all }}</ref> blogging,<ref name=\"floghelper\">{{cite web | url=https://github.com/freenet/plugin-FlogHelper-staging|title=Flog Helper: Easy Blogging over Freenet}}</ref> a generic [[web of trust]] for decentralized [[Anti-spam techniques|spam resistance]],<ref name=\"weboftrust\">{{cite web|url=http://wiki.freenetproject.org/WoT|title=Web Of Trust: A freenet plugin for pseudonymous, decentral spam resistance}}</ref> Shoeshop for using Freenet over [[Sneakernet]],<ref name=\"shoeshop\">{{cite web| title=Freenet over Sneakernet. Freenet Key: USK@MYLAnId-ZEyXhDGGbYOa1gOtkZZrFNTXjFl1dibLj9E,Xpu27DoAKKc8b0718E-ZteFrGqCYROe7XBBJI57pB4M,AQACAAE/Shoeshop/2/ }}</ref> and many more.\n\n==History==\nThe origin of Freenet can be traced to Ian Clarke's student project at the [[University of Edinburgh]], which he completed as a graduation requirement in the summer of 1999.<ref>{{cite news| url=https://www.nytimes.com/2000/05/10/business/cyberspace-programmers-confront-copyright-laws.html | title = Cyberspace Programmers Confront Copyright Laws| author = John Markoff| work=The New York Times | date = 10 May 2000}}</ref><ref>{{cite news| url=http://news.bbc.co.uk/2/hi/science/nature/1216486.stm | work=BBC News | title=Coders prepare son of Napster | date=12 March 2001}}</ref><ref>{{cite news| url=http://www.cnn.com/2005/TECH/12/19/internet.freedom/index.html?iref=allsearch | work=CNN | title= Fighting for free speech on the Net | date = 19 December 2005}}</ref> Ian Clarke's resulting unpublished report \"A distributed decentralized information storage and retrieval system\" (1999) provided foundation for the seminal paper written in collaboration with other researchers, \"Freenet: A Distributed Anonymous Information Storage and Retrieval System\" (2001).<ref>Ian Clarke. [https://freenetproject.org/papers/ddisrs.pdf A distributed decentralised information storage and retrieval system]. Unpublished report, Division of Informatics, University of Edinburgh, 1999.</ref><ref name=\"Freenet: A Distributed Anonymous Information Storage and Retrieval System\">Ian Clarke, Oskar Sandberg, Brandon Wiley, and Theodore W. Hong. [https://www.cs.cornell.edu/People/egs/615/freenet.pdf Freenet: A Distributed Anonymous Information Storage and Retrieval System]. In: Proceedings of the International Workshop on Designing Privacy Enhancing Technologies: Design Issues in Anonymity and Unobservability. New York, NY: Springer-Verlag, 2001, p. 46-66.</ref> According to [[CiteSeer]], it became one of the most frequently cited [[computer science]] articles in 2002.<ref>[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.4919 CiteSeer: Freenet: A Distributed Anonymous Information Storage and Retrieval System (2001) ]</ref>\n\nResearchers suggested that Freenet can provide anonymity on the Internet by storing small encrypted snippets of content distributed on the computers of its users and connecting only through intermediate computers which pass on requests for content and sending them back without knowing the contents of the full file, similar to how [[Router (computing)|routers]] on the Internet route [[Network packet|packets]] without knowing anything about files—except Freenet has caching, a layer of strong encryption, and no reliance on [[Centralized computing|centralized structures]].<ref name=\"Freenet: A Distributed Anonymous Information Storage and Retrieval System\" /> This allows users to publish anonymously or retrieve various kinds of information.<ref name=\"Peers in a Client/Server World, 2005\" />{{rp|152}}\n\n[[File:freenet darknet.png|thumb|The Freenet 0.7 darknet peers list.]]\nFreenet has been under continuous development since 2000.\n\nFreenet 0.7, released on 8 May 2008, is a major re-write incorporating a number of fundamental changes. The most fundamental change is support for [[darknet]] operation. Version 0.7 offered two modes of operation: a  mode in which it connects only to friends, and an opennet-mode in which it connects to any other Freenet user. Both modes can be run simultaneously. When a user switches to pure darknet operation, Freenet becomes very difficult to detect from the outside. The [[transport layer]] created for the darknet mode allows communication over restricted routes as commonly found in [[mesh networking|mesh networks]], as long as these connections follow a [[Small-world network|small-world]] structure.<ref>Singh, Munindar P. The Practical Handbook of Internet Computing. Boca Raton, Fl.: Chapman & Hall, 2005.</ref>{{rp|815–816}} Other modifications include switching from [[Transmission Control Protocol|TCP]] to [[User Datagram Protocol|UDP]], which allows [[UDP hole punching]] along with faster transmission of messages between peers in the network.<ref>{{cite news|last1=Ihlenfeld|first1=Jens|title=Freenet 0.7 soll globales Darknet schaffen|url=http://www.golem.de/0604/44448.html|accessdate=17 September 2015|publisher=Golem|date=2006-04-04}}</ref>\n\nFreenet 0.7.5, released on 12 June 2009, offers a variety of improvements over 0.7.  These include reduced memory usage, faster insert and retrieval of content, significant improvements to the FProxy web interface used for browsing freesites, and a large number of smaller bugfixes, performance enhancements, and usability improvements.  Version 0.7.5 also shipped with a new version of the Windows installer.<ref>[https://freenetproject.org/news.html#freenet-0-7-5-released release information for Freenet 0.7.5], last accessed 2015-09-17</ref>\n\nAs of build 1226, released on 30 July 2009, features that have been written include significant security improvements against both attackers acting on the network and physical seizure of the computer running the node.<ref>[https://freenetproject.org/news.html#build1226 release information for Freenet build 1226], last accessed 2015-09-17</ref>\n\nAs of build 1468, released on 11 July 2015, the Freenet core stopped using the [[db4o]] database and laid the foundation for an efficient interface to the Web of Trust plugin which provides spam resistance.<ref>[https://freenetproject.org/news.html#20150711-1468-release Freenet 1468 release notes] 2015</ref>\n\nFreenet has always been free software, but until 2011 it required users to install [[Java (software platform)|Java]]. This problem was solved by making Freenet compatible with [[OpenJDK]], a free and open source implementation of the Java Platform.\n\nOn 11 February 2015, Freenet received the SUMA-Award for \"protection against total surveillance.\"<ref name=suma_award>[http://suma-awards.de/en/index.html SUMA Award], 11 February 2015.</ref><ref name=suma_award_recording>[https://www.youtube.com/watch?v=dZpsBSPsHDI recording of the SUMA Award Ceremony 2015], published on 14 April 2015.</ref><ref name=suma_award_heise>[http://www.heise.de/newsticker/meldung/SUMA-Award-fuer-das-Freenet-Project-2548577.html SUMA Award für das Freenet Projekt] Jo Bager in Heise online, 2015</ref>\n\n==Features and user interface of Freenet==\nFreenet is different from most other peer-to-peer applications, both in how users interact with it and in the security it offers. It separates the underlying network structure and protocol from how users interact with the network; as a result, there are a variety of ways to access content on the Freenet network. The simplest is via FProxy, which is integrated with the node software and provides a web interface to content on the network. Using FProxy, a user can browse freesites (websites that use normal [[HTML]] and related tools, but whose content is stored within Freenet rather than on a traditional web server). The web interface is also used for most configuration and node management tasks. Through the use of separate applications or plugins loaded into the node software, users can interact with the network in other ways, such as forums similar to web forums or Usenet or interfaces more similar to traditional P2P \"filesharing\" interfaces.\n\nWhile Freenet provides an [[HTTP]] interface for browsing freesites, it is not a [[Proxy server|proxy]] for the [[World Wide Web]]; Freenet can be used to access only the content that has been previously inserted into the Freenet network. In this way, it is more similar to [[Tor (anonymity network)#Hidden services|Tor's hidden services]] than to anonymous proxy software like [[Tor (anonymity network)|Tor's proxy]].\n\nFreenet's focus lies on [[Freedom of speech|free speech]] and anonymity. Because of that, Freenet acts differently at certain points that are (directly or indirectly) related to the anonymity part. Freenet attempts to protect the anonymity of both people inserting data into the network (uploading) and those retrieving data from the network (downloading). Unlike file sharing systems, there is no need for the uploader to remain on the network after uploading a file or group of files. Instead, during the upload process, the files are broken into chunks and stored on a variety of other computers on the network. When downloading, those chunks are found and reassembled. Every node on the Freenet network contributes storage space to hold files and bandwidth that it uses to route requests from its peers.\n\nAs a direct result of the anonymity requirements, the node requesting content does not normally connect directly to the node that has it; instead, the request is routed across several intermediaries, none of which know which node made the request or which one had it. As a result, the total bandwidth required by the network to transfer a file is higher than in other systems, which can result in slower transfers, especially for unpopular content.\n\nSince version 0.7, Freenet offers two different levels of security: Opennet and Darknet. With Opennet, users connect to arbitrary other users. With Darknet, users connect only to \"friends\" with whom they previously exchanged [[Public-key cryptography|public keys]], named node-references. Both modes can be used together.\n\n==Content==\nFreenet's founders argue that true freedom of speech comes only with true anonymity and that the beneficial uses of Freenet outweigh its negative uses.<ref name=\"philosophy\">[https://freenetproject.org/philosophy.html The Philosophy behind Freenet]</ref> Their view is that free speech, in itself, is not in contradiction with any other consideration—the information is not the crime. Freenet attempts to remove the possibility of any group imposing its beliefs or values on any data. Although many states censor communications to different extents, they all share one commonality in that a body must decide what information to censor and what information to allow. What may be acceptable to one group of people may be considered offensive or even dangerous to another. In essence, the purpose of Freenet is to ensure that no one is allowed to decide what is acceptable.\n\nReports of Freenet's use in authoritarian nations is difficult to track due to the very nature of Freenet's goals. One group, ''Freenet China'', used to introduce the Freenet software to [[China|Chinese]] users starting from 2001 and distribute it within China through e-mails and on disks after the group's website was blocked by the Chinese authorities on the mainland. It was reported that in 2002 ''Freenet China'' had several thousand dedicated users.<ref>Damm, Jens, and Simona Thomas. ''Chinese Cyberspaces Technological Changes and Political Effects''. London: Routledge, 2006.</ref>{{rp|70–71}} However, Freenet traffic is blocked in China around the 2010s.{{Citation needed|reason=This sounds plausible but would benefit from a source|date=November 2017}}\n\n==Technical design==\n{{See also|Cryptography}}\n\nThe Freenet [[file sharing]] network stores documents and allows them to be retrieved later by an associated key, as is now possible with protocols such as [[HyperText Transfer Protocol|HTTP]]. The network is designed to be highly survivable. The system has no central servers and is not subject to the control of any one individual or organization, including the designers of Freenet. Information stored on Freenet is distributed around the network and stored on several different nodes. Encryption of data and relaying of requests makes it difficult to determine who inserted content into Freenet, who requested that content, or where the content was stored.  This protects the anonymity of participants, and also makes it very difficult to censor specific content.  Content is stored encrypted, making it difficult for even the operator of a node to determine what is stored on that node.  This provides [[plausible deniability]], and in combination with the request relaying means that [[Safe harbor (law)|safe harbor]] laws that protect service providers may also protect Freenet node operators. When asked about the topic, Freenet developers defer to the EFF discussion which says that not being able to filter anything is a safe choice.<ref>{{cite web|url=https://emu.freenetproject.org/pipermail/chat/2009-February/001872.html|last=Toseland|first=Matthew |title=Does Freenet qualify for DMCA Safe Harbor? |accessdate=27 January 2013}}</ref><ref>{{cite web|url=https://www.eff.org/wp/iaal-what-peer-peer-developers-need-know-about-copyright-law|title=\nIAAL*: What Peer-to-Peer Developers Need to Know about Copyright Law\n |accessdate=15 September 2015}}</ref>\n\n===Distributed storage and caching of data===\nUnlike other [[P2P network]]s, Freenet not only transmits data between nodes but actually stores them, working as a huge distributed cache. To achieve this, each node allocates some amount of disk space to store data; this is configurable by the node operator, but is typically several GB (or more).\n\nFiles on Freenet are typically split into multiple small blocks, with duplicate blocks created to provide [[Forward error correction|redundancy]].  Each block is handled independently, meaning that a single file may have parts stored on many different nodes.\n\nInformation flow in Freenet is different from networks like [[eMule]] or [[BitTorrent (protocol)|BitTorrent]]; in Freenet:\n\n# A user wishing to share a file or update a freesite \"inserts\" the file \"to the network\"\n# After \"insertion\" is finished, the publishing node is free to shut down, because the file is stored in the network. It will remain available for other users whether or not the original publishing node is online. No single node is responsible for the content; instead, it is replicated to many different nodes.\n\nTwo advantages of this design are high reliability and anonymity. Information remains available even if the publisher node goes offline, and is anonymously spread over many hosting nodes as encrypted blocks, not entire files.\n\nThe key disadvantage of the storage method is that no one node is responsible for any chunk of data. If a piece of data is not retrieved for some time and a node keeps getting new data, it will drop the old data sometime when its allocated disk space is fully used. In this way Freenet tends to 'forget' data which is not retrieved regularly (see also [[Freenet#Effect|Effect]]).\n\nWhile users can insert data into the network, there is no way to delete data. Due to Freenet's anonymous nature the original publishing node or owner of any piece of data is unknown. The only way data can be removed is if users don't request it.\n\n===Network===\nTypically, a host computer on the network runs the software that acts as a node, and it connects to other hosts running that same software to form a large distributed, variable-size network of peer nodes. Some nodes are end user nodes, from which documents are requested and presented to human users. Other nodes serve only to route data. All nodes communicate with each other identically – there are no dedicated \"clients\" or \"servers\". It is not possible for a node to rate another node except by its capacity to insert and fetch data associated with a key. This is unlike most other P2P networks where node administrators can employ a ratio system, where users have to share a certain amount of content before they can download.\n\nFreenet may also be considered a [[small world network]].\n\nThe Freenet protocol is intended to be used on a network of complex topology, such as the Internet ([[Internet Protocol]]). Each node knows only about some number of other nodes that it can reach directly (its conceptual \"neighbors\"), but any node can be a neighbor to any other; no hierarchy or other structure is intended. Each message is routed through the network by passing from neighbor to neighbor until it reaches its destination. As each node passes a message to a neighbor, it does not know whether the neighbor will forward the message to another node, or is the final destination or original source of the message. This is intended to protect the anonymity of users and publishers.\n\nEach node maintains a data store containing documents associated with keys, and a routing table associating nodes with records of their performance in retrieving different keys.\n\n===Protocol===\n[[File:Freenet Request Sequence ZP.svg|thumb|A typical request sequence. The request moves through the network from node to node, backing out of a dead-end (step 3) and a loop (step 7) before locating the desired file.]]\n\nThe Freenet protocol uses a [[key-based routing]] protocol, similar to [[distributed hash table]]s. The routing algorithm changed significantly in version 0.7. Prior to version 0.7, Freenet used a [[heuristic routing]] algorithm where each node had no fixed location, and routing was based on which node had served a key closest to the key being fetched (in version 0.3) or which is estimated to serve it faster (in version 0.5).  In either case, new connections were sometimes added to downstream nodes (i.e. the node that answered the request) when requests succeeded, and old nodes were discarded in least recently used order (or something close to it). Oskar Sandberg's research (during the development of version 0.7) shows that this \"path folding\" is critical, and that a very simple routing algorithm will suffice provided there is path folding.\n\nThe disadvantage of this is that it is very easy for an attacker to find Freenet nodes, and connect to them, because every node is continually attempting to find new connections. In version 0.7, Freenet supports both 'Opennet' (similar to the old algorithms, but simpler), and \"Darknet\" (all node connections are set up manually, so only your friends know your node's IP address). Darknet is less convenient, but much more secure against a distant attacker.\n\nThis change required major changes in the routing algorithm. Every node has a location, which is a number between 0 and 1. When a key is requested, first the node checks the local data store. If it's not found, the key's hash is turned into another number in the same range, and the request is routed to the node whose location is closest to the key. This goes on until some number of hops is exceeded, there are no more nodes to search, or the data is found. If the data is found, it is cached on each node along the path. So there is no one source node for a key, and attempting to find where it is currently stored will result in it being cached more widely. Essentially the same process is used to insert a document into the network: the data is routed according to the key until it runs out of hops, and if no existing document is found with the same key, it is stored on each node. If older data is found, the older data is propagated and returned to the originator, and the insert \"collides\".\n\nBut this works only if the locations are clustered in the right way. Freenet assumes that the Darknet (a subset of the global social network) is a small-world network, and nodes constantly attempt to swap locations (using the [[Metropolis–Hastings algorithm]]) in order to minimize their distance to their neighbors. If the network actually is a small-world network, Freenet should find data reasonably quickly; ideally on the order of [[Big O notation|<math>O\\left(\\left[log\\left(n\\right)\\right]^2\\right)</math>]] hops. However, it does not guarantee that data will be found at all.<ref name=Clarke2010>{{cite book|last1=Clarke|first1=Ian|title=Private Communication Through a Network of Trusted Connections: The Dark Freenet|date=2010|url=https://freenetproject.org/papers/freenet-0.7.5-paper.pdf|accessdate=2015-09-15|ref=Clarke2010}}</ref>\n\nEventually, either the document is found or the hop limit is exceeded. The terminal node sends a reply that makes its way back to the originator along the route specified by the intermediate nodes' records of pending requests. The intermediate nodes may choose to cache the document along the way. Besides saving bandwidth, this also makes documents harder to censor as there is no one \"source node.\"\n\n===Effect===\n[[File:Freenet datastore specialisation.ani.gif|thumb|450px|right|The effect of the node specialising on the particular location.]]\nInitially, the locations in Darknet are distributed randomly. This means that routing of requests is essentially random. In Opennet connections are established by a join request which provides an optimized network structure if the existing network is already optimized.<ref name=Roos2014>{{cite book|last1=Roos|first1=Stefanie|title=Measuring Freenet in the Wild: Censorship-Resilience under Observation|date=2014|publisher=Springer International Publishing|isbn=978-3-319-08505-0|pages=263–282|url=https://freenetproject.org/papers/roos-pets2014.pdf|accessdate=2015-09-15|ref=Roos2014}}</ref> So the data in a newly started Freenet will be distributed somewhat randomly.{{citation needed|date=July 2013}}\n\nAs location swapping (on Darknet) and path folding (on Opennet) progress, nodes which are close to one another will increasingly have close locations, and nodes which are far away will have distant locations. Data with similar keys will be stored on the same node.<ref name=Roos2014 />\n\nThe result is that the network will self-organize into a distributed, clustered structure where nodes tend to hold data items that are close together in key space. There will probably be multiple such clusters throughout the network, any given document being replicated numerous times, depending on how much it is used. This is a kind of \"[[spontaneous symmetry breaking]]\", in which an initially symmetric state (all nodes being the same, with random initial keys for each other) leads to a highly asymmetric situation, with nodes coming to specialize in data that has closely related keys.{{citation needed|date=July 2013}}\n\nThere are forces which tend to cause clustering (shared closeness data spreads throughout the network), and forces that tend to break up clusters (local caching of commonly used data). These forces will be different depending on how often data is used, so that seldom-used data will tend to be on just a few nodes which specialize in providing that data, and frequently used items will be spread widely throughout the network. This automatic mirroring counteracts the times when [[web traffic]] becomes overloaded, and due to a mature network's intelligent routing, a network of size ''n'' should require only log(''n'') time to retrieve a document on average.{{citation needed|date=July 2013}}\n\n===Keys===\nKeys are [[hash function|hashes]]: there is no notion of [[semantic closeness]] when speaking of key closeness. Therefore, there will be no correlation between key closeness and similar popularity of data as there might be if keys did exhibit some semantic meaning, thus avoiding bottlenecks caused by popular subjects.\n\nThere are two main varieties of keys in use on Freenet, the Content Hash Key (CHK) and the Signed Subspace Key (SSK). A subtype of SSKs is the Updatable Subspace Key (USK) which adds versioning to allow secure updating of content.\n\nA CHK is a [[SHA-256]] hash of a document (after encryption, which itself depends on the hash of the plaintext) and thus a node can check that the document returned is correct by hashing it and checking the digest against the key. This key contains the meat of the data on Freenet. It carries all the binary data building blocks for the content to be delivered to the client for reassembly and decryption. The CHK is unique by nature and provides tamperproof content. A hostile node altering the data under a CHK will immediately be detected by the next node or the client. CHKs also reduce the redundancy of data since the same data will have the same CHK and when multiple sites reference the same large files, they can reference to the same CHK.<ref>{{cite web|title=freesitemgr, code for inserting files as CHK, fixed revision|url=https://github.com/freenet/lib-pyFreenet/blob/b78aea05222c4afe5145d8b529d2a54d4b93887a/fcp/sitemgr.py#L976|accessdate=2017-11-29}}</ref>\n\nSSKs are based on public-key cryptography. Currently Freenet uses the [[Digital Signature Algorithm|DSA]] algorithm. Documents inserted under SSKs are signed by the inserter, and this signature can be verified by every node to ensure that the data is not tampered with. SSKs can be used to establish a verifiable [[pseudonymity|pseudonymous]] identity on Freenet, and allow for multiple documents to be inserted securely by a single person. Files inserted with an SSK are effectively [[Immutable object|immutable]], since inserting a second file with the same name can cause collisions. USKs resolve this by adding a version number to the keys which is also used for providing update notification for keys registered as bookmarks in the web interface.<ref>{{cite web|last1=Babenhauserheide|first1=Arne|title=USK and Date-Hints: Finding the newest version of a site in Freenet's immutable datastore|url=http://draketo.de/light/english/freenet/usk-and-date-hints|website=http://draketo.de|accessdate=2017-11-29}}</ref> Another subtype of the SSK is the Keyword Signed Key, or KSK, in which the key pair is generated in a standard way from a simple human-readable string. Inserting a document using a KSK allows the document to be retrieved and decrypted if and only if the requester knows the human-readable string; this allows for more convenient (but less secure) [[Uniform Resource Identifier|URIs]] for users to refer to.<ref>{{cite web|last1=Babenhauserheide|first1=Arne|title=Effortless password protected sharing of files via Freenet|url=http://draketo.de/light/english/freenet/effortless-password-protected-sharing-files|website=http://draketo.de|accessdate=2017-11-29}}</ref>\n\n==Scalability==\nA [[Computer networking|network]] is said to be scalable if its performance does not deteriorate even if the network is very large. The scalability of Freenet is being evaluated, but similar architectures have been shown to scale logarithmically.<ref>{{cite book |doi=10.1145/335305.335325 |chapter=The Small-World Phenomenon: An Algorithmic Perspective |title=Proceedings of the thirty-second annual ACM symposium on Theory of computing |year=2000 |last1=Kleinberg |first1=Jon |isbn=978-1-58113-184-0 |pages=163–70 |chapterurl=http://www.cs.cornell.edu/home/kleinber/swn.pdf}}</ref> This work indicates that Freenet can find data in <math>O(\\log^2 n)</math> hops on a small-world network (which includes both opennet and darknet style Freenet networks), when ignoring the caching which could improve the scalability for popular content. However, this scalability is difficult to test without a very large network. Furthermore, the security features inherent to Freenet make detailed performance analysis (including things as simple as determining the size of the network) difficult to do accurately. As of now, the scalability of Freenet has yet to be tested.\n\n==Darknet versus Opennet==\nAs of version 0.7, Freenet supports both \"darknet\" and \"opennet\" connections.  Opennet connections are made automatically by nodes with opennet enabled, while darknet connections are manually established between users that know and trust each other.  Freenet developers describe the trust needed as “will not crack their Freenet node”.<ref>{{cite web|url=https://d6.gnutella2.info/freenet/USK@sUm3oJISSEU4pl2Is9qa1eRoCLyz6r2LPkEqlXc3~oc,yBEbf-IJrcB8Pe~gAd53DEEHgbugUkFSHtzzLqnYlbs,AQACAAE/random_babcom/135/#Requiredtrustforformingadarknetconnection|title=Required trust for forming a darknet connection|last=|first=|date=2017-11-29|website=random_babcom|accessdate=2017-11-29}}</ref>  Opennet connections are easy to use, but darknet connections are more secure against attackers on the network, and can make it difficult for an attacker (such as an oppressive government) to even determine that a user is running Freenet in the first place.<ref>{{cite news|url=http://www.golem.de/0805/59592.html|title=Darknet-Fähigkeiten sollen Softwarenutzung verbergen|date=2008-05-09|work=|accessdate=2017-11-29|publisher=Golem}}</ref>\n\nThe core innovation in Freenet 0.7 is to allow a globally scalable darknet, capable (at least in theory) of supporting millions of users.  Previous darknets, such as [[WASTE]], have been limited to relatively small disconnected networks.  The scalability of Freenet is made possible by the fact that human relationships tend to form small-world networks, a property that can be exploited to find short paths between any two people. The work is based on a speech given at [[DEF CON (convention)|DEF CON 13]] by [[Ian Clarke (computer scientist)|Ian Clarke]] and Swedish mathematician [[Oskar Sandberg]].  Furthermore, the routing algorithm is capable of routing over a mixture of opennet and darknet connections, allowing people who have only a few friends using the network to get the performance from having sufficient connections while still receiving some of the security benefits of darknet connections.  This also means that small darknets where some users also have opennet connections are fully integrated into the whole Freenet network, allowing all users access to all content, whether they run opennet, darknet, or a hybrid of the two, except for darknet pockets connected only by a single hybrid node.<ref name=Roos2014 />\n\n== Tools and applications ==\n[[File:frost screenshot.png|thumb|Screenshot of Frost running on [[Microsoft Windows]]]]\n\nUnlike many other P2P applications Freenet does not provide comprehensive functionality itself. Freenet is modular and features an [[Application programming interface|API]] called Freenet Client Protocol (FCP) for other programs to use to implement services such as [[message boards]], file sharing, or [[online chat]].<ref>[http://freesocial.draketo.de/ Freenet Social Networking guide] Justus Ranvier, 2013</ref>\n\n===Communication===\n; Freenet Messaging System (FMS): FMS was designed to address problems with Frost such as [[Denial-of-Service attack|denial of service]] attacks and spam. Users publish trust lists, and each user downloads messages only from identities they trust and identities trusted by identities they trust. FMS is developed anonymously and can be downloaded from ''the FMS freesite'' within Freenet. It does not have an official site on the normal Internet. It features random post delay, support for many identities, and a distinction between trusting a user's posts and trusting their trust list. It is written in C++ and is a separate application from Freenet which uses the Freenet Client Protocol (FCP) to interface with Freenet.\n\n; Frost: Frost includes support for convenient file sharing, but its design is inherently vulnerable to spam and [[Denial-of-Service attack|denial of service]] attacks.<ref>[https://www.mail-archive.com/devl@freenetproject.org/msg17363.html Developer discussion about fixing Frost shortcomings] Matthew Toseland, 2007</ref> Frost can be downloaded from the Frost home page on Sourceforge, or from ''the Frost freesite'' within Freenet. It is not endorsed by the Freenet developers. Frost is written in Java and is a separate application from Freenet.\n\n; Sone: Sone provides a simpler interface inspired by Facebook<ref>[https://flattr.com/thing/81996/Sone-The-Freenet-Social-Network-Plugin description of Sone by its developer], \"it’s a Facebook clone on top of Freenet\", retrieved 2015-09-15</ref> with public anonymous discussions and image galleries. It provides an API for control from other programs<ref>[https://wiki.freenetproject.org/Sone Sone in Freenet Wiki], with the description of the FCP API, retrieved 2015-09-14</ref> is also used to implement a comment system for static websites in the regular internet.<ref>[http://draketo.de/proj/freecom/ babcom description], “it submits a search request on your local Sone instance by creating an iframe with the right URL”, 2014.</ref><ref>[https://d6.gnutella2.info/freenet/USK@nwa8lHa271k2QvJ8aa0Ov7IHAV-DFOCFgmDt3X6BpCI,DuQSUZiI~agF8c-6tjsFFGuZ8eICrzWCILB60nT8KKo,AQACAAE/sone/71/]</ref>\n\n===Utilities===\n; jSite: jSite is a tool to upload websites. It handles keys and manages uploading files.\n\n; Infocalypse: Infocalypse is an extension for the distributed revision control system [[Mercurial]]. It uses an optimized structure to minimize the number of requests to retrieve new data, and allows supporting a repository by securely reuploading most parts of the data without requiring the owner's private keys.<ref name=\"infocalypse-info\">{{cite web | url=http://draketo.de/light/english/freenet/infocalypse-mercurial-survive-the-information-apocalypse#advocacy | title=Information about infocalypse. A mirror of the included documentation}}</ref>\n\n===Libraries===\n; FCPLib: FCPLib (Freenet Client Protocol Library) aims to be a [[cross-platform]] natively [[Compiler|compiled]] set of [[C++]]-based functions for storing and retrieving information to and from Freenet. FCPLib supports Windows NT/2K/XP, [[Debian]], [[BSD]], [[Solaris (operating system)|Solaris]], and [[macOS]].\n\n; lib-pyFreenet: lib-pyFreenet exposes Freenet functionality to [[Python (programming language)|Python]] programs. Infocalypse uses it.\n\n==Vulnerabilities==\nLaw enforcement agencies have claimed to have successfully infiltrated freenet opennet in order to deanonymize users<ref>[http://www.thedickinsonpress.com/news/north-dakota/3885239-predators-police-online-struggle Predators, police in online struggle]</ref> but no technical details have been given to support these allegations. One report stated that, \"A child-porn investigation focused on... [the suspect] when the authorities were monitoring the online network, Freenet.\"<ref>{{Cite news|url=https://arstechnica.com/tech-policy/2017/03/man-jailed-indefinitely-for-refusing-to-decrypt-hard-drives-loses-appeal/|title=Man jailed indefinitely for refusing to decrypt hard drives loses appeal|last=|first=|date=March 20, 2017|work=Ars Technica|access-date=2017-03-21|language=en-us}}</ref> A different report indicated arrests may have been based on the BlackICE project leaks, that are debunked for using bad math.<ref>{{Cite web|url=https://freenetproject.org/police-departments-tracking-efforts-based-on-false-statistics.html|title=Police department's tracking efforts based on false statistics|website=freenetproject.org|access-date=2017-09-23}}</ref>\n\nA recent court case in the Peel Region of Ontario, Canada  R. v. Owen, 2017 ONCJ 729 (CanLII), illustrated that Law Enforcement do in fact have a presence, after Peel Regional Police, located who had been downloading illegal material on the Freenet network.<ref>{{Cite web|url=https://www.canlii.org/en/on/oncj/doc/2017/2017oncj729/2017oncj729.html|title=https://www.canlii.org/en/on/oncj/doc/2017/2017oncj729/2017oncj729.html|last=|first=|date=|website=|access-date=}}</ref> The court decision, indicates, Law Enforcement agencies operates several nodes running modified Freenet software that can be used to monitor and track user download activity.   \n\n==Notability==\nFreenet has had significant publicity in the mainstream press, including articles in the ''[[New York Times]]'', and coverage on [[CNN]], [[60 Minutes II]], the [[BBC]], [[The Guardian]],<ref>[https://www.theguardian.com/technology/2009/nov/26/dark-side-internet-freenet The dark side of the internet] Andy Beckett in the Guardian 2009</ref> and elsewhere.\n\nFreenet received the SUMA-Award 2014 for \"protection against total surveillance.\"<ref name=suma_award /><ref name=suma_award_recording /><ref name=suma_award_heise />\n\n==Freesite==\nA \"freesite\" is a site hosted on the Freenet network. Because it contains only static content, it cannot contain any active content like server side scripts or databases. Freesites are coded in HTML and support as many features as the browser viewing the page allows; however, there are some exceptions where the Freenet software will remove parts of the code that may be used to reveal the identity of the person viewing the page (making a page access something on the internet, for example).\n\nDue to the much slower latency and bandwidth of the Freenet network, complex web technologies such as [[PHP]] and [[MySQL]] are impossible to use, making Freesites appear very simplistic, they are described by the community as being \"90s-style\"{{Citation needed|date=October 2016}}.\n\n==See also==\n{{Portal|Free software|Computer Science|Cryptography}}\n* [[Peer-to-peer web hosting]]\n* [[Rendezvous protocol]]\n* [[Anonymous P2P]]\n* [[Crypto-anarchism]]\n* [[Cypherpunk]]\n* [[Distributed file system]]\n* [[Freedom of information]]\n* [[Friend-to-friend]]\n===Comparable software===\n* [[GNUnet]]\n* [[I2P]]\n* [[Osiris (Serverless Portal System)|Osiris]]\n* [[Java Anon Proxy]] (also known as JonDonym)\n* [[Perfect Dark (P2P)|Perfect Dark]] – employs many of Freenet's principles; the successor to [[Share (P2P)|Share]], which itself is the successor of [[Winny]].\n* [[Tahoe-LAFS]]\n* [[Entropy (anonymous data store)]] (Discontinued)\n* [[ZeroNet]]\n\n==References==\n{{reflist|25em}}\n\n==Further reading==\n*{{cite journal |doi=10.1109/4236.978368 |title=Protecting free expression online with Freenet |year=2002 |last1=Clarke |first1=I. |last2=Miller |first2=S.G. |last3=Hong |first3=T.W. |last4=Sandberg |first4=O. |last5=Wiley |first5=B. |journal=IEEE Internet Computing |volume=6 |issue=1 |pages=40–9|url=http://www.doc.ic.ac.uk/~twh1/longitude/papers/ieee-final.pdf }}\n*{{cite journal |doi=10.1016/S0048-7333(03)00050-7 |title=Community, joining, and specialization in open source software innovation: A case study |year=2003 |last1=Von Krogh |first1=Georg |last2=Spaeth |first2=Sebastian |last3=Lakhani |first3=Karim R |journal=Research Policy |volume=32 |issue=7 |pages=1217–41}}\n*{{cite book |doi=10.1007/3-540-44702-4_5 |chapter=The Free Haven Project: Distributed Anonymous Storage Service |title=Designing Privacy Enhancing Technologies |series=Lecture Notes in Computer Science |year=2001 |last1=Dingledine |first1=Roger |last2=Freedman |first2=Michael J. |last3=Molnar |first3=David |isbn=978-3-540-41724-8 |pages=67–95}}\n*{{cite book |doi=10.1007/3-540-44702-4_4 |chapter=Freenet: A Distributed Anonymous Information Storage and Retrieval System |title=Designing Privacy Enhancing Technologies |series=Lecture Notes in Computer Science |year=2001 |last1=Clarke |first1=Ian |last2=Sandberg |first2=Oskar |last3=Wiley |first3=Brandon |last4=Hong |first4=Theodore W. |isbn=978-3-540-41724-8 |pages=46–66}}\n*{{cite journal |first1=Damien A. |last1=Riehl |year=2000 |title=Peer-to-Peer Distribution Systems: Will Napster, Gnutella, and Freenet Create a Copyright Nirvana or Gehenna? |journal=The William Mitchell Law Review |volume=27 |issue=3 |pages=1761}}\n*{{cite journal |first1=Ryan |last1=Roemer |date=Fall 2002 |title=The Digital Evolution: Freenet and the Future of Copyright on the Internet |journal=UCLA Journal of Law and Technology |volume=5 |url=http://www.lawtechjournal.com/articles/2002/05_021229_roemer.php}}\n*{{cite journal |last1=Sun |first1=Xiaoqing |last2=Liu |first2=Baoxu |last3=Feng |first3=Dengguo |year=2005 |title=Analysis of Next Generation Routing of Freenet |journal=Computer Engineering |issue=17 |pages=126–8}}\n*{{cite book |doi=10.1109/INFCOM.2002.1019373 |chapter=Using the small-world model to improve Freenet performance |title=INFOCOM 2002: Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies |year=2002 |last1=Hui Zhang |last2=Goel |first2=Ashish |last3=Govindan |first3=Ramesh |isbn=978-0-7803-7476-8 |volume=3 |pages=1228–37}}\n\n==External links==\n* [https://freenetproject.org/ The Freenet Project]\n\n{{File sharing}}\n{{Internet censorship circumvention technologies}}\n\n[[Category:Free file transfer software]]\n[[Category:Free file sharing software]]\n[[Category:Distributed file systems]]\n[[Category:Anonymous file sharing networks]]\n[[Category:Anonymity networks]]\n[[Category:Distributed data storage systems]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Cross-platform software]]\n[[Category:Beta software]]",
            "slug": "freenet",
            "date_updated": 1517191314776,
            "imported": "https://en.wikipedia.org/wiki/Freenet"
        },
        {
            "title": "Development ideas",
            "text": "This is a list of development ideas for [[Kiwipedia]].\n\nTest",
            "slug": "development-ideas",
            "date_updated": 1517191354230,
            "imported": ""
        },
        {
            "title": "Development ideas",
            "text": "This is a list of development ideas for [[Kiwipedia]].",
            "slug": "development-ideas",
            "date_updated": 1517191358861,
            "imported": ""
        },
        {
            "title": "",
            "text": "{{about|the branch of computer science and mathematics|the journal|Theoretical Computer Science (journal)}}\n[[Image:Maquina.png|thumb|An artistic representation of a [[Turing machine]]. Turing machines are used to model general computing devices.]]\n\n'''Theoretical computer science''', or TCS, is a subset of general [[computer science]] and [[mathematics]] that focuses on more mathematical topics \nof computing and includes the [[theory of computation]].\n\nIt is difficult to circumscribe the theoretical areas precisely. The [[Association for Computing Machinery|ACM]]'s [[ACM SIGACT|Special Interest Group on Algorithms and Computation Theory]] (SIGACT) provides the following description:<ref>{{cite web | title =  SIGACT | url = https://www.sigact.org/ | accessdate = 2017-01-19}}</ref>\n\n{{\"|TCS covers a wide variety of topics including [[algorithms]], [[data structure]]s, [[computational complexity theory|computational complexity]], [[parallel computation|parallel]] and [[distributed computation|distributed]] computation, [[probabilistic computation]], [[quantum computation]], [[automata theory]], [[information theory]], [[cryptography]], [[program semantics]] and [[Formal methods|verification]], [[machine learning]], [[computational biology]], [[computational economics]], [[computational geometry]], and [[computational number theory]] and [[Symbolic computation|algebra]]. Work in this field is often distinguished by its emphasis on mathematical technique and [[rigor#Mathematical rigour|rigor]].}}\n\nIn this list, the ACM's journal Transactions on Computation Theory includes [[coding theory]] and [[computational learning theory]], as well as theoretical computer science aspects of areas such as [[databases]], [[information retrieval]], economic models, and [[Computer network|networks]].<ref>{{cite web | title =  ToCT| url = http://toct.acm.org/journal.html | accessdate = 2010-06-09}}</ref> Despite this broad scope, the \"theory people\" in computer science self-identify as different from the \"applied people\"{{Citation needed|date=September 2017}}. Some characterize themselves as doing the \"(more fundamental) 'science(s)' underlying the field of computing.\"<ref>{{cite web | title = Challenges for Theoretical Computer Science: Theory as the Scientific Foundation of Computing | url = http://www.research.att.com/%7Edsj/nsflist.html#Intro | accessdate = 2009-03-29}}</ref> Other \"theory-applied people\" suggest that it is impossible to separate theory and application. This means that the so-called \"theory people\" regularly use experimental science(s) done in less-theoretical areas such as [[software system]] research{{Citation needed|date=September 2017}}. It also means that there is more cooperation than mutually exclusive competition between theory and application{{Citation needed|date=September 2017}}.\n\n== History ==\n{{Main|History of computer science}}\n\nWhile logical inference and mathematical proof had existed previously, in 1931 [[Kurt Gödel]] proved with his [[incompleteness theorem]] that there are fundamental limitations on what statements could be proved or disproved.\n\nThese developments have led to the modern study of logic and [[computability]], and indeed the field of theoretical computer science as a whole{{Citation needed|date=September 2017}}. [[Information theory]] was added to the field{{by whom|date=September 2017}} with a 1948 mathematical theory of communication by [[Claude Shannon]]. In the same decade, [[Donald Hebb]] introduced a mathematical model of [[Hebbian learning|learning]] in the brain. With mounting biological data supporting this hypothesis with some modification, the fields of [[neural network]]s and [[parallel distributed processing]] were established. In 1971, [[Stephen Cook]] and, working independently, [[Leonid Levin]], proved that there exist practically relevant problems that are [[NP-complete]] &ndash; a landmark result in [[computational complexity theory]]{{Citation needed|date=September 2017}}.\n\nWith the development of [[quantum mechanics]] in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction. In other words, one could compute functions on multiple states simultaneously. This led to the concept of a [[quantum computer]] in the latter half of the 20th century that took off in the 1990s when [[Peter Shor]] showed that such methods could be used to factor large numbers in [[polynomial time]], which, if implemented, would render most modern [[public key cryptography]] systems uselessly insecure.{{citation needed|reason=Your explanation here|date=October 2014}}\n\nModern theoretical computer science research is based on these basic developments, but includes many other mathematical and interdisciplinary problems that have been posed, as shown below:\n\n{| style=\"border:1px solid #ddd; text-align:center;  margin: 0 auto;\" cellspacing=\"15\"\n| <math> P \\rightarrow Q \\,</math>\n| [[File:DFAexample.svg|96px]]\n| [[File:Elliptic curve simple.png|96px]]\n| [[File:6n-graf.svg|96px]]\n| [[File:Wang tiles.svg|96px]]\n| '''P = NP''' ?\n|-\n| [[Mathematical logic]]\n| [[Automata theory]]\n| [[Number theory]]\n| [[Graph theory]]\n| [[Computability theory]]\n| [[Computational complexity theory]]\n|-\n| '''GNITIRW-TERCES'''\n| <math>\\Gamma\\vdash x: \\text{Int}</math>\n| [[File:Commutative diagram for morphism.svg|96px]]\n| [[File:SimplexRangeSearching.svg|96px]]\n| [[File:TSP Deutschland 3.png|96px]]\n| [[File:Blochsphere.svg|96px]]\n|-\n| [[Cryptography]]\n| [[Type theory]]\n| [[Category theory]]\n| [[Computational geometry]]\n| [[Combinatorial optimization]]\n| [[Quantum computer|Quantum computing theory]]\n|}\n\n== Topics ==\n\n===Algorithms===\n{{main|Algorithm}}\nAn [[algorithm]] is a step-by-step procedure for calculations.  Algorithms are used for [[calculation]], [[data processing]], and [[automated reasoning]].\n\nAn algorithm is an [[effective method]] expressed as a [[wikt:finite|finite]] list<ref>\"Any classical mathematical algorithm, for example, can be described in a finite number of English words\" (Rogers 1987:2).</ref> of well-defined instructions<ref>Well defined with respect to the agent that executes the algorithm: \"There is a computing agent, usually human, which can react to the instructions and carry out the computations\" (Rogers 1987:2).</ref> for calculating a [[Function (mathematics)|function]].<ref>\"an algorithm is a procedure for computing a ''function'' (with respect to some chosen notation for integers) ... this limitation (to numerical functions) results in no loss of generality\", (Rogers 1987:1).</ref>  Starting from an initial state and initial input (perhaps [[null string|empty]]),<ref>\"An algorithm has [[zero]] or more inputs, i.e., [[quantity|quantities]] which are given to it initially before the algorithm begins\" (Knuth 1973:5).</ref> the instructions describe a [[computation]] that, when [[Execution (computing)|executed]], proceeds through a finite<ref>\"A procedure which has all the characteristics of an algorithm except that it possibly lacks finiteness may be called a 'computational method'\" (Knuth 1973:5).</ref> number of well-defined successive states, eventually producing \"output\"<ref>\"An algorithm has one or more outputs, i.e. quantities which have a specified relation to the inputs\" (Knuth 1973:5).</ref> and terminating at a final ending state. The transition from one state to the next is not necessarily [[deterministic]]; some algorithms, known as [[randomized algorithms]], incorporate random input.<ref>Whether or not a process with random interior processes (not including the input) is an algorithm is debatable. Rogers opines that: \"a computation is carried out in a discrete stepwise fashion, without use of continuous methods or analogue devices . . . carried forward deterministically, without resort to random methods or devices, e.g., dice\" Rogers 1987:2.</ref>\n\n===Data structures===\n{{main|Data structure}}\nA [[data structure]] is a particular way of organizing [[data (computing)|data]] in a computer so that it can be used [[algorithmic efficiency|efficiently]].<ref>Paul E. Black (ed.), entry for ''data structure'' in ''[[Dictionary of Algorithms and Data Structures]]. U.S. [[National Institute of Standards and Technology]]. 15 December 2004. [http://xlinux.nist.gov/dads/HTML/datastructur.html Online version] Accessed May 21, 2009.''</ref><ref>Entry ''data structure'' in the [[Encyclopædia Britannica]] (2009) [http://www.britannica.com/EBchecked/topic/152190/data-structure Online entry] accessed on May 21, 2009.</ref>\n\nDifferent kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, databases use [[B-tree]] indexes for small percentages of data retrieval and [[compiler]]s and databases use dynamic [[hash table]]s as look up tables.\n\nData structures provide a means to manage large amounts of data efficiently for uses such as large [[database]]s and [[web indexing|internet indexing services]]. Usually, efficient data structures are key to designing efficient [[algorithm]]s. Some formal design methods and [[programming language]]s emphasize data structures, rather than algorithms, as the key organizing factor in software design. Storing and retrieving can be carried out on data stored in both [[main memory]] and in [[secondary memory]].\n\n===Computational complexity theory===\n{{main|Computational complexity theory}}\n[[Computational complexity theory]] is a branch of the [[theory of computation]] that focuses on classifying [[computational problems]] according to their inherent difficulty, and relating those [[Complexity class|classes]] to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an [[algorithm]].\n\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the [[algorithm]] used. The theory formalizes this intuition, by introducing mathematical [[models of computation]] to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other [[complexity]] measures are also used, such as the amount of communication (used in [[communication complexity]]), the number of [[logic gate|gates]] in a circuit (used in [[circuit complexity]]) and the number of processors (used in [[parallel computing]]). One of the roles of computational complexity theory is to determine the practical limits on what [[computer]]s can and cannot do.\n\n===Distributed computation===\n{{main|Distributed computation}}\n[[Distributed computing]] studies distributed systems. A distributed system is a software system in which components located on [[computer network|networked computers]] communicate and coordinate their actions by [[message passing|passing messages]].<ref name=\"Coulouris\">{{cite book|last=Coulouris|first=George|author2=Jean Dollimore|author3=Tim Kindberg|author4=Gordon Blair|title=Distributed Systems: Concepts and Design (5th Edition)|publisher = Addison-Wesley|year=2011|location=Boston|isbn=0-132-14301-1}}</ref> The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.<ref name=\"Coulouris\"/> Examples of distributed systems vary from [[Service-oriented architecture|SOA-based systems]] to [[massively multiplayer online game]]s to [[Peer-to-peer| peer-to-peer applications]].\n\nA [[computer program]] that runs in a distributed system is called a '''distributed program''', and distributed programming is the process of writing such programs.<ref>{{harvtxt|Andrews|2000}}. {{harvtxt|Dolev|2000}}. {{harvtxt|Ghosh|2007}}, p. 10.</ref> There are many alternatives for the message passing mechanism, including [[Remote procedure call|RPC-like]] connectors and [[Message-oriented middleware|message queues]].  An important goal and challenge of distributed systems is [[location transparency]].\n\n===Parallel computation===\n{{main|Parallel computation}}\n[[Parallel computing]] is a form of [[computing|computation]] in which many calculations are carried out simultaneously,<ref>{{cite book|last=Gottlieb|first=Allan|title=Highly parallel computing|year=1989|publisher=Benjamin/Cummings|location=Redwood City, Calif.|isbn=0-8053-0177-1|url=http://dl.acm.org/citation.cfm?id=160438|author2=Almasi, George S.}}</ref> operating on the principle that large problems can often be divided into smaller ones, which are then solved [[Parallelism (computing)|\"in parallel\"]]. There are several different forms of parallel computing: [[bit-level parallelism|bit-level]], [[instruction level parallelism|instruction level]], [[data parallelism|data]], and [[task parallelism]]. Parallelism has been employed for many years, mainly in [[high performance computing|high-performance computing]], but interest in it has grown lately due to the physical constraints preventing [[frequency scaling]].<ref>S.V. Adve et al. (November 2008). [http://www.upcrc.illinois.edu/documents/UPCRC_Whitepaper.pdf \"Parallel Computing Research at Illinois: The UPCRC Agenda\"] (PDF). Parallel@Illinois, University of Illinois at Urbana-Champaign. \"The main techniques for these performance benefits&nbsp;– increased clock frequency and smarter but increasingly complex architectures&nbsp;– are now hitting the so-called power wall. The computer industry has accepted that future performance increases must largely come from increasing the number of processors (or cores) on a die, rather than making a single core go faster.\"</ref> As power consumption (and consequently heat generation) by computers has become a concern in recent years,<ref>Asanovic et al. Old [conventional wisdom]: Power is free, but transistors are expensive. New [conventional wisdom] is [that] power is expensive, but transistors are \"free\".</ref> parallel computing has become the dominant paradigm in [[computer architecture]], mainly in the form of [[multi-core processor]]s.<ref name=\"View-Power\">Asanovic, Krste et al. (December 18, 2006). [http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.pdf \"The Landscape of Parallel Computing Research: A View from Berkeley\"] (PDF). University of California, Berkeley. Technical Report No. UCB/EECS-2006-183. \"Old [conventional wisdom]: Increasing clock frequency is the primary method of improving processor performance. New [conventional wisdom]: Increasing parallelism is the primary method of improving processor performance&nbsp;... Even representatives from Intel, a company generally associated with the 'higher clock-speed is better' position, warned that traditional approaches to maximizing performance through maximizing clock speed have been pushed to their limit.\"</ref>\n\n[[Parallel algorithm|Parallel computer programs]] are more difficult to write than sequential ones,<ref>{{cite book|last=Hennessy|first=John L.|title=Computer organization and design : the hardware/software interface|year=1999|publisher=Kaufmann|location=San Francisco|isbn=1-55860-428-6|edition=2. ed., 3rd print.|author2=Patterson, David A. |author3=Larus, James R. }}</ref> because concurrency introduces several new classes of potential [[software bug]]s, of which [[race condition]]s are the most common. [[Computer networking|Communication]] and [[Synchronization (computer science)|synchronization]] between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.\n\nThe maximum possible [[speedup|speed-up]] of a single program as a result of parallelization is known as [[Amdahl's law]].\n\n===Very-large-scale integration===\n{{main|VLSI}}\n[[Very-large-scale integration]] ('''VLSI''') is the process of creating an [[integrated circuit]] (IC) by combining thousands of [[transistors]] into a single chip. VLSI began in the 1970s when complex [[semiconductor]] and [[communication]] technologies were being developed. The [[microprocessor]] is a VLSI device. Before the introduction of VLSI technology most ICs had a limited set of functions they could perform. An [[electronic circuit]] might consist of a [[central processing unit|CPU]], [[Read-only memory|ROM]], [[Random Access Memory|RAM]] and other [[glue logic]]. VLSI allows IC makers to add all of these circuits into one chip.\n\n===Machine learning===\n{{main|Machine learning}}\n[[Machine learning]] is a [[academic disciplines|scientific discipline]] that deals with the construction and study of [[algorithm]]s that can [[learning|learn]] from data.<ref>{{cite journal |title=Glossary of terms |author1=Ron Kovahi |author2=Foster Provost |journal=[[Machine Learning (journal)|Machine Learning]] |volume=30 |pages=271–274 |year=1998 |url=http://ai.stanford.edu/~ronnyk/glossary.html}}</ref> Such algorithms operate by building a [[Statistical model|model]] based on inputs<ref name=\"bishop\">{{cite book |author=C. M. Bishop |authorlink=Christopher M. Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |isbn=0-387-31073-8}}</ref>{{rp|2}} and using that to make predictions or decisions, rather than following only explicitly programmed instructions.\n\nMachine learning can be considered a subfield of computer science and [[statistics]]. It has strong ties to [[artificial intelligence]] and [[mathematical optimization|optimization]], which deliver methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit, rule-based [[algorithm]]s is infeasible. Example applications include [[spam filter]]ing, [[optical character recognition]] (OCR),<ref name=Wernick-Signal-Proc-July-2010>Wernick, Yang, Brankov, Yourganov and Strother, Machine Learning in Medical Imaging, ''[[IEEE Signal Processing Society|IEEE Signal Processing Magazine]]'', vol. 27, no. 4, July 2010, pp. 25-38</ref> [[Learning to rank|search engines]] and [[computer vision]]. Machine learning is sometimes conflated with [[data mining]],<ref>{{cite conference |last=Mannila |first=Heikki |title=Data mining: machine learning, statistics, and databases |conference=Int'l Conf. Scientific and Statistical Database Management |publisher=IEEE Computer Society |year=1996}}</ref> although that focuses more on exploratory data analysis.<ref>{{cite journal |last=Friedman |first=Jerome H. |authorlink=Jerome H. Friedman |title=Data Mining and Statistics: What's the connection? |journal=Computing Science and Statistics |volume=29 |issue=1 |year=1998 |pages=3–9}}</ref> Machine learning and [[pattern recognition]] \"can be viewed as two facets of\nthe same field.\"<ref name=\"bishop\"/>{{rp|vii}}\n\n===Computational biology===\n{{main|Computational biology}}\n[[Computational biology]] involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.<ref name=\"nih\">\n{{cite web\n| url          = http://www.bisti.nih.gov/docs/compubiodef.pdf\n| title        = NIH working definition of bioinformatics and computational biology\n| date         = 17 July 2000\n| accessdate   = 18 August 2012\n| publisher    = Biomedical Information Science and Technology Initiative\n}}\n</ref> The field is broadly defined and includes foundations in computer science, [[applied mathematics]], [[animation]], [[statistics]], [[biochemistry]], [[chemistry]], [[biophysics]], [[molecular biology]], [[genetics]], [[genomics]], [[ecology]], [[evolution]], [[anatomy]], [[neuroscience]], and [[scientific visualization|visualization]].<ref name=\"brown\">\n{{cite web\n| url          = http://www.brown.edu/research/projects/computational-molecular-biology/\n| title        = About the CCMB\n| accessdate   = 18 August 2012\n| publisher    = Center for Computational Molecular Biology\n}}\n</ref>\n\nComputational biology is different from [[biological computation]], which is a subfield of computer science and [[computer engineering]] using [[bioengineering]] and [[biology]] to build [[computer]]s, but is similar to [[bioinformatics]], which is an interdisciplinary science using computers to store and process biological data.\n\n===Computational geometry===\n{{main|Computational geometry}}\n[[Computational geometry]] is a branch of computer science devoted to the study of algorithms that can be stated in terms of [[geometry]]. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. An ancient precursor is the [[Sanskrit]] treatise [[Shulba Sutras]], or \"Rules of the Chord\", that is a book of algorithms written in 800 BCE. The book prescribes step-by-step procedures for constructing geometric objects like altars using a peg and chord.\n\nThe main impetus for the development of computational geometry as a discipline was progress in [[computer graphics]] and computer-aided design and manufacturing ([[Computer-aided design|CAD]]/[[Computer-aided manufacturing|CAM]]), but many problems in computational geometry are classical in nature, and may come from [[mathematical visualization]].\n\nOther important applications of computational geometry include [[robotics]] (motion planning and visibility problems), [[geographic information system]]s (GIS) (geometrical location and search, route planning), [[integrated circuit]] design (IC geometry design and verification), [[computer-aided engineering]] (CAE) (mesh generation), [[computer vision]] (3D reconstruction).\n\n===Information theory===\n{{main|Information theory}}\n[[Information theory]] is a branch of [[applied mathematics]], [[electrical engineering]], and [[computer science]] involving the [[Quantification (science)|quantification]] of [[information]].  Information theory was developed by [[Claude E. Shannon]] to find fundamental limits on [[signal processing]] operations such as [[data compression|compressing data]] and on reliably [[Computer data storage|storing]] and [[Telecommunication|communicating]] data. Since its inception it has broadened to find applications in many other areas, including [[statistical inference]], [[natural language processing]], [[cryptography]], [[neurobiology]],<ref>{{cite book|author1=F. Rieke |author2=D. Warland |author3=R Ruyter van Steveninck |author4=W Bialek |title=Spikes: Exploring the Neural Code|publisher=The MIT press|year=1997|isbn=978-0262681087}}</ref> the evolution<ref>cf. Huelsenbeck, J. P., F. Ronquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny and its impact on evolutionary biology, ''Science'' '''294''':2310-2314</ref> and function<ref>Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, [http://alum.mit.edu/www/toms/ Thomas D. Schneider], Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, ''Gene'' '''215''':1, 111-122</ref> of molecular codes, [[model selection]] in statistics,<ref>Burnham, K. P. and Anderson D. R. (2002) ''Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition'' (Springer Science, New York) {{isbn|978-0-387-95364-9}}.</ref> thermal physics,<ref>Jaynes, E. T. (1957) [http://bayes.wustl.edu/ Information Theory and Statistical Mechanics], ''Phys. Rev.'' '''106''':620</ref> [[quantum computing]], [[linguistics]], plagiarism detection,<ref>Charles H. Bennett, Ming Li, and Bin Ma (2003) [http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 Chain Letters and Evolutionary Histories], ''Scientific American'' '''288''':6, 76-81</ref> [[pattern recognition]], [[anomaly detection]] and other forms of [[data analysis]].<ref>\n{{Cite web\n | author = David R. Anderson\n | title = Some background on why people in the empirical sciences may want to better understand the information-theoretic methods\n | date = November 1, 2003\n | url = http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf\n | format = pdf\n | accessdate = 2010-06-23}}\n</ref>\n\nApplications of fundamental topics of information theory include [[lossless data compression]] (e.g. [[ZIP (file format)|ZIP files]]), [[lossy data compression]] (e.g. [[MP3]]s and [[JPEG]]s), and [[channel capacity|channel coding]] (e.g. for [[DSL|Digital Subscriber Line (DSL)]]).  The field is at the intersection of [[mathematics]], [[statistics]], [[computer science]], [[physics]], [[neurobiology]], and [[electrical engineering]]. Its impact has been crucial to the success of the [[Voyager program|Voyager]] missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the [[Internet]], the study of [[linguistics]] and of human perception, the understanding of [[black hole]]s, and numerous other fields. Important sub-fields of information theory are [[source coding]], [[channel coding]], [[algorithmic complexity theory]], [[algorithmic information theory]], [[information-theoretic security]], and measures of information.\n\n===Cryptography===\n{{main|Cryptography}}\n[[Cryptography]]  is the practice and study of techniques for [[secure communication]] in the presence of third parties (called [[adversary (cryptography)|adversaries]]).<ref name=\"rivest90\">{{cite book|first=Ronald L.|last=Rivest|authorlink=Ron Rivest|editor=J. Van Leeuwen|title=Handbook of Theoretical Computer Science|chapter=Cryptology|volume=1|publisher=Elsevier|year=1990}}</ref> More generally, it is about constructing and analyzing [[communications protocol|protocol]]s that overcome the influence of adversaries<ref name=\"modern-crypto\">{{Cite book|first1=Mihir|last1=Bellare|first2=Phillip|last2=Rogaway|title=Introduction to Modern Cryptography|chapter=Introduction|page=10|date=21 September 2005}}</ref> and that are related to various aspects in [[information security]] such as data [[confidentiality]], [[data integrity]], [[authentication]], and [[non-repudiation]].<ref name=\"hac\">{{cite book |first=A. J. |last=Menezes |first2=P. C. |last2=van Oorschot |first3=S. A. |last3=Vanstone |url=https://web.archive.org/web/20050307081354/www.cacr.math.uwaterloo.ca/hac/ |title=Handbook of Applied Cryptography |publisher= |isbn=0-8493-8523-7}}</ref> Modern cryptography intersects the disciplines of [[mathematics]], [[computer science]], and [[electrical engineering]]. Applications of cryptography include [[automated teller machine|ATM cards]], [[password|computer passwords]], and [[electronic commerce]].\n\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around [[computational hardness assumption]]s, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in [[integer factorization]] algorithms, and faster computing technology require these solutions to be continually adapted. There exist [[Information theoretic security|information-theoretically secure]] schemes that {{not a typo|provably}} cannot be broken even with unlimited computing power—an example is the [[one-time pad]]—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.\n\n===Quantum computation===\n{{main|Quantum computation}}\nA [[quantum computer]] is a [[computation]] system that makes direct use of [[quantum mechanics|quantum-mechanical]] [[phenomena]], such as [[quantum superposition|superposition]] and [[quantum entanglement|entanglement]], to perform [[Instruction (computer science)|operations]] on [[data]].<ref>\"[http://cba.mit.edu/docs/papers/98.06.sciqc.pdf Quantum Computing with Molecules]\" article in ''[[Scientific American]]'' by [[Neil Gershenfeld]] and [[Isaac L. Chuang]]</ref> Quantum computers are different from digital computers based on [[transistor]]s. Whereas digital computers require data to be encoded into binary digits ([[bit]]s), each of which is always in one of two definite states (0 or 1), quantum computation uses [[qubits]] (quantum bits), which can be in [[quantum superposition|superpositions]] of states. A theoretical model is the [[quantum Turing machine]], also known as the universal quantum computer.  Quantum computers share theoretical similarities with [[Non-deterministic Turing machine|non-deterministic]] and [[probabilistic automaton|probabilistic computers]]; one example is the ability to be in more than one state simultaneously.  The field of quantum computing was first introduced by [[Yuri Manin]] in 1980<ref name=\"manin1980vychislimoe\">{{cite book| author=Manin, Yu. I.| title=Vychislimoe i nevychislimoe |trans-title=Computable and Noncomputable | year=1980| publisher=Sov.Radio| url=http://publ.lib.ru/ARCHIVES/M/MANIN_Yuriy_Ivanovich/Manin_Yu.I._Vychislimoe_i_nevychislimoe.(1980).%5Bdjv%5D.zip| pages=13–15| language=Russian| accessdate=4 March 2013}}</ref> and [[Richard Feynman]] in 1982.<ref name=\"Feynman82\">{{cite journal |last=Feynman |first=R. P. |title=Simulating physics with computers |journal=[[International Journal of Theoretical Physics]] |year=1982 |volume=21 |issue=6 |pages=467–488 |doi=10.1007/BF02650179 }}</ref><ref>{{cite journal |title=Quantum computation |authorlink=David Deutsch |first=David |last=Deutsch |journal=Physics World |date=1992-01-06 }}</ref> A quantum computer with spins as quantum bits was also formulated for use as a quantum [[space–time]] in 1968.<ref>{{cite book |first=David |last=Finkelstein |chapter=Space-Time Structure in High Energy Interactions |title=Fundamental Interactions at High Energy |editor1-first=T. |editor1-last=Gudehus |editor2-first=G. |editor2-last=Kaiser |location=New York |publisher=Gordon & Breach |year=1968 }}</ref>\n\n{{as of|2014}}, quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on a very small number of qubits.<ref>{{cite web|url=http://phys.org/news/2013-01-qubit-bodes-future-quantum.html|title=New qubit control bodes well for future of quantum computing|publisher=|accessdate=26 October 2014}}</ref> Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum [[computer]]s for both civilian and national security purposes, such as [[cryptanalysis]].<ref>[http://qist.lanl.gov/qcomp_map.shtml Quantum Information Science and Technology Roadmap] for a sense of where the research is heading.</ref>\n\n===Information-based complexity===\n{{main|Information-based complexity}}\nInformation-based complexity (IBC) studies optimal algorithms and computational complexity for continuous problems. IBC has studied continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration.\n\n===Computational number theory===\n{{main|Computational number theory}}\n[[Computational number theory]], also known as '''algorithmic number theory''', is the study of [[algorithm]]s for performing [[number theory|number theoretic]] [[computation]]s. The best known problem in the field is [[integer factorization]].\n\n===Symbolic computation===\n{{main|Symbolic computation}}\n[[Computer algebra]], also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of [[algorithm]]s and [[software]] for manipulating [[expression (mathematics)|mathematical expressions]] and other [[mathematical object]]s. Although, properly speaking, computer algebra should be a subfield of [[scientific computing]], they are generally considered as distinct fields because scientific computing is usually based on [[numerical computation]] with approximate [[floating point number]]s, while symbolic computation emphasizes ''exact'' computation with expressions containing [[variable (mathematics)|variable]]s that have not any given value and are thus manipulated as symbols (therefore the name of ''symbolic computation'').\n\n[[Software]] applications that perform symbolic calculations are called ''[[computer algebra system]]s'', with the term ''system'' alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a [[user interface]] for the input/output of mathematical expressions, a large set of [[function (computer science)|routines]] to perform usual operations, like simplification of expressions, [[differentiation (mathematics)|differentiation]] using [[chain rule]], [[polynomial factorization]], [[indefinite integration]], etc.\n\n===Program semantics===\n{{main|Program semantics}}\nIn [[programming language theory]], '''semantics''' is the field concerned with the rigorous mathematical study of the meaning of [[programming language]]s. It does so by evaluating the meaning of [[programming language syntax|syntactically]] legal [[String (computer science)|strings]] defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically illegal strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will execute on a certain [[computer platform|platform]], hence creating a [[model of computation]].\n\n===Formal methods===\n{{main|Formal methods}}\n[[Formal methods]] are a particular kind of [[mathematics]] based techniques for the [[formal specification|specification]], development and [[formal verification|verification]] of [[software]] and [[computer hardware|hardware]] systems.<ref name=\"butler\">{{cite web|author=R. W. Butler|title=What is Formal Methods?|url=http://shemesh.larc.nasa.gov/fm/fm-what.html|date=2001-08-06|accessdate=2006-11-16}}</ref> The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.<ref>{{cite journal|author=C. Michael Holloway|title=Why Engineers Should Consider Formal Methods|url=http://klabs.org/richcontent/verification/holloway/nasa-97-16dasc-cmh.pdf| publisher=16th Digital Avionics Systems Conference (27–30 October 1997)|accessdate=2006-11-16}}</ref>\n\nFormal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular [[logic in computer science|logic]] calculi, [[formal language]]s, [[automata theory]], and [[program semantics]], but also [[type systems]] and [[algebraic data types]] to problems in software and hardware specification and verification.<ref>Monin, pp.3-4</ref>\n\n===Automata theory===\n{{main|Automata theory}}\n[[Automata theory]] is the study of ''[[abstract machine]]s'' and ''[[automaton|automata]]'', as well as the computational problems that can be solved using them. It is a theory in theoretical computer science, under [[Discrete mathematics]] (a section of [[Mathematics]] and also of [[Computer Science]]). ''Automata'' comes from the Greek word αὐτόματα meaning \"self-acting\".\n\nAutomata Theory is the study of self-operating virtual machines to help in logical understanding of input and output process, without or with intermediate stage(s) of [[computation]] (or any [[Function (engineering)|function]] / process).\n\n===Coding theory===\n{{main|Coding theory}}\n[[Coding theory]] is the study of the properties of codes and their fitness for a specific application. Codes are used for [[data compression]], [[cryptography]], [[error-correction]] and more recently also for [[network coding]]. Codes are studied by various scientific disciplines—such as [[information theory]], [[electrical engineering]],  [[mathematics]], and [[computer science]]—for the purpose of designing efficient and reliable [[data transmission]] methods. This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.\n\n===Computational learning theory===\n{{main|Computational learning theory}}\nTheoretical results in machine learning mainly deal with a type of\ninductive learning called supervised learning.  In supervised\nlearning, an algorithm is given samples that are labeled in some\nuseful way.  For example, the samples might be descriptions of\nmushrooms, and the labels could be whether or not the mushrooms are\nedible.  The algorithm takes these previously labeled samples and\nuses them to induce a classifier.  This classifier is a function that\nassigns labels to samples including the samples that have never been\npreviously seen by the algorithm.  The goal of the supervised learning\nalgorithm is to optimize some measure of performance such as\nminimizing the number of mistakes made on new samples.\n\n== Organizations ==\n* [[European Association for Theoretical Computer Science]]\n* [[SIGACT]]\n* [[Simons Institute for the Theory of Computing]]\n\n== Journals and newsletters ==\n* ''[[Information and Computation]]''\n* ''[[Theory of Computing (journal)|Theory of Computing]]'' ([[Open access (publishing)|open access]] journal)\n* ''[[Formal Aspects of Computing]]''\n* ''[[Journal of the ACM]]''\n* ''[[SIAM Journal on Computing]]'' (SICOMP)\n* ''[[SIGACT News]]''\n* ''[[Theoretical Computer Science (journal)|Theoretical Computer Science]]''\n* ''[[Theory of Computing Systems]]''\n* ''[[International Journal of Foundations of Computer Science]]''\n* ''[[Chicago Journal of Theoretical Computer Science]]'' ([[Open access (publishing)|open access]] journal)\n* ''[[Foundations and Trends in Theoretical Computer Science]]''\n* ''[[Journal of Automata, Languages and Combinatorics]]''\n* ''[[Acta Informatica]]''\n* ''[[Fundamenta Informaticae]]''\n* ''[[ACM Transactions on Computation Theory]]''\n* ''[[Computational Complexity (journal)|Computational Complexity]]''\n* ''[[Journal of Complexity (journal)|Journal of Complexity]]''\n* ACM Transactions on Algorithms\n* Information Processing Letters\n\n== Conferences ==\n* Annual ACM [[Symposium on Theory of Computing]] (STOC)<ref name=\"core-a-plus\">The [http://www.core.edu.au/rankings/Conference%20Ranking%20Main.html 2007 Australian Ranking of ICT Conferences]: tier A+.</ref>\n* Annual IEEE [[Symposium on Foundations of Computer Science]] (FOCS)<ref name=\"core-a-plus\"/>\n* [[Innovations in Theoretical Computer Science]] (ITCS)\n* [[Mathematical Foundations of Computer Science]] (MFCS)<ref>[http://mfcs2017.cs.aau.dk/ MFCS 2017]</ref>\n* [[International Computer Science Symposium in Russia]] (CSR)<ref>[https://logic.pdmi.ras.ru/csr2018/ CSR 2018]</ref>\n* ACM–SIAM [[Symposium on Discrete Algorithms]] (SODA)<ref name=\"core-a-plus\"/>\n* IEEE [[Symposium on Logic in Computer Science]] (LICS)<ref name=\"core-a-plus\"/>\n* [[Computational Complexity Conference]] (CCC)<ref name=\"core-a\"/>\n* [[International Colloquium on Automata, Languages and Programming]] (ICALP)<ref name=\"core-a\"/>\n* Annual [[Symposium on Computational Geometry]] (SoCG)<ref name=\"core-a\">The [http://www.core.edu.au/rankings/Conference%20Ranking%20Main.html 2007 Australian Ranking of ICT Conferences]: tier A.</ref>\n* ACM [[Symposium on Principles of Distributed Computing]] (PODC)<ref name=\"core-a-plus\"/>\n* ACM [[Symposium on Parallelism in Algorithms and Architectures]] (SPAA)<ref name=\"core-a\"/>\n* [[Annual Conference on Learning Theory]] (COLT)<ref name=\"core-a\"/>\n* [[Symposium on Theoretical Aspects of Computer Science]] (STACS)<ref name=\"core-a\"/>\n* [[European Symposium on Algorithms]] (ESA)<ref name=\"core-a\"/>\n* [[Workshop on Approximation Algorithms for Combinatorial Optimization Problems]] (APPROX)<ref name=\"core-a\"/>\n* [[Workshop on Randomization and Computation]] (RANDOM)<ref name=\"core-a\"/>\n* [[International Symposium on Algorithms and Computation]] (ISAAC)<ref name=\"core-a\"/>\n* [[International Symposium on Fundamentals of Computation Theory]] (FCT)<ref>[http://fct11.ifi.uio.no/ FCT 2011] (retrieved 2013-06-03)</ref>\n* [[International Workshop on Graph-Theoretic Concepts in Computer Science]] (WG)\n\n==See also==\n* [[Formal science]]\n* [[Unsolved problems in computer science]]\n* [[List of important publications in theoretical computer science]]\n\n== Notes ==\n<references/>\n\n== Further reading ==\n\n* [[Martin Davis]], Ron Sigal, Elaine J. Weyuker, ''Computability, complexity, and languages: fundamentals of theoretical computer science'', 2nd ed., Academic Press, 1994, {{isbn|0-12-206382-1}}. Covers [[theory of computation]], but also [[program semantics]] and [[quantification theory]]. Aimed at graduate students.\n\n== External links ==\n* [http://www.sigact.org/webpages.php SIGACT directory of additional theory links]\n* [http://theorymatters.org/ Theory Matters Wiki] Theoretical Computer Science (TCS) Advocacy Wiki\n* [news://comp.theory Usenet comp.theory]\n* [http://www.confsearch.org/confsearch/faces/pages/topic.jsp?topic=Theory&sortMode=1&graphicView=1 List of academic conferences in the area of theoretical computer science] at [http://www.confsearch.org confsearch]\n* [http://cstheory.stackexchange.com/ Theoretical Computer Science - StackExchange], a Question and Answer site for researchers in theoretical computer science\n* [http://www.csanimated.com/browse.php Computer Science Animated]\n* http://theory.csail.mit.edu/   @ [[Massachusetts Institute of Technology]]\n\n{{Computer science}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Theoretical Computer Science}}\n[[Category:Theoretical computer science|*]]\n[[Category:Formal sciences]]",
            "slug": "",
            "date_updated": 1517191519717,
            "imported": "https://en.wikipedia.org/wiki/Theoretical_computer_science"
        },
        {
            "title": "Grace Hopper",
            "text": "{{Use mdy dates|date=October 2014}}\n{{Infobox military person\n|name=Grace Murray Hopper\n|birth_date = {{birth date|1906|12|9}}\n|death_date = {{death date and age|1992|1|1|1906|12|9}}\n|birth_place=New York City, New York, U.S.\n|death_place=[[Arlington, Virginia]], U.S.\n|placeofburial=[[Arlington National Cemetery]]\n|placeofburial_label= Place of burial\n|image=Commodore Grace M. Hopper, USN (covered).jpg\n|caption=Rear Admiral Grace M. Hopper, 1984\n|nickname=\"Amazing Grace\"\n|alma_mater = Yale University\n|allegiance= {{flagu|United States of America}}\n|serviceyears=1943–1966, 1967–1971, 1972–1986\n|rank= [[File:US-O7 insignia.svg|24px]] [[Rear admiral (United States)|Rear admiral]] (lower half)\n|branch={{flag|United States Navy}}\n|commands=\n|awards=[[File:Defense Distinguished Service ribbon.svg|border|22px]] [[Defense Distinguished Service Medal]]<br />[[File:Legion of Merit ribbon.svg|border|22px]] [[Legion of Merit]]<br />[[File:Meritorious Service ribbon.svg|border|22px]] [[Meritorious Service Medal (USA)|Meritorious Service Medal]]<br />[[File:American Campaign Medal ribbon.svg|border|22px]] [[American Campaign Medal]]<br />[[File:World War II Victory Medal ribbon.svg|border|22px]] [[World War II Victory Medal]]<br />[[File:National Defense Service Medal ribbon.svg|border|22px]] [[National Defense Service Medal]]<br />[[File:AFRM with Hourglass Device (Silver).jpg|border|22px]] [[Armed Forces Reserve Medal]] with two [[Hourglass Device]]s<br />[[File:U.S. Naval Reserve Medal ribbon.svg|border|22px]] [[Naval Reserve Medal]] <br />[[File:Presidential Medal of Freedom (ribbon).png|border|22px]] [[Presidential Medal of Freedom]] (posthumous)\n|relations=\n|laterwork=\n}}\n\n'''Grace Brewster Murray Hopper''' ({{née|'''Murray'''}}; December 9, 1906 – January 1, 1992) was an American [[computer scientist]] and [[United States Navy]] [[Rear admiral (United States)|rear admiral]].<ref>{{cite news|url = http://content.yudu.com/A2qfj4/201403March/resources/3.htm|title = Amazing Grace: Rear Adm. Grace Hopper, USN, was a pioneer in computer science|first = Mark|last = Cantrell|magazine = Military Officer|publisher = Military Officers Association of America|volume = 12|issue = 3|pages = 52–55, 106|date = March 2014|accessdate = March 1, 2014}}</ref> One of the first programmers of the [[Harvard Mark I]] computer, she was a pioneer of computer programming who invented one of the first [[compiler]] related tools.  She popularized the idea of machine-independent programming languages, which led to the development of [[COBOL]], an early [[high-level programming language]] still in use today.\n\nHopper had attempted to enlist in the Navy during [[World War II]], but she was rejected by the military because she was 34 years of age and too old to enlist. She instead joined the Navy Reserves. Hopper began her computing career when she worked on the Harvard Mark I team that was led by [[Howard H. Aiken]]. In 1949, she joined the [[Eckert–Mauchly Computer Corporation]] and was part of the development team that designed the [[UNIVAC I]] computer in 1944.  It was at Eckert–Mauchly that she began developing the compiler. She believed that computer code could be written in English by using a programming language that was based on English words. The compiler would convert that code into [[machine code]] that would be understood by computers. By 1952, Hopper finished her program [[Linker_(computing)|linker]] (originally called a compiler), which was written for the [[A-0 System]].<ref name=\"Spencer85\">{{cite book|title=Computers and Information Processing|publisher=C.E. Merrill Publishing Co|year=1985|isbn=978-0-675-20290-9|author=Donald D. Spencer}}</ref><ref name=\"Laplante01\">{{cite book|title=Dictionary of computer science, engineering, and technology|publisher=CRC Press|year=2001|isbn=978-0-8493-2691-2|author=Phillip A. Laplante}}</ref><ref name=\"Bunch93\">{{cite book|title=The Timetables of Technology: A Chronology of the Most Important People and Events in the History of Technology|publisher=Simon & Schuster|year=1993|isbn=978-0-671-76918-5|author=Bryan H. Bunch, Alexander Hellemans}}</ref><ref name=\"Booss03\">{{cite book|title=Mathematics and War|publisher=Birkhäuser Verlag|year=2003|isbn=978-3-7643-1634-1|author=Bernhelm Booss-Bavnbek, Jens Høyrup}}</ref>\n\nIn 1954, Eckert–Mauchly chose Hopper to lead their department for automatic programming, and she led the release of some of the first compiled languages like [[FLOW-MATIC]]. In 1959, she participated in the [[CODASYL]] consortium, which consulted Hopper to guide them in creating a machine-independent programming language. This led to the [[COBOL]] language, which was inspired by her idea of a language being based on English words. In 1966, she retired from the Naval Reserve, but in 1967, the Navy recalled her to active duty.  She retired from the Navy in 1986 and found work as a consultant for the [[Digital Equipment Corporation]], sharing her computing experiences.\n\nOwing to her accomplishments and her naval rank, she was sometimes referred to as \"Amazing Grace\".<ref name=\"urlCyber Heroes of the past: Amazing Grace Hopper\">{{cite web|url=http://wvegter.hivemind.net/abacus/CyberHeroes/Hopper.htm|title=Cyber Heroes of the past: \"Amazing Grace\" Hopper|accessdate=December 12, 2012}}</ref><ref name=\"urlGrace Murray Hopper\">{{cite web|url=http://www.agnesscott.edu/lriddle/women/hopper.htm|title=Grace Murray Hopper|accessdate=December 12, 2012}}</ref> The U.S. Navy {{sclass-|Arleigh Burke|destroyer|0}} guided-missile destroyer {{USS|Hopper}} was named for her, as was the [[Cray XE6]] \"Hopper\" supercomputer at [[NERSC]].<ref>{{Cite web|url=http://www.nersc.gov/users/computational-systems/retired-systems/hopper/|title=Hopper|website=www.nersc.gov|access-date=2016-03-19}}</ref> During her lifetime, Hopper was awarded 40 honorary degrees from universities across the world. A [[Hopper College|college]] at [[Yale University]] is named in her honor.  In 1991, she received the [[National Medal of Technology and Innovation|National Medal of Technology]]. On November 22, 2016, she was posthumously awarded the [[Presidential Medal of Freedom]] by President [[Barack Obama]].<ref>{{cite web|url=http://www.cbsnews.com/news/white-house-medal-of-freedom-margaret-hamilton-grace-hopper/|title=White House honors two of tech's female pioneers|work=cbsnews.com|accessdate=November 23, 2016}}</ref>\n\n==Early life and education==\n{{Listen|type=speech|pos=right|filename=Grace Hopper (As Told By U.S. Chief Technology Officer Megan Smith).oggvorbis.ogg|title=Grace Hopper (As Told By U.S. Chief Technology Officer Megan Smith)|description= }}\nHopper was born in New York City. She was the eldest of three children. Her parents, Walter Fletcher Murray and Mary Campbell Van Horne, were of [[Scottish people|Scottish]] and [[Dutch people|Dutch]] descent, and attended [[West End Collegiate Church]].<ref>{{Cite book | publisher = Naval Institute Press| isbn = 1557509522| last = Williams| first = Kathleen Broome| title = Grace Hopper: admiral of the cyber sea| location = Annapolis, Md| series = Library of naval biography| date = 2004}}</ref> Her great-grandfather, Alexander Wilson Russell, an admiral in the US Navy, fought in the [[Battle of Mobile Bay]] during the [[American Civil War|Civil War]].\n\nGrace was very curious as a child; this was a lifelong trait. At the age of seven, she decided to determine how an alarm clock worked and dismantled seven alarm clocks before her mother realized what she was doing (she was then limited to one clock).<ref>{{Cite Journal |last1=Dickason |first=Elizabeth |url=http://inventors.about.com/library/inventors/bl_Grace_Murray_Hopper.htm |title=Looking Back: Grace Murray Hopper's Younger Years |journal=Chips |date=April 1992}}</ref> For her [[University-preparatory school|preparatory school]] education, she attended the [[Wardlaw-Hartridge School|Hartridge School]] in [[Plainfield, New Jersey]]. Hopper was initially rejected for early admission to [[Vassar College]] at age 16 (her test scores in Latin were too low), but she was admitted the following year.  She graduated [[Phi Beta Kappa Society|Phi Beta Kappa]] from Vassar in 1928 with a bachelor's degree in mathematics and physics and earned her master's degree at [[Yale University]] in 1930.\n\nIn 1934, she earned a Ph.D. in mathematics from Yale<ref name=\"NWHM\">{{cite web| url=http://www.nwhm.org/education-resources/biography/biographies/grace-murray-hopper/| title=Grace Murray Hopper (1906-1992)| accessdate=September 1, 2014| publisher=National Women's History Museum| website=nwhm.org}}</ref> under the direction of [[Øystein Ore]].<ref name=\"greenladuke09\"/><ref>Though some books, including Kurt Beyer's ''Grace Hopper and the Invention of the Information Age'', reported that Hopper was the first woman to earn a Yale PhD in mathematics, the first of ten women prior to 1934 was Charlotte Cynthia Barnum (1860–1934). {{Cite news | last = Murray | first = Margaret A. M. | publication-date = May–June 2010 | title = The first lady of math? | periodical = Yale Alumni Magazine | volume = 73 | issue = 5 | pages = 5–6 | issn = 0044-0051 | postscript = <!--None-->}}</ref> Her [[dissertation]], ''New Types of Irreducibility Criteria'', was published that same year.<ref>G. M. Hopper and O. Ore, \"New types of irreducibility criteria,\" ''Bull. Amer. Math. Soc.'' 40 (1934) 216 {{cite web | title=New types of irreducibility criteria | url=http://www.ams.org/journals/bull/1934-40-03/S0002-9904-1934-05818-X/}}</ref> Hopper began teaching mathematics at Vassar in 1931, and was promoted to associate professor in 1941.<ref name=Ogilvie>{{cite book|last=Ogilvie|first=Marilyn|title=The biographical dictionary of women in science: pioneering lives from ancient times to the mid-20th century|year=2000|publisher=Routledge|location=New York|isbn=0-415-92040-X|author2= Joy Harvey|url=https://books.google.com/books?id=QmfyK0QtsRAC&q=hopper#v=snippet&q=hopper&f=false}}{{check cite|reason=doesn't seem to support those dates|date=November 2013}}</ref>\n\nShe was married to [[New York University]] professor Vincent Foster Hopper (1906–76) from 1930 until their divorce in 1945.<ref name=\"greenladuke09\">{{cite book| first1 = Judy | last1 = Green | author1-link = Judy Green (mathematician) | first2 = Jeanne | last2 = LaDuke | author2-link = Jeanne LaDuke|title=Pioneering Women in American Mathematics: The Pre-1940 PhD's |accessdate= |edition= |year=2009 |publisher=[[American Mathematical Society]] |location=Providence, Rhode Island |isbn=978-0821843765}} Biography on p.281-289 of the [https://www.ams.org/bookpages/hmath-34-PioneeringWomen.pdf Supplementary Material] at [https://www.ams.org/publications/authors/books/postpub/hmath-34 AMS]</ref><ref>{{cite news|title=Prof. Vincent Hopper of N.Y.U., Literature Teacher, Dead at 69|newspaper=The New York Times|date=January 21, 1976}}</ref> She did not marry again, but chose to retain his surname.\n\n==Career==\n\n===World War II===\n[[File:Harvard Mark I sign-up.agr.jpg|thumb|Hopper's signatures on a duty officer signup sheet for the Bureau of Ships Computation Project at Harvard, which built and operated the [[Harvard Mark I|Mark I]]]]\nHopper had tried to enlist in the Navy early in [[World War II]]. She was rejected for multiple reasons. At age 34, she was too old to enlist, and her weight to height ratio was too low. She was also denied on the basis that her job as a mathematician and mathematics professor at Vassar College was valuable to the war effort.<ref>{{Cite web|url=https://www.thocp.net/biographies/hopper_grace.html|title=Grace Hopper|website=www.thocp.net|access-date=2016-12-12}}</ref> During the war in 1943, Hopper obtained a leave of absence from Vassar and was sworn into the [[United States Navy Reserve]]; she was one of many women who volunteered to serve in the [[WAVES]]. She had to get an exemption to enlist; she was {{convert|15|lb}} below the Navy minimum weight of {{convert|120|lb}}. She reported in December and trained at the Naval Reserve Midshipmen's School at [[Smith College]] in [[Northampton, Massachusetts]]. Hopper graduated first in her class in 1944, and was assigned to the [[Bureau of Ships]] Computation Project at [[Harvard University]] as a lieutenant, junior grade. She served on the [[Harvard Mark I|Mark I computer]] programming staff headed by [[Howard H. Aiken]]. Hopper and Aiken co-authored three papers on the Mark I, also known as the Automatic Sequence Controlled Calculator. Hopper's request to transfer to the regular Navy at the end of the war was declined due to her advanced age of 38. She continued to serve in the Navy Reserve. Hopper remained at the Harvard Computation Lab until 1949, turning down a full professorship at Vassar in favor of working as a research fellow under a Navy contract at Harvard.<ref name=\"KBW\">{{cite book |last=Williams |first=Kathleen Broome |title=Improbable Warriors: Women Scientists and the U.S. Navy in World War II |accessdate= |edition= |year=2001 |publisher=Naval Institute Press |location=Annapolis, Maryland |isbn=978-1-55750-961-1}}</ref>\n\n[[File: Grace Murray Hopper, in her office in Washington DC, 1978, ©Lynn Gilbert.jpg|thumb|left|Hopper in a computer room in Washington DC, 1978, photographed by [[Lynn Gilbert]]]]\n\n===UNIVAC===\nIn 1949, Hopper became an employee of the [[Eckert–Mauchly Computer Corporation]] as a senior mathematician and joined the team developing the [[UNIVAC I]].<ref name=Ogilvie />  Hopper also served as UNIVAC director of Automatic Programming Development for Remington Rand. The UNIVAC was the first known large-scale electronic computer to be on the market in 1950,  and was more competitive at processing information than the Mark I.<ref>{{Cite book|url=https://www.worldcat.org/oclc/48398924|title=American women inventors|last=Ann.|first=Camp, Carole|date=2004|publisher=Enslow Publishers|isbn=0766015386|location=Berkeley Heights, NJ|oclc=48398924}}</ref>\n\nWhen Hopper recommended the development a new programming language that would use entirely English words, she \"was told very quickly that [she] couldn't do this because computers didn't understand English.\" Her idea was not accepted for 3 years, and she published her first paper on the subject, compilers, in 1952. In the early 1950s, the company was taken over by the [[Remington Rand]] corporation, and it was while she was working for them that her original [[compiler]] work was done.  The program was known as the A compiler and its first version was [[A-0 programming language|A-0]].<ref name=\"mcgee2004\" />{{rp|11}}\n\nIn 1952 she had an operational link-loader, which at the time was referred to as a compiler. She later said that \"Nobody believed that,\" and that she \"had a running compiler and nobody would touch it. They told me computers could only do arithmetic.\"<ref>{{cite web|url=http://www.cs.yale.edu/homes/tap/Files/hopper-wit.html|title=The Wit and Wisdom of Grace Hopper}}</ref> She goes on to say that her compiler \"translated mathematical notation into machine code. Manipulating symbols was fine for mathematicians but it was no good for data processors who were not symbol manipulators. Very few people are really symbol manipulators. If they are they become professional mathematicians, not data processors. It’s much easier for most people to write an English statement than it is to use symbols. So I decided data processors ought to be able to write their programs in English, and the computers would translate them into machine code. That was the beginning of COBOL, a computer language for data processors. I could say “Subtract income tax from pay” instead of trying to write that in octal code or using all kinds of symbols. COBOL is the major language used today in data processing.”<ref>{{cite web|url=https://itunes.apple.com/us/book/grace-murray-hopper/id1197529986?mt=11|title=Lynn Gilbert, Women of Wisdom of Grace Hopper}}</ref>“\n\nIn 1954 Hopper was named the company's first director of automatic programming, and her department released some of the first compiler-based programming languages, including [[MATH-MATIC]] and [[FLOW-MATIC]].<ref name=Ogilvie />\n\n===COBOL===\n[[File: Grace Hopper and UNIVAC.jpg|thumb|Hopper at the [[UNIVAC I]] console, c. 1960]]\nIn the spring of 1959, computer experts from industry and government were brought together in a two-day conference known as the Conference on Data Systems Languages ([[CODASYL]]). Hopper served as a technical consultant to the committee, and many of her former employees served on the short-term committee that defined the new language [[COBOL]] (an acronym for '''CO'''mmon '''B'''usiness-'''O'''riented '''L'''anguage). The new language extended Hopper's FLOW-MATIC language with some ideas from the [[IBM]] equivalent, [[COMTRAN]].  Hopper's belief that programs should be written in a language that was close to English (rather than in [[machine code]] or in languages close to machine code, such as [[assembly language]]s) was captured in the new business language, and COBOL went on to be the most ubiquitous business language to date.<ref name=\"KWB\">{{cite book |last=Beyer |first=Kurt W. |title=Grace Hopper and the Invention of the Information Age |accessdate= |edition= |year=2009 |publisher=MIT Press |location=Cambridge, Massachusetts |isbn=978-0-262-01310-9}}</ref> Among the members of the committee that worked on COBOL was [[Mount Holyoke College]] alumnus [[Jean E. Sammet]]. <ref>{{cite web|url=https://www.nytimes.com/2017/06/04/technology/obituary-jean-sammet-software-designer-cobol.html|title=Jean Sammet, Co-Designer of a Pioneering Computer Language, Dies at 89|first=Steve|last=Lohr|date=June 4, 2017|publisher=|via=www.nytimes.com}}</ref>\n\nFrom 1967 to 1977, Hopper served as the director of the Navy Programming Languages Group in the Navy's Office of Information Systems Planning and was promoted to the rank of [[Captain (United States O-6)|captain]] in 1973.<ref name=\"KBW\"/> She developed validation software for COBOL and its compiler as part of a COBOL standardization program for the entire Navy.<ref name=\"KBW\" />\n\n===Standards===\nIn the 1970s, Hopper advocated for the Defense Department to replace large, centralized systems with networks of small, distributed computers. Any user on any computer node could access common databases located on the network.<ref name=\"mcgee2004\">{{cite book |last=McGee |first=Russell C.|title=My Adventure with Dwarfs: A Personal History in Mainframe Computers |url=http://www.cbi.umn.edu/hostedpublications/pdf/McGee_Book-4.2.2.pdf |publisher=Charles Babbage Institute |location=University of Minnesota |date=2004 |accessdate=May 7, 2014}}</ref>{{rp|119}} She developed the implementation of [[standardization|standards]] for testing computer systems and components, most significantly for early [[programming language]]s such as [[FORTRAN]] and COBOL. The Navy tests for conformance to these standards led to significant convergence among the programming language dialects of the major computer vendors. In the 1980s, these tests (and their official administration) were assumed by the National Bureau of Standards (NBS), known today as the [[National Institute of Standards and Technology]] (NIST).\n\n==Retirement==\n[[File:Grace Hopper being promoted to Commodore.JPEG|thumb|left|upright|Hopper being promoted to the rank of commodore in 1983]]\nIn accordance with Navy attrition regulations, Hopper retired from the Naval Reserve with the rank of [[Commander (United States)|commander]] at age 60 at the end of 1966.<ref name=\"urlAttrition/Retirement\">{{cite web |title=Attrition/Retirement |url=http://www.public.navy.mil/BUPERS-NPC/CAREER/RESERVEPERSONNELMGMT/OFFICERS/Pages/AttritionRetirement.aspx |accessdate=April 29, 2013}}</ref>  She was recalled to active duty in August 1967 for a six-month period that turned into an indefinite assignment.  She again retired in 1971 but was again asked to return to active duty in 1972. She was promoted to [[Captain (U.S. Navy)|captain]] in 1973 by [[Admiral (United States)|Admiral]] [[Elmo R. Zumwalt, Jr.]]<ref name=navybio/>\n\nAfter [[Republican Party (United States)|Republican]] Representative [[Philip Crane]] saw her on a March 1983 segment of ''[[60 Minutes]]'', he championed {{USBill|98|h.j.res|341}}, a joint [[resolution (law)|resolution]] originating in the [[United States House of Representatives|House of Representatives]], which led to her promotion to [[Commodore (United States)|commodore]] by special Presidential appointment.<ref name=navybio>{{cite web |url=http://www.history.navy.mil/research/histories/bios/hopper-grace.html |title=Rear Admiral Grace Murray Hopper, USN |accessdate=May 28, 2007 |work=Biographies in Naval History |publisher=United States Navy Naval Historical Center}}</ref><ref>{{cite web |url=http://www.history.navy.mil/photos/pers-us/uspers-h/g-hoppr7.htm|title=Rear Admiral Grace Murray Hopper, USNR, (1906–1992) Informal Images taken during the 1980s|quote=Commodore Grace M. Hopper, USNR. receives congratulations from President Ronald Reagan, following her promotion from the rank of Captain to Commodore in ceremonies at the White House, 15 December 1983 |accessdate=July 2, 2013 |work=Biographies in Naval History |publisher=United States Navy Naval Historical Center}}</ref><ref>{{cite web|archiveurl=https://web.archive.org/web/20131019185550/http://www.defense.gov/specials/reagan/reaganphotoessay/grace_11.html|archivedate=October 19, 2013|url=http://www.defense.gov/specials/reagan/reaganphotoessay/grace_11.html|accessdate=March 7, 2016 |title=Historic Images of Ronald Reagan|quote=President Ronald Reagan greets Navy Capt. Grace Hopper as she arrives at the White House for her promotion to Commodore, Dec. 15, 1983. Hopper was a computer technology pioneer|publisher=U.S. Defense Department}}</ref><ref name=\"DavidLetterman86\"/> She remained on active duty for several years beyond mandatory retirement by special approval of Congress.<ref>{{Cite book|title=American Military Technology: The Life Story of a Technology|first=Barton C.|last=Hacker|publisher=Greenwood Publishing Group|year=2006|isbn=9780313333088|page=131|url=https://books.google.com/books?id=ufpinQqFJ_gC&pg=PA131}}</ref>  Effective November 8, 1985, the rank of commodore was renamed [[Rear Admiral (United States)|rear admiral]] (lower half) and Hopper became one of the Navy's few female admirals.\n\nFollowing a career that spanned more than 42 years, Admiral Hopper took mandatory retirement from the Navy on August 14, 1986.  At a celebration held in Boston on the {{USS|Constitution}} to commemorate her retirement, Hopper was awarded the [[Defense Distinguished Service Medal]], the highest non-combat decoration awarded by the Department of Defense.\n\nAt the time of her retirement, she was the oldest active-duty commissioned officer in the United States Navy (79 years, eight months and five days), and had her retirement ceremony aboard the oldest commissioned ship in the United States Navy (188 years, nine months and 23 days).<ref>{{Cite news|work=[[Detroit Free Press]] |date= August 15, 1986 |page= 4A |url=http://www.waterholes.com/~dennette/1996/hopper/860815.htm |title=Computer Whiz Retires from Navy |agency=United Press International}}</ref>  (Admirals [[William D. Leahy]], [[Chester W. Nimitz]], [[Hyman G. Rickover]] and [[Charles Stewart (1778–1869)|Charles Stewart]] were the only other officers in the Navy's history to serve on active duty at a higher age.  Leahy and Nimitz served on active duty for life due to their promotions to the rank of [[Fleet Admiral (United States)|fleet admiral]].)\n\n==Post retirement==\nFollowing her retirement from the Navy, she was hired as a senior consultant to [[Digital Equipment Corporation]] (DEC). Hopper was initially offered the job position by Rita Yavinsky, but she insisted on applying for the position at [[Digital Equipment Corporation]], and going through the typical formal interview process. She also sent a letter to Yavinsky's boss explaining that she would be available on alternating Thursdays, receiving a high salary, and have access to an unlimited expense account if she were to be exhibited at their museum of computing as a pioneer. After the proposal from Hopper, she was hired as a full-time senior consultant. As part of her position, she would report to Yavinsky. In this position, Hopper represented the company at industry forums, serving on various industry committees, along with other obligations.<ref>{{Cite book|title=Grace Hopper : admiral of the cyber sea|last=Williams|first=Kathleen|publisher=Naval Institute Press|year=2004|isbn=|location=Annapolis, Md|pages=}}</ref> She retained that position until her death at age 85 in 1992.\n\nHopper was a goodwill ambassador in her primary activity in this capacity. She lectured widely about the early days of computing, her career, and on efforts that computer vendors could take to make life easier for their users. She visited most of Digital's engineering facilities, where she generally received a standing ovation at the conclusion of her remarks.\n\nShe often recounted that during her service she was frequently asked by admirals and generals why satellite communication would take so long.  So during many of her lectures, she illustrated a nanosecond using salvaged obsolete Bell System 25 pair telephone cable, cut it to 11.8&nbsp; inch (30&nbsp; cm) lengths, [[Light-nanosecond|the distance that light travels in one nanosecond]], and handed out the individual wires to her listeners. Although no longer a serving officer, she always wore her Navy full dress uniform to these lectures, which is allowed by US Navy uniform regulations.\n\n{{quote|The most important thing I've accomplished, other than building the compiler, is training young people. They come to me, you know, and say, 'Do you think we can do this?' I say, \"Try it.\" And I back 'em up. They need that. I keep track of them as they get older and I stir 'em up at intervals so they don't forget to take chances.<ref>{{Cite book |last=Gilbert |first=Lynn |title=Particular Passions: Grace Murray Hopper |series=Women of Wisdom Series |edition=1st |date=December 10, 2012 |publisher=Lynn Gilbert Inc. |location=New York City |isbn=978-1-61979-403-0}}</ref>}}\n\n==Anecdotes==\n[[File:H96566k.jpg |thumb|Photo of \"first [[software bug|computer bug]]\" (a moth)]]\n* Throughout much of her later career, Hopper was much in demand as a speaker at various computer-related events. She was well known for her lively and irreverent speaking style, as well as a rich treasury of early war stories. She also received the nickname \"Grandma COBOL\".\n* While she was working on a [[Harvard Mark II|Mark II]] Computer at a US Navy research lab in Dahlgren, Virginia in 1947, her associates discovered a [[moth]] that was stuck in a [[relay]]; the moth impeded the operation of the relay. While neither Hopper nor her crew mentioned the phrase \"[[debugging]]\" in their logs, the case was held as an instance of literal \"debugging.\" For many years, the term ''[[computer bug|bug]]'' had been in use in engineering.<ref>Edison to Puskas, November 13, 1878, Edison papers, Edison National Laboratory, U.S. National Park Service, West Orange, N.J., cited in Thomas P. Hughes, ''American Genesis: A History of the American Genius for Invention,'' Penguin Books, 1989, {{ISBN|0-14-009741-4}}, on page 75.</ref><ref>{{cite web\n |url=http://theinstitute.ieee.org/technology-focus/technology-history/did-you-know-edison-coined-the-term-bug \n |title=Did You Know? Edison Coined the Term \"Bug\" \n |author=Alexander Magoun and Paul Israel \n |date=August 23, 2013 \n |accessdate=August 27, 2013 \n |work=IEEE: The Institute \n |archiveurl=https://web.archive.org/web/20160304130915/http://theinstitute.ieee.org/technology-focus/technology-history/did-you-know-edison-coined-the-term-bug \n |archivedate=March 4, 2016 \n |deadurl=yes |df=mdy-all \n}}</ref> The remains of the moth can be found in the group's log book at the [[Smithsonian Institution]]'s [[National Museum of American History]] in [[Washington, D.C.]]<ref>{{cite web |url= http://americanhistory.si.edu/collections/search/object/nmah_334663|title=Log Book With Computer Bug|work=[[National Museum of American History]]|accessdate=May 7, 2014}}</ref>\n* Grace Hopper is famous for her ''nanoseconds'' visual aid. People (such as generals and admirals) used to ask her why [[satellite]] communication took so long.  She started handing out pieces of wire that were just under one foot long (11.80 inches)—the distance that light travels in one [[nanosecond]]. She gave these pieces of wire the [[Metonymy|metonym]] \"nanoseconds.\"<ref name=\"DavidLetterman86\">{{Cite episode | title = Late Night with David Letterman | series = Late Night with David Letterman| serieslink = Late Night with David Letterman| network = [[NBC]]| location = New York City| airdate = October 2, 1986| season = 5| number = 771|quote=\"[to President Ronald Reagan on her promotion] Sir ... I'm older than you are ... YouTube title: Grace Hopper on Letterman}}</ref>  She was careful to tell her audience that the length of her nanoseconds was actually the maximum speed the signals would travel in a vacuum, and that signals would travel more slowly through the actual wires that were her teaching aids. Later she used the same pieces of wire to illustrate why computers had to be small to be fast. At many of her talks and visits, she handed out \"nanoseconds\" to everyone in the audience, contrasting them with a coil of wire 984 feet long<ref>{{cite web|url=https://www.youtube.com/watch?v=JEpsKnWZrJ8|title=Nanoseconds lecture by Grace Hopper}}</ref>, representing a [[microsecond]]. Later, while giving these lectures while working for DEC, she passed out packets of pepper, calling the individual grains of ground pepper [[picosecond]]s.<ref>{{cite magazine  |magazine=InformationWeek  |date=January 6, 1992 |page=4   |title=Good-Bye and Good Wishes}}</ref>\n\n* Jay Elliot described Grace Hopper as appearing to be \"'all Navy', but when you reach inside, you find a 'Pirate' dying to be released\".<ref>{{cite book|first1 = Jay|last1 = Elliott|first2 = William L.|last2 = Simon|year = 2011|title = The Steve Jobs way: iLeadership for a new generation|place = Philadelphia|publisher = Vanguard|page = 71|isbn = 978-1-59315-639-8}}</ref>\n\n==Death==\nOn New Year's Day 1992, Hopper died in her sleep of natural causes at her home in Arlington, Virginia; she was 85 years of age. She was interred with full military honors in [[Arlington National Cemetery]].<ref>{{Find a Grave|1784|RADM Grace Brewster ''Murray'' Hopper}}</ref>\n\n==Dates of rank==\n{{unreferenced|section|date=December 2017}}\n*Ensign - December 1943\n*Lieutenant (junior grade) - June 27, 1944\n*Lieutenant - January 1, 1946\n*Lieutenant Commander - April 1, 1952\n*Commander - July 1, 1957\n*Retired - December 31, 1966\n*Recalled to active duty - August 1967\n*Retired - 1971\n*Recalled to active duty - 1972\n*Captain - August 2, 1973\n*Commodore - December 15, 1983\n*Rear Admiral (Lower Half) - November 8, 1985\n*Final retirement - August 31, 1986\n\n==Awards and honors==\n\n===Military awards===\n<center>\n{|\n|colspan=\"2\" align=\"left\"|{{ribbon devices|number=0|type=oak|ribbon=Defense Distinguished Service Medal ribbon.svg{{!}}border|width=106}}\n|colspan=\"2\" align=\"center\"|{{ribbon devices|ribbon=Legion of Merit ribbon.svg{{!}}border|width=106}}\n|colspan=\"2\" align=\"left\"|{{ribbon devices|number=0|type=oak|ribbon=Meritorious Service Medal ribbon.svg{{!}}border|width=106}}\n|-\n|colspan=\"2\" align=\"left\"|{{ribbon devices|number=0|type=oak|ribbon=Presidential Medal of Freedom (ribbon).png{{!}}border|width=106}} \n|colspan=\"2\"|<center>{{ribbon devices|ribbon=American Campaign Medal ribbon.svg{{!}}border|width=106}}</center>\n|colspan=\"2\"|<center>{{ribbon devices|ribbon=World War II Victory Medal ribbon.svg{{!}}border|width=106}}</center>\n|-\n|colspan=\"2\"|<center>{{ribbon devices|number=1|type=service-star|ribbon=National Defense Service Medal ribbon.svg{{!}}border|width=106}}</center>\n|colspan=\"2\"|<center>{{ribbon devices|ribbon=Armed_Forces_Reserve_Medal_with_two_bronze_hourglass_devices.png{{!}}border|width=106}}</center>\n|colspan=\"2\"|<center>{{ribbon devices|ribbon=U.S. Naval Reserve Medal ribbon.svg{{!}}border|width=106}}</center>\n|}\n{| class=\"wikitable\"\n!Top Row\n|colspan=\"2\" align=\"center\"|<center>[[Defense Distinguished Service Medal]]<br>(1986)</center>\n|colspan=\"2\" align=\"center\"|<center>[[Legion of Merit]]<br>(1967)</center>\n|colspan=\"2\" align=\"center\"|<center>[[Meritorious Service Medal (United States)|Meritorious Service Medal]]<br>(1980)</center>\n|-\n!2nd Row\n|colspan=\"2\" align=\"center\"|<center>[[Presidential Medal of Freedom]]<br>(2016, Posthumous)</center>\n|colspan=\"2\" align=\"center\"|<center>[[American Campaign Medal]]<br>(1944)</center>\n|colspan=\"2\" align=\"center\"|<center>[[World War II Victory Medal]]<br>(1945)</center>\n|-\n!Bottom Row\n|colspan=\"2\" align=\"center\"|<center>[[National Defense Service Medal]] <br>with bronze service star<br>(1953, 1966)</center> \n|colspan=\"2\" align=\"center\"|<center>[[Armed Forces Reserve Medal]]<br>with two bronze hourglass devices<br>(1963, 1973, 1983)</center>\n|colspan=\"2\" align=\"center\"|<center>[[Naval Reserve Medal]]<br>(1953)</center>\n|}\n</center>\n\n===Other awards===\n* 1964: Hopper was awarded the [[Society of Women Engineers]] Achievement Award, the Society’s highest honor, “In recognition of her significant contributions to the burgeoning computer industry as an engineering manager and originator of automatic programming systems.”.<ref>{{cite web|url=http://philadelphia.swe.org/first-ladies.html|title=First Ladies}}</ref>\n* 1969: Hopper was awarded the inaugural [[Association of Information Technology Professionals|Data Processing Management Association]] Man of the Year award (now called the Distinguished Information Sciences Award).<ref>{{cite web|url=http://www.aitp.org/?DISA|title=DISA Recipients - Association of Information Technology Professionals|accessdate=June 28, 2016}}</ref>\n* 1971: The annual [[Grace Murray Hopper Award|Grace Murray Hopper Award for Outstanding Young Computer Professionals]] was established in 1971 by the [[Association for Computing Machinery]].\n* 1973: First American and the first woman of any nationality to be made a [[DFBCS|Distinguished Fellow of the British Computer Society]].\n* 1982: [[American Association of University Women]] Achievement Award and an Honorary Doctor of Science from [[Marquette University]].<ref>{{cite web|url=http://www.marquette.edu/universityhonors/honorary_degrees_recipients_year.shtml |title=Honorary Degrees {{!}} University Honors {{!}} Marquette University |publisher=Marquette.edu |accessdate=August 19, 2014}}</ref>\n* 1985: Honorary Doctor of Letters from [[Western New England College]] (now [[Western New England University]]).<ref>{{cite web|last=Lee|first=J.A.N.|url=http://history.computer.org/pioneers/hopper.html |title=Computer Pioneers — Grace Brewster Murray Hopper|publisher=IEEE Computer Society and the Institute of Electrical and Electronics Engineers |accessdate=April 29, 2017}}</ref><ref>{{cite web|url=http://www1.wne.edu/assets/10/WNE_History.pdf|title=Western New England: From College to University A Retrospective: 1919-2011 |publisher=Western New England University |accessdate=May 21, 2014}}</ref>\n* 1986: Upon her retirement, she received the Defense Distinguished Service Medal.\n* 1987: The first [[Computer History Museum]] Fellow Award Recipient \"for contributions to the development of programming languages, for standardization efforts, and for lifelong naval service.\"<ref>{{cite web|url=http://www.computerhistory.org/fellowawards/hall/bios/Grace,Hopper/ |title=Grace Hopper - Computer History Museum Fellow Award Recipient |publisher=Computerhistory.org |accessdate=March 30, 2015}}</ref>\n* 1988: Golden Gavel Award at the [[Toastmasters International]] convention in Washington, DC.\n* 1991: [[National Medal of Technology]].\n* 1991: Elected a Fellow of the [[American Academy of Arts and Sciences]].<ref name=AAAS>{{cite web|title=Book of Members, 1780–2010: Chapter H|url=http://www.amacad.org/publications/BookofMembers/ChapterH.pdf|publisher=American Academy of Arts and Sciences|accessdate=July 22, 2014}}</ref>\n* 1996: {{USS|Hopper|DDG-70}} was launched. Nicknamed ''Amazing Grace'', it is on a very short [[list of U.S. military vessels named after women]].\n* 2001: [[Eavan Boland]] wrote a poem dedicated to Grace Hopper titled \"Code\" in her 2001 release ''Against Love Poetry''.\n* 2001: The Gracies, the Government Technology Leadership Award were named in her honor.<ref>{{cite web|title=The 2002 Government Technology Leadership Awards|url=http://www.govexec.com/technology/2002/04/the-2002-government-technology-leadership-awards/7622/|publisher=Government Executive|accessdate=May 20, 2014|date=April 1, 2002}}</ref>\n* 2009: The Department of Energy's [[National Energy Research Scientific Computing Center]] named its flagship system \"Hopper\".<ref>{{cite web |url=http://www.nersc.gov/nusers/systems/hopper/ |title=Hopper Home Page |publisher=nersc.gov}}</ref>\n* 2009: [[Office of Naval Intelligence]] creates the Grace Hopper Information Services Center.<ref>{{citation|title=Naval Intelligence Ramps up Activities | date=February 2009 | author=Robert K. Ackerman | journal=Signals | publisher=[[AFCEA]] | url=http://www.afcea.org/content/?q=node/1831}}</ref>\n* 2013: Google made the [[Google Doodle]] for Hopper's 107th birthday an animation of her sitting at a computer, using COBOL to print out her age. At the end of the animation, a moth flies out of the computer.<ref name=\"Google Doodle\">{{cite web |url= https://www.google.com/doodles/grace-hoppers-107th-birthday|title=Grace Hopper's 107th Birthday |publisher=[[Google]] |accessdate= December 9, 2013}}</ref><ref>{{cite news |url= http://www.telegraph.co.uk/technology/google/google-doodle/10505145/Grace-Hopper-honoured-with-Google-doodle.html |title=Grace Hopper honoured with Google doodle |author=Matthew Sparkes |work=The Daily Telegraph |location= London |date=December 9, 2013 |accessdate=December 9, 2013}}</ref>\n* 2016: On November 22, 2016 Hopper was posthumously awarded a [[Presidential Medal of Freedom]] for her accomplishments in the field of computer science.<ref>{{Cite web|url=http://www.npr.org/sections/thetwo-way/2016/11/16/502347068/these-are-the-21-people-receiving-the-nations-highest-civilian-honor|title=These Are The 21 People Receiving The Nation's Highest Civilian Honor|last=|first=|date=November 16, 2016|website=npr.org|publisher=|access-date=November 16, 2016}}</ref>\n* 2017: [[Hopper College]] at [[Yale University]] was named in her honor.<ref>{{Cite web|url=https://www.nytimes.com/2017/09/03/nyregion/yale-calhoun-college-grace-hopper.html|title=Calhoun Who? Yale Drops Name of Slavery Advocate for Computer Pioneer|work=N.Y. Times|access-date=September 3, 2017|date=September 3, 2017}}</ref>\n\n==Legacy==\n* Grace Hopper was awarded 40 honorary degrees from universities worldwide during her lifetime.<ref>{{cite web |url= http://web.mit.edu/invent/iow/hopper.html |title= Inventor of the Week: Archive |publisher= Web.mit.edu |accessdate= December 9, 2013}}</ref><ref>{{cite web |url= http://www-history.mcs.st-and.ac.uk/Biographies/Hopper.html |title= Hopper biography |publisher= History.mcs.st-and.ac.uk |accessdate= December 9, 2013}}</ref><ref>{{cite web |url= http://www.history.navy.mil/bios/hopper_grace.htm#honors |title= Biography&nbsp;– Rear Admiral Grace Murray Hopper, USN |publisher= United States Navy |accessdate= December 9, 2013}}</ref>\n* ''Born with Curiosity: The Grace Hopper Story'' is an upcoming [[documentary film]].<ref>{{IMDb title|3545258|Born with Curiosity: The Grace Hopper Story}}</ref>\n\n===Places===\n* The Navy's [[Fleet Numerical Meteorology and Oceanography Center]] is located at 7 Grace Hopper Avenue in [[Monterey, California]]; the [[National Weather Service]]'s San Francisco / Monterey Bay Area Hydrology / Geomorphology office is at 21 Grace Hopper Avenue.\n*[[Grace M. Hopper Navy Regional Data Automation Center]] at [[Naval Air Station, North Island]], California.\n* ''[[Grace Murray Hopper Park]]'', located on South Joyce Street in [[Arlington, Virginia]], is a small memorial park in front of her former residence (River House Apartments) and is now owned by [[Arlington County, Virginia]].\n* [[Brewster Academy]], a school located in [[Wolfeboro, New Hampshire]], United States, dedicated their computer lab to her in 1985, calling it the Grace Murray Hopper Center for Computer Learning.<ref name=navybio/> The academy bestows a Grace Murray Hopper Prize to a graduate who excelled in the field of computer systems.<ref>{{cite web|url=http://www.brewsteracademy.org/customized/uploads/documents/Summer2007CorrectedWithCovers.pdf|title=Brewster Connections: Summer 2007}}</ref> Hopper had spent her childhood summers at a family home in Wolfeboro.\n* An administration building on Naval Support Activity Annapolis (previously known as Naval Station Annapolis) in Annapolis, Maryland is named the Grace Hopper Building in her honor.<ref name=navybio/>\n* [[Walter E. Carter Jr.|Vice Admiral Walter E. \"Ted\" Carter]] announced on 8 September 2016 at the Athena Conference that the [[United States Naval Academy|Naval Academy]]'s newest Cyber Operations building would be named Hopper Hall after Admiral Grace Hopper. This is the first building at any service academy named after a woman. In his words, \"Grace Hopper was the admiral of the Cyber Seas.\" \n* The US Naval Academy also owns a Cray XC-30 supercomputer named \"Grace,\" hosted at the University of Maryland-College Park.<ref>{{cite web|url=https://www.hpc.mil/index.php/2013-08-29-16-06-21/press-releases/us-naval-academy-dedicates-new-supercomputer|title=US Naval Academy Dedicates New Supercomputer}}</ref>\n* Building 1482 aboard [[Naval Air Station North Island]], housing the Naval Computer and Telecommunication Station San Diego, is named the Grace Hopper Building.\n* Building 6007, C2/CNT West, Command, control, communications, computers, intelligence, surveillance, and reconnaissance, or [[C4ISTAR|C4ISR]], Center of Excellence in [[Aberdeen Proving Ground]], Maryland is named the Rear Admiral Grace Hopper Building.\n* Grace Hopper Academy is a for-profit immersive programming school in New York City named in Grace Hopper's honor. It opened in January 2016 with the goal of increasing the proportion of women in software engineering careers.<ref>{{Cite web |title= Grace Hopper Academy |url= http://gracehopper.com/ |website= gracehopper.com |accessdate= 2015-10-15}}</ref><ref>{{Cite web |title= Exclusive: Grace Hopper Academy, An All-Women Coding School, To Open in New York |url= http://www.ibtimes.com/exclusive-grace-hopper-academy-all-women-coding-school-open-new-york-2141588 |website= International Business Times |accessdate= 2015-10-15}}</ref>\n* A bridge over Goose Creek, to join the north and south sides of the [[Naval Support Activity Charleston]] side of [[Joint Base Charleston]], [[South Carolina]], is named the Grace Hopper Memorial Bridge in her honor.<ref>{{cite web |url= http://www.charleston.af.mil/news/story.asp?id=123293768 |title= Women's History Month: Beyond the bridge: Story of 'Amazing Grace' Hopper |first1=Tom |last1=Brading |date= March 13, 2012 |accessdate= February 12, 2013}}</ref>\n\n===Programs===\n* Women at [[Microsoft Corporation]] formed an employee group called Hoppers and established a scholarship in her honor.  Hoppers has over 3000 members worldwide.\n* Beginning in 2015, one of the nine competition fields at the [[FIRST Robotics Competition]] world championship is named for Hopper.<ref>{{Cite web |title= New Subdivision Names |work= First Robotics Corporation |accessdate= 2016-03-16 |date= 2015-02-09 |url= http://www.firstinspires.org/node/7951}}</ref>\n* On February 11, 2017 [[Yale University]] announced its intent to rename [[Calhoun College]], one of its twelve undergraduate residential colleges, after Hopper following years of controversy about its previous namesake [[John C. Calhoun]]. Hopper was a graduate of Yale University, receiving an M.A. in 1930 and a Ph.D in 1934.\n* A named professorship in the Department of Computer Sciences was established at Yale University in her honor. [[Joan Feigenbaum]] was named to this chair in 2008.<ref>Yale News, July 18, 2008</ref>\n\n====Grace Hopper Celebration of Women in Computing====\n'''[[Grace Hopper Celebration of Women in Computing]]''' is a convention for Women in the field of Computer Science and Technology. It is named after Hopper to honor her for her work and influence in the field of computing, and her push for more women to enter and stay in the tech field. It features a wide array of educational and professional development courses and workshops, including a lesson on compilers, which Hopper invented and pioneered, and a career fair, in order to help connect women in the computing field with potential employers. \n\nHer legacy was an inspiring factor in the creation of the [[Grace Hopper Celebration of Women in Computing]].<ref>{{cite web|url=http://www.gracehopper.org/ |title=Grace Hopper Celebration of Women in Computing |publisher=Gracehopper.org |accessdate=December 9, 2013}}</ref> Held yearly, this conference is designed to bring the research and career interests of women in computing to the forefront.<ref>{{cite newspaper  |newspaper=The New York Times  |date=October 31, 2016\n|url=https://open.blogs.nytimes.com/2016/10/31/we-went-to-the-grace-hopper-celebration-heres-what-were-bringing-back\n|title=We Went to the Grace Hopper Celebration. Here’s What We’re Bringing Back}}</ref>\n\n==Obituary notices==\n* Betts, Mitch (''[[Computerworld]]'' 26: 14, 1992)\n* Bromberg, Howard (''[[IEEE Software]]'' 9: 103–104, 1992)\n* Danca, Richard A. (''Federal Computer Week'' 6: 26–27, 1992)\n* Hancock, Bill (''Digital Review'' 9: 40, 1992)\n* Power, Kevin (''Government Computer News'' 11: 70, 1992)\n* [[Jean E. Sammet|Sammet, J. E.]]  (''[[Communications of the ACM]]'' 35 (4): 128–131, 1992)\n* Weiss, Eric A. (''[[IEEE Annals of the History of Computing]]'' 14: 56–58, 1992)\n\n==See also==\n{{Portal|United States Navy|Software Testing|Biography}}\n* ''[[Code: Debugging the Gender Gap]]''\n* [[Grace Hopper Celebration of Women in Computing]]\n* [[List of pioneers in computer science]]\n* [[Systems engineering]]\n* [[Women in computing]]\n* [[Women in the United States Navy]]\n* [[List of female United States military generals and flag officers]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{Cite book |last=Beyer |first=Kurt W. |title=Grace Hopper and the Invention of the Information Age |edition=1st |date=September 30, 2009 |publisher=MIT Press |location=Cambridge, Massachusetts |isbn=978-0-262-01310-9}}\n* {{Cite book |last=Marx |first=Christy |authorlink=Christy Marx |title=Grace Hopper: the first woman to program the first computer in the United States | edition=1st |series=Women hall of famers in mathematics and science | date=August 2003 |publisher=Rosen Publishing Group |location=New York City |isbn=978-0-8239-3877-3}}\n* {{cite web |url=http://www.agnesscott.edu/lriddle/women/hopper.htm |title=Biographies of Women Mathematicians: Grace Murray Hopper |last=Norman |first=Rebecca |publisher=[[Agnes Scott College]] |date=June 1997 |accessdate=2014-11-17}}\n* {{Cite book |last=Williams |first=Kathleen Broome |title=Grace Hopper: Admiral of the Cyber Sea |edition=1st |date=November 15, 2004 |publisher=Naval Institute Press |location=Annapolis, Maryland |isbn=978-1-55750-952-9}}\n* {{Cite book |last=Williams |first=Kathleen Broome |title=Improbable Warriors: Women Scientists and the U.S. Navy in World War II |accessdate= |edition= |year=2001 |publisher=Naval Institute Press |location=Annapolis, Maryland |isbn=978-1-55750-961-1}} Williams' book focuses on the lives and contributions of four notable women scientists: [[Mary Sears (oceanographer)|Mary Sears]] (1905–1997); [[Florence van Straten]] (1913–1992); Grace Murray Hopper (1906–1992); [[Mina Spiegel Rees]] (1902–1997).\n\n==External links==\n{{Commons category}}\n{{Wikiquote}}\n* [https://web.archive.org/web/20171225202555/http://archive.computerhistory.org/resources/text/Oral_History/Hopper_Grace/102702026.05.01.pdf Oral History of Captain Grace Hopper - Interviewed by: Angeline Pantages] 1980, Naval Data Automation Command, Maryland.\n* {{webarchive |url=https://web.archive.org/web/20100224101438/http://www.chips.navy.mil/links/grace_hopper/womn.htm |date=February 24, 2010 |title=RADM Grace Hopper, USN Ret. }} from ''Chips'', the United States Navy [[information technology]] magazine.\n* [http://usnhistory.navylive.dodlive.mil/2014/12/09/grace-hopper-navy-to-the-core-a-pirate-at-heart/ ''Grace Hopper: Navy to the Core, a Pirate at Heart''] (2014), To learn more about Hopper's story and Navy legacy [[navy.mil]].\n* [http://fivethirtyeight.com/features/the-queen-of-code/ ''The Queen of Code''] (2015), a documentary film about Grace Hopper produced by [[FiveThirtyEight]].\n* Norwood, Arlisha. [https://www.nwhm.org/education-resources/biographies/grace-hopper \"Grace Hopper\"]. National Women's History Museum. 2017.\n\n{{Navboxes\n|title = Articles related to Grace Hopper\n|list  =\n{{Timelines of computing}}\n{{Software engineering}}\n{{National Women's Hall of Fame}}\n{{Virginia Women in History}}\n}}\n{{Authority control}}\n\n{{DEFAULTSORT:Hopper, Grace}}\n[[Category:American computer programmers]]\n[[Category:American computer scientists]]\n[[Category:1906 births]]\n[[Category:1992 deaths]]\n[[Category:American women scientists]]\n[[Category:COBOL]]\n[[Category:Programming language designers]]\n[[Category:Women computer scientists]]\n[[Category:Women inventors]]\n[[Category:Women mathematicians]]\n[[Category:Women in technology]]\n[[Category:American military personnel of World War II]]\n[[Category:American women in World War II]]\n[[Category:United States Navy rear admirals (lower half)]]\n[[Category:Female admirals of the United States Navy]]\n[[Category:Fellows of the American Academy of Arts and Sciences]]\n[[Category:Fellows of the British Computer Society]]\n[[Category:National Medal of Technology recipients]]\n[[Category:Recipients of the Defense Distinguished Service Medal]]\n[[Category:Recipients of the Legion of Merit]]\n[[Category:Harvard University people]]\n[[Category:Vassar College faculty]]\n[[Category:Military personnel from New York City]]\n[[Category:Vassar College alumni]]\n[[Category:Yale University alumni]]\n[[Category:American people of Dutch descent]]\n[[Category:American people of Scottish descent]]\n[[Category:Burials at Arlington National Cemetery]]\n[[Category:20th-century American engineers]]\n[[Category:20th-century American mathematicians]]\n[[Category:20th-century American scientists]]\n[[Category:20th-century women scientists]]\n[[Category:Presidential Medal of Freedom recipients]]\n[[Category:Computer science educators]]\n[[Category:Phi Beta Kappa]]",
            "slug": "grace-hopper",
            "date_updated": 1517191848447,
            "imported": "https://en.wikipedia.org/wiki/Grace_Hopper"
        },
        {
            "title": "Home",
            "text": "{{ambox|type=discussion|text='''Welcome'''|text-small=Welcome to the English hub of [[Kiwipedia]]! If you have any suggestions, please add them to [[Development ideas]].}}\n\nTest",
            "slug": "home",
            "date_updated": 1517191917459,
            "imported": ""
        },
        {
            "title": "Home",
            "text": "{{ambox|type=discussion|text='''Welcome'''|text-small=Welcome to the English hub of [[Kiwipedia]]! If you have any suggestions, please add them to [[Development ideas]].}}",
            "slug": "home",
            "date_updated": 1517191921965,
            "imported": ""
        }
    ]
}