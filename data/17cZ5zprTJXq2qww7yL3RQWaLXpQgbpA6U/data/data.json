{
	"title": "贪吃的猫",
	"description": "随便玩玩",
	"links": "[一些有用的东西](http://127.0.0.1:43110/17cZ5zprTJXq2qww7yL3RQWaLXpQgbpA6U/?Post:6:%E4%B8%80%E4%BA%9B%E6%9C%89%E7%94%A8%E7%9A%84%E4%B8%9C%E8%A5%BF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89)\n***\n\n胜人者力，自胜者强。\n知人者智，自知者明。\n***\n\n#友情链接\n<a href=\"http://127.0.0.1:43110/cxg2014.bit/\" target=\"_blank\">CXG2014</a>\n<a href=\"http://127.0.0.1:43110/Blog.ZeroNetwork.bit\" target=\"_blank\">ZeroBlog</a>\n<a href=\"http://127.0.0.1:43110/gfwtalk.bit\" target=\"_blank\">GFW Talk</a>\n<a href=\"http://127.0.0.1:43110/0net123.bit/\" target=\"_blank\">🎈Net123 中文导航</a>\n<a href=\"http://127.0.0.1:43110/mosen.bit/\" target=\"_blank\">浮生網志</a>\n\n- [Source code](https://github.com/HelloZeroNet)",
	"next_post_id": 29,
	"demo": false,
	"modified": 1472745217,
	"post": [
		{
			"post_id": 28,
			"title": "python学习小记",
			"date_published": 1472744824.908,
			"body": " 我对Python感兴趣的原因是想要用Python来写爬虫，用来爬一些数据（最主要的是sfacg的图书元数据 **它的搜索太让人抓狂了！！！**）。\n \n 然后我就开始学Python\n \n\n先看的是[Python 2.7教程 - 廖雪峰的官方网站](http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000)，看了大半感觉掌握了一些**Python知识**，可以写一些简单的东西了。然后开始看**爬虫方面**的知识，看的是[Python爬虫学习系列教程](http://cuiqingcai.com/1052.html)，按照上面的一篇篇的学，基本上刷了一遍。\n\n然后开始动手。\n\n写了一些小脚本用来爬数据**（`book.sfacg.com`）**，开始的时候从目录页爬，然后发现结果不全，而且封面也都是一些小图**（要的就是封面）**，这可不行。\n\n然后准备上框架，然后开始学**Scrapy框架**，没有在网上发现好的教程，主要是自己看它的[官方文件](https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html)，边看边写，遇到问题了就上网搜。后来发现中文翻译的已经有些落后了，就开始看[英文的官方文本](http://doc.scrapy.org/en/latest/intro/overview.html)，发现基本和用的软件版本对的上，没有什么问题。\n\n后来基本上写完了，bug也不完了后，然后就觉得自己的任务终于完了，一种空虚感。\n\n后来有写了一个爬**`iqing.in`**的爬虫，这个是爬全文的。\n\n当然，练习的时候也写了一些小作品练了练手。\n\n现在学业繁忙，准备放下这些杂业，所以今天写文总结一下。\n\n写的几个成熟的作品也发出来让大家看看。**（新手，不要见笑啊！）**\n\n#PS:一些小作品\n##目录：\n- 跟踪路由并现实ip归属地的小脚本\n- 代理爬虫\n- sfacg 爬虫\n- iqing.in 爬虫\n\n---\n##跟踪路由并现实ip归属地的小脚本\n```\n#!/usr/bin/env python2\n# -*- coding:utf-8 -*-\n\n__author__='YYW'\n\nimport requests\nimport re\nimport sys\nimport commands\n\nclass IPlocation(object):\n    def __init__(self):\n        self.patten = re.compile(r'(((2[0-4]\\d|25[0-5]|[01]?\\d\\d?)\\.){3}(2[0-4]\\d|25[0-5]|[01]?\\d\\d?))')\n        self.ips = []\n        self.webstatus = 403\n        self.trace = ''\n        self.input = sys.argv\n\n    def tracerouter(self, input):\n       if len(input) == 2:\n           t = commands.getoutput('traceroute -n ' +str(input[1]))\n           self.trace = t\n       else:\n           print('请输入IP或域名')\n           exit()\n\n    def dealdata(self):#处理输出ips\n        o = self.trace\n        ips = re.findall(self.patten, o)\n        self.ips = ips\n        return None\n    \n    def query_ip(self, ip):#利用ipip.net api 查询地址\n        payload = {'ip': ip}\n        r = requests.get(\"http://freeapi.ipip.net/\", params=payload)\n        self.webstatus = r.status_code\n        return r.text\n    \n    def netest(self):#网络检测\n        temp = self.query_ip('8.8.8.8')\n        nettest = re.search(\"202.204.32.54\", temp)\n        if nettest:\n            print('未连接校园网,请检查网络')\n            exit()\n    \n    def start(self):\n        self.netest()\n        self.tracerouter(self.input)\n        print(self.trace)\n        self.dealdata()\n        i = -1\n        if self.input[1][:1] in [str(x) for x in range(10)]:\n            m = 1\n        else:\n            m = 0\n        print('\\n路由跟踪:')\n        for ipt in self.ips[m:]:\n            i = i + 1\n            ip = ipt[0]\n            ipq = self.query_ip(ip)\n            while self.webstatus != 200:\n                ipq = self.query_ip(ip)\n            print('%d\\t%s\\t%s' % (i, ipt[0], ipq))  \n\ns = IPlocation()\ns.start()\n```\n***\n##代理爬虫\n###items.py\n```\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass HttpProxiesItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    proxie_ip = scrapy.Field()\n    #proxie_type = scrapy.Field()\n    proxie_location = scrapy.Field()\n```\n###spider *(proxies_spiders.py)*\n```\n# -*- coding: utf-8 -*-\n\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom http_proxies.items import HttpProxiesItem\nfrom scrapy.linkextractors import LinkExtractor\nimport re\n\nclass Http_proxie_spider(CrawlSpider):\n    name = 'http_proxies'\n    allowed_domains = ['youdaili.net']\n    start_urls = ['http://www.youdaili.net/Daili/http/']\n    rules = (\n        Rule(LinkExtractor(allow=('/Daili/http/list_\\d+.html'))), \n        Rule(LinkExtractor(allow=('/Daili/http/\\d+.html')), callback='parse_item')\n    )\n    \n    def parse_item(self, response):\n        self.log('Hi, this is an item page! %s' % response.url)\n        \n        patten = re.compile(r'(((2[0-4]\\d|25[0-5]|[01]?\\d\\d?)\\.){3}(2[0-4]\\d|25[0-5]|[01]?\\d\\d?):\\d+)@(\\w+)#(.*?)<')\n        t = re.findall(patten, response.text)\n        proxie_ips = [t[x][0] for x in range(len(t))]\n        proxie_types = [t[x][-2] for x in range(len(t))]\n        proxie_locations = [t[x][-1] for x in range(len(t))]\n        item = HttpProxiesItem()\n        for i in range(len(proxie_ips)):\n            item['proxie_ip'] = proxie_ips[i]\n            #item['proxie_type'] = proxie_types[i]\n            item['proxie_location'] = proxie_locations[i]\n            yield item\n```\n***\n##sfacg爬虫\n###items.py\n```\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass SfacgItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    bookid = scrapy.Field()\n    bookname = scrapy.Field()\n    author = scrapy.Field()\n    word_number = scrapy.Field()\n    view_time = scrapy.Field()\n    sum_point = scrapy.Field()  #总评分\n    point = scrapy.Field()  #评分具体\n    bookMarknum = scrapy.Field() #类型\n    favSticks = scrapy.Field()  #赞\n    pointUserCount = scrapy.Field() #评分人数\n    type = scrapy.Field()   #类型\n    update_time = scrapy.Field()\n    brief_introduction = scrapy.Field()\n    cover = scrapy.Field()\n    cover_time = scrapy.Field()\n    cover_status = scrapy.Field()\n    cover_path = scrapy.Field()\n    MonTicketNum = scrapy.Field()\n    Tags = scrapy.Field() #<type 'list'>,<type 'dict'>\n    is_VIP = scrapy.Field()\n    VIP_time = scrapy.Field()\n    hav_beitou = scrapy.Field()\n    left_beitou = scrapy.Field()\n    left_beitou_time = scrapy.Field()\n    left_beitou_status = scrapy.Field()\n    left_beitou_path = scrapy.Field()\n    right_beitou = scrapy.Field()\n    right_beitou_time = scrapy.Field()\n    right_beitou_status = scrapy.Field()\n    right_beitou_path = scrapy.Field()\n    status = scrapy.Field() #状态\n    sumNumbook = scrapy.Field()#共有X本书\n    max_page = scrapy.Field()#共有X页\n    brief_commen_num = scrapy.Field() #短评数\n    long_commen_num = scrapy.Field() #长评数\n    text_time = scrapy.Field()\n```\n###setting.py\n```\n# -*- coding: utf-8 -*-\n\n# Scrapy settings for sfacg project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     http://doc.scrapy.org/en/latest/topics/settings.html\n#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\n#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'sfacg'\n\nSPIDER_MODULES = ['sfacg.spiders']\nNEWSPIDER_MODULE = 'sfacg.spiders'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\nCONCURRENT_REQUESTS = 96\n#CONCURRENT_REQUESTS = 32\n\n# Configure a delay for requests for the same website (default: 0)\n# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n#DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n#COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\nTELNETCONSOLE_ENABLED = True\nTELNETCONSOLE_PORT = '6321'\n\n# Override the default request headers:\nDEFAULT_REQUEST_HEADERS = {\n   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n   'Accept-Language': 'zh',\n}\n\n# Enable or disable spider middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'sfacg.middlewares.MyCustomSpiderMiddleware': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\nDOWNLOADER_MIDDLEWARES = {\n    'sfacg.middlewares.Test': 543,\n}\n\n# Enable or disable extensions\n# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': 300,\n##    'scrapy.extensions.feedexport.FeedExporter': 500,\n#}\n\n# Configure item pipelines\n# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n#    'sfacg.pipelines.Pre_deal': 200,\n    'sfacg.pipelines.Save_image': 300,\n#    'sfacg.pipelines.JsonWriterPipeline': 400\n#    'sfacg.pipelines.Csv_Writer':400\n}\n#LOG_LEVEL = 'ERROR'\nLOG_LEVEL = 'INFO'\n#LOG_LEVEL = 'DEBUG'\nIMAGES_STORE = '/srv/xxx'\n#IMAGES_EXPIRES = 90\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See http://doc.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = 'httpcache'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n\n#Feed exports\nFEED_URI = 'file:///srv/xxx.json'\nFEED_FORMAT = 'json'\n#CSV_STORE = '/srv/xxx.csv'\n#JSON_STORE = '/srv/xxx.json'\n#FEED_URI = 'file:///srv/xxx.csv'\n#FEED_FORMAT = 'csv'\n```\n###piplines.py\n```\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\n#from scrapy.pipelines.images import ImagesPipeline\n#from scrapy.exceptions import DropItem\n#import json\nfrom sfacg import settings\nfrom hashlib import md5\nimport time, os,  requests\n\nclass Save_image(object):\n    def __init__(self):\n        self.dir_path = settings.IMAGES_STORE\n#        self.session = requests.Session()\n\n    def download_img(self, item, name):#下载图片\n        request_url = item[name]\n        checksun = md5(request_url.encode('utf-8')).hexdigest()\n        #创建session,修改包头\n        s = requests.Session()\n        headers={\n#        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:39.0) Gecko/20100101 Firefox/39.0',\n        'Referer': 'http://book.sfacg.com/Novel/%s/' % str(item['bookid'])\n        }\n        s.headers.update(headers)\n        #下载\n        try:\n            img = s.get(request_url, timeout = 5, stream=True)\n        except Exception:\n            try:\n                img = s.get(request_url, timeout = 5, stream=True)\n            except Exception:\n                item['%s_status' % name] = False\n                return item\n        finally:\n            if img.status_code == 200:\n                img_type= os.path.splitext(request_url)[-1]\n                path = '%s/%s/%s/%s%s' % (self.dir_path, name, checksun[:2], checksun, str(img_type))\n                t_c = img.content\n                #写入文件\n                with open(path, 'wb') as f:\n                    f.write(t_c)\n                #返回状态\n                now_time = time.ctime()\n                item['%s_status' % name] = True\n                item['%s_time' % name] = now_time\n                item['%s_path' % name] = checksun\n                return item\n            else:\n                item['%s_status' % name] = False\n                return item\n\n    def isexists(self, item, name):\n        request_url = item[name]\n        checksun = md5(request_url.encode('utf-8')).hexdigest()\n        img_type= os.path.splitext(request_url)[-1]\n        path = '%s/%s/%s/%s%s' % (self.dir_path, name, checksun[:2], checksun, str(img_type))\n        dir_path = '%s/%s/%s' % (self.dir_path, name, checksun[:2])\n        isExist_dir = os.path.exists(dir_path)\n        if not isExist_dir:\n            os.makedirs(dir_path)\n        isExists = os.path.exists(path)\n        if isExists:\n            item['%s_path' % name] = checksun\n            return [item, isExists]\n        else:\n            return [item, isExists]\n\n    def mkdir(self):\n        cover_path = '%s/%s' % (self.dir_path, 'cover')\n        left_beitou_path = '%s/%s' % (self.dir_path, 'left_beitou')\n        right_beitou_path = '%s/%s' % (self.dir_path, 'right_beitou')\n        if not os.path.exists(self.dir_path):\n            os.mkdir(self.dir_path)\n        for x in [cover_path, left_beitou_path, right_beitou_path]:\n            if not os.path.exists(x):\n                os.mkdir(x)\n        return\n\n    def process_item(self,item, spider):\n        self.mkdir()\n        if item.get('cover'):\n            #cover\n            t = self.isexists(item,'cover')\n            item = t[0]\n            if not t[1]:\n                item = self.download_img(item,'cover')\n            #beitou\n            if item['hav_beitou'] :\n                t = self.isexists(item,'left_beitou')\n                item = t[0]\n                if not t[1]:\n                    item = self.download_img(item,'left_beitou')\n                t = self.isexists(item,'right_beitou')\n                item = t[0]\n                if not t[1]:\n                    item = self.download_img(item,'right_beitou')\n                return item\n            else:\n                return item\n        else:\n            return item\n```\n\n###middleware.py\n```\n# -*- coding: utf-8 -*-\n\n#from scrapy.exceptions import IgnoreRequest\n#from scrapy.contrib.downloadermiddleware import DownloaderMiddleware\nimport time\nimport re\n\nclass Test(object):\n    def __init__(self):\n        self.T = {}\n        \n    def get_bookid(self, request):\n        bookid = int(re.findall(r'Novel/(\\d+\\d)', str(request.url))[0])\n        return bookid\n        \n    def process_request(self,request,spider):\n        if re.search(r'Novel/\\d+\\d','request.url'):        \n            now_time = time.time()\n            bookid = self.get_bookid(request)\n            if self.T.get('%d' % bookid) == None:        \n                self.T['%d' % bookid] = now_time\n                return None\n            elif now_time - self.T['%d' % bookid] >= 10800 :\n                return None\n            else:\n                raise IgnoreRequest(\"The novel is't update\")\n        else:\n            return None\n```\n\n###spider.py\n```\n# -*- coding: utf-8 -*-\n\nimport scrapy\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\nfrom sfacg.items import SfacgItem\nimport re, requests, time, random\n\nclass SfacfgSpider(CrawlSpider):\n    name = 'sfacg'\n    rules=(\n        Rule(LinkExtractor(allow=('/List/default.aspx?PageIndex=\\d+'))),\n        Rule(LinkExtractor(allow=('/Novel/\\d+/'),deny=('/List/default.aspx?PageIndex=\\d+')),callback='detail_parse')\n        )\n    allowed_domains = ['sfacg.com']\n    start_urls = ['http://book.sfacg.com/List/']\n\n    def test_url(self, url):#检查url\n        if re.match(r'^/', url):\n            out = 'http://book.sfacg.com' +str(url)\n        else:\n            out = url\n        return out\n\n    def parse_start_url(self, response):#初始化下载，获得总页数、总书籍数\n        max_page = int(response.xpath('//div[@class=\"list_pages\"]/ul[@class=\"nav pagebar\"]/li[last()-1]/a/text()').extract()[0])\n        sumNumbook = int(response.xpath('//div[@class=\"list_pages\"]/div[@class=\"page_l\"]/span/text()').extract()[0])\n        page_baseurl = 'http://book.sfacg.com/List/default.aspx?PageIndex='\n        Item = SfacgItem()\n        Item['bookid'] = 0\n        Item['bookname'] = 'sum'\n        Item['text_time'] = time.ctime()\n        Item['sumNumbook'] = sumNumbook\n        Item['max_page'] = max_page\n        t = requests.post(\"http://book.sfacg.com/ajax/ashx/Common.ashx?op=getunlockhour\", {'nid': 44068})\n        Item['VIP_time'] = int(re.findall(r'(\\d+)', t.text)[0])\n        yield Item\n        for page_url in [page_baseurl +str(i) for i in range(max_page+1)[1:]]:\n            yield scrapy.Request(page_url)\n\n    def detail_parse(self, response):#书籍信息获得\n        bookid = int(re.findall(r'Novel/(\\d+\\d)', str(response.url))[0])\n        Item =SfacgItem()\n        Item['text_time'] = time.ctime()\n        ##静态部分获取\n        Item['bookid'] = bookid\n        Item['bookname'] = response.xpath('//ul[@class=\"synopsises_font\"]/li[1]/img/@alt').extract()[0]\n        Item['author'] = response.xpath('//ul[@class=\"synopsises_font\"]/li[2]/a/text()').extract()[1]\n        Item['word_number'] = response.xpath('//ul[@class=\"synopsises_font\"]/li[2]/span/text()').extract()[4]\n        Item['type'] = response.xpath('//ul[@class=\"synopsises_font\"]/li[2]/a/text()').extract()[0]\n        commen_num = response.xpath('//span[@class=\"content_title\"]/span/a/text()').re('\\d+')\n        l_c = len(commen_num)\n        if  l_c == 0:       \n            Item['brief_commen_num'] = 0\n            Item['long_commen_num'] = 0\n        elif l_c == 1:\n            Item['brief_commen_num'] = int(commen_num[0])\n            Item['long_commen_num'] = 0\n        else:\n            Item['brief_commen_num'] = int(commen_num[0])\n            Item['long_commen_num'] = int(commen_num[1])\n        #VIP\n        if response.xpath('//ul[@class=\"synopsises_font\"]/descendant::img/@src').re('vip.gif'):\n            is_VIP = True\n        else:\n            is_VIP = False\n        Item['is_VIP'] = is_VIP\n        b = response.xpath(\"//ul[@class='synopsises_font']/li[2]/child::text()\").extract()\n        i = 0\n        while b[i] == u'\\r\\n' or b[i] == u'\\r\\n\\r\\n':\n            i = i + 1\n        if i >= 5:\n            Item['brief_introduction'] = ''\n        else:\n            Item['brief_introduction'] = b[i]\n        Item['update_time'] = response.xpath('//ul[@class=\"synopsises_font\"]/descendant::text()').re('(\\d{4}.+\\d\\d)')[-1]\n        Item['status'] = response.xpath('//ul[@class=\"synopsises_font\"]/li[2]/descendant::text()').re('\\[(.*)\\]')[-1]\n        ##动态部分获取\n        #浏览器\n        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}\n        s = requests.Session()\n        s.headers.update(headers)\n        #一般动态\n        data0 = {'nid': bookid}\n        t1 = s.get(\"http://book.sfacg.com/ajax/ashx/GetTags.ashx\", params=data0)\n        Item['Tags'] = t1.json()\n        t2 = s.get(\"http://book.sfacg.com/Ajax/ashx/GetTicketNum.ashx\", params=data0)\n        Item['MonTicketNum'] = t2.json()\n        t3 = s.get(\"http://book.sfacg.com/Ajax/ashx/GetNovelPointSet.ashx\", params=data0)\n        Item['point'] = t3.json()\n        t4 = s.get(\"http://book.sfacg.com/NovelData/Statistic/%d.js\" % bookid)\n        time.sleep(0.15*random.random())\n        p1 = int(re.findall(r'viewTimes.*?(\\d+)', t4.text)[0])\n        p2 = int(re.findall(r'bookMarknum.*?(\\d+)', t4.text)[0])\n        p3 = int(re.findall(r'favSticks.*?(\\d+)', t4.text)[0])\n        p4 = int(re.findall(r'pointUserCount.*?(\\d+)', t4.text)[0])\n        p5 = int(re.findall(r'point.*?(\\d+)', t4.text)[0])\n        Item['view_time'] = p1\n        Item['bookMarknum'] = p2\n        Item['favSticks'] = p3\n        Item['pointUserCount'] = p4\n        Item['sum_point'] = p5\n        ##图片\n        cover_url = response.xpath(\"//ul[@class='synopsises_font']/li[1]/img/@src\").extract()[0]\n        Item['cover'] = self.test_url(cover_url) \n        #背投\n        beitou = response.xpath('//head/style/text()').re('(http://rs.sfacg.com/web/novel/images/images/beitou.*?)\\\\\"')        \n        if len(beitou) == 0:\n            Item['hav_beitou'] = False\n        else:\n            Item['hav_beitou'] = True\n            Item['left_beitou'] = self.test_url(beitou[0])\n            Item['right_beitou'] = self.test_url(beitou[1])\n        return Item\n\n```\n##iqing.in爬虫\n###iqing_in.py\n```\n#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n\nimport requests,re, os, time, codecs, random, json\nimport markdown_make\n\nclass down_book(object):\n    def __init__(self,url,down_type, sleep=False, update=False):\n        #base\n        self.in_url = url\n        self.sleep = sleep\n        self.time = time.ctime()\n        self.os_name = os.name\n        if down_type == 'l':    #web(w) or local(l)\n            self.down_type = True\n        else:\n            self.down_type = False\n        #base2\n        self.cover_url = ''\n        self.c_ids = []\n        self.i = 0\n        self.img_dir = ''\n        self.txt_dir = ''\n        self.c_start = []\n        self.book_url = ''\n        self.json1 = dict()\n        self.json2 = dict()\n        self.json3 = set()\n        #extends\n        if re.search('book',self.in_url):\n            book_id = re.findall('(\\d+)',url)[0]\n            self.index(book_id)\n            self.c_start = self.down_web(self.c_ids[0])\n        elif re.search('read',self.in_url):\n            c_id = re.findall('(\\d+)',url)[0]\n            self.c_start = self.down_web(c_id)\n            book_id = self.c_start[3]\n            self.index(book_id)\n        else:\n            print('the url error')\n        path_now = os.path.abspath('.')\n        if self.down_type:\n            dir_name = self.c_start[3] +'_' +self.c_start[2] +'_' +self.c_start[4] +'_l'\n        else:\n            dir_name = self.c_start[3] +'_' +self.c_start[2] +'_' +self.c_start[4] +'_w'\n        self.s_dir = os.path.join(path_now,dir_name)\n        if not os.path.exists(self.s_dir):\n            os.mkdir(self.s_dir)\n        self.txt_dir = os.path.join(self.s_dir, 'md')\n        #json\n        json_path = os.path.join(self.s_dir,'json.json')\n        if os.path.exists(json_path):\n            with codecs.open(json_path,'r','utf-8') as j:\n                json_s = j.read()\n            json_in = json.loads(json_s)\n            self.json1 = json_in[0]\n            self.json2 = json_in[1]\n            self.json3 = set(json_in[2])\n            if update:\n                self.json3.clear()               \n        #down\n        print('\\n下载开始……')\n        print(self.s_dir)\n        print(self.c_ids)\n        print(self.book_url)\n        if self.down_type:\n            self.down_img(self.cover_url,'cover')\n        for c_id in self.c_ids:\n            if not self.json3.__contains__(c_id):\n                self.down_web(c_id)\n                if self.sleep:\n                    sleep_time = 0.05+random.random()*0.1\n                    time.sleep(sleep_time)\n        #log\n        self.log()\n\n    def index(self,book_id):\n        in_url = 'http://www.iqing.in/book/%s/' % book_id\n        self.book_url = in_url\n        index = requests.get(in_url)\n        i_c = index.text\n        cover_urls = re.findall(r'src=\"(https://image.iqing.in/cover/.+?)\\?imageView|src=\"(https://image.iqing.in/submit/image/.+?)\\?imageView',i_c)[0]\n        for cover_url in cover_urls:\n            if cover_url != '':\n                c_ids = re.findall(r'href=\"/read/(\\d+)\"',i_c)\n                c_ids.pop(0)\n                self.c_ids = c_ids\n                i = 1\n                for t in self.c_ids:\n                    self.json1[i] = t\n                    i = i + 1\n                self.cover_url = cover_url\n                return\n\n    def down_img(self,url,append=''):\n        self.img_dir = os.path.join(self.s_dir,'img')\n        print(url)\n        if not os.path.exists(self.img_dir):\n            os.mkdir(self.img_dir)\n        if append == '':\n            img_name = os.path.split(url)[1]\n        else:\n            img_name = '!' +append +'_' +os.path.split(url)[1]\n        img_path = os.path.join(self.img_dir,img_name)\n        if (not os.path.exists(img_path)) and append != 'n':\n            i_r = requests.get(url, stream=True)\n            i_r_c = i_r.content\n            with open(img_path, 'wb') as i_f:\n                i_f.write(i_r_c)\n        return img_name\n\n    def down_web(self,c_id):\n        print(c_id)\n        c_url = 'http://www.iqing.in/content/' +c_id +'/chapter/'\n        r = requests.get(c_url).json()\n        volume_title = r[u'volume_title']\n        chapter_title = r[u'chapter_title']\n        book_title = r[u'book_title']\n        book_id = str(r[u'book_id'])\n        # next_chapter_id = r[u'next_chapter_id']\n        author_name = r[u'author_name']\n        updated_time = r[u'updated_time']\n        self.json2[c_id] = {'volume_title':volume_title,'chapter_title':chapter_title,'updated_time':updated_time}\n        t = [volume_title,chapter_title,book_title,book_id,author_name,updated_time,c_id]\n        if self.i != 0:\n            self.text_write(r,t)\n            self.json3.add(c_id)\n        self.i = self.i + 1\n        return t\n\n    def text_write(self,r,t):\n        self.txt_dir = os.path.join(self.s_dir, 'md')\n        if not os.path.exists(self.txt_dir):\n            os.mkdir(self.txt_dir)\n        # file_name_r = str(self.i) +'_' +t[0] +'_' +t[6] +'_' +t[1] +'.txt'\n        # if self.os_name == 'nt':\n        #     file_name = re.sub(r'\\\\|\\/|\\:|\\*|\\?|\\\"|\\<|\\>|\\|', '_', file_name_r)\n        # else:\n        #     file_name = re.sub(r'\\/', '_', file_name_r)\n        file_name = t[6] +'.md'\n        txt_path = os.path.join(self.txt_dir, file_name)\n        updated_time = '*' +re.sub('T',' ',t[-2]) +'*'\n        # f = codecs.open(txt_path,'w','utf-8')\n        with codecs.open(txt_path,'w','utf-8') as f:\n            c = '##%s\\n\\n**%s**\\n\\n%s\\n\\n---\\n\\n' % (t[1],t[0],updated_time)\n            f.write(c)\n            for c_r in r[u'results']:\n                #文字\n                if c_r[u'type'] == 0:\n                    r_c = c_r[u'value'] +'\\n'\n                    c = re.sub('\\n', '\\n\\n', r_c)\n                    f.write(c)\n                elif c_r[u'type'] == 1:\n                    img_url = c_r[u'value']\n                    if self.down_type:\n                        img_name = self.down_img(img_url)\n                        c = '![%s](img/%s)' % (img_name, img_name) +'\\n\\n'\n                        f.write(c)\n                    else:\n                        img_name = self.down_img(img_url,'n')\n                        c = '![%s](%s)' % (img_name,img_url) +'\\n\\n'\n                        f.write(c)\n            f.close()\n\n    def log(self):\n        end_path = os.path.join(self.txt_dir,'9999.md')\n        with codecs.open(end_path, 'w', 'utf-8') as end:\n            i_end_c = '##Book Infomations\\n\\n---\\n\\n**BookName:** [%s](%s)\\n\\n**BookId:** %s\\n\\n**BookAuthor:** %s\\n\\n**DownTime:** %s' % (self.c_start[2],self.book_url,self.c_start[3],self.c_start[4],self.time)\n            end.write(i_end_c)\n        if self.json1.get('9999', True) == True:\n            self.json1[9999]='9999'\n            self.json2['9999']={'volume_title':'END', 'chapter_title':'Book Infomations'}\n        #json\n        json_path = os.path.join(self.s_dir,'json.json')\n        json3 = list(self.json3)\n        with codecs.open(json_path,'w','utf-8') as j:\n            json_out = [self.json1,self.json2,json3]\n            json_s = json.dumps(json_out)\n            j.write(json_s)\n        #xhtml\n        markdown_make.mark2html(self.s_dir)\n        #final log\n        with codecs.open('iqing.in_history.log' , 'a', 'utf-8') as f:\n            history = '%s,%s,%s,%s,%s,%s\\n' % (self.time, self.c_start[2], self.in_url,self.c_start[3],self.c_start[4], self.down_type)\n            f.write(history)\n\nif __name__=='__main__':\n    url = input('plese input url: ')\n    down_type = input('plese input download type(web(w) or local(l)): ')\n    update_in = input('update(t:True,f:False)')\n    if update_in == 't':\n        update = True\n    else:\n        update = False\n    Iqing = down_book(url,down_type, update=update)\n```\n\n###markdown_make.py\n```\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport markdown,codecs,os,re,json, shutil\n\nclass mark2html(object):\n    def __init__(self,folder):\n        #根目录\n        self.folder_r = folder\n        print('开始转换……')\n        print(folder)\n        #源\n        self.folder_s = os.path.join(self.folder_r, 'md')\n        #结果\n        self.folder = os.path.join(folder, 'xhtml')\n        #delete dir\n        if os.path.exists(self.folder):\n            shutil.rmtree(self.folder, True)\n            print('%s removed!' % self.folder)\n        #mkdir\n        if not os.path.exists(self.folder):\n            os.mkdir(self.folder)\n        #json\n        json_path = os.path.join(self.folder_r,'json.json')\n        if os.path.exists(json_path):\n            with codecs.open(json_path,'r','utf-8') as j:\n                json_s = j.read()\n            json_in = json.loads(json_s)\n            self.json1 = json_in[0]\n            self.json2 = json_in[1]\n            self.i_n = dict()\n            for ts_0 in self.json1:\n                self.i_n[int(ts_0)]=self.json1[ts_0]\n        #covert\n        self.volume = set()\n        self.markdown_covert()\n\n    def markdown_covert(self):\n        for md_n in self.i_n:\n            #md\n            md_cid = self.i_n[md_n]\n            md_name = md_cid +'.md'\n            md_path = os.path.join(self.folder_s, md_name)\n            with codecs.open(md_path, 'r', 'utf-8') as f_in:\n                md_content = f_in.read()\n            #json\n            json2 = self.json2[md_cid]\n            volume_title = json2['volume_title']\n            chapter_title = json2['chapter_title']\n            #updated_time = json2['updated_time']\n            #volume\n            self.volume_file(md_n, volume_title)\n            #web\n            web_title = volume_title +'-' +chapter_title\n            web_title = self.web_title_deal(web_title)\n            web_head = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n<head>\\n<title>%s</title>\\n</head>\\n<body>\\n' % web_title\n            web_body = markdown.markdown(md_content)\n            web_tail = '</body>\\n</html>'\n            web = web_head +web_body +web_tail\n            #xhtml\n            xhtml_name = str(md_n) +'_' +md_cid +'_' +volume_title +'_' +chapter_title +'.xhtml'\n            if os.name == 'nt':\n                xhtml_name = re.sub(r'\\\\|\\/|\\:|\\*|\\?|\\\"|\\<|\\>|\\|', '_', xhtml_name)\n            else:\n                xhtml_name = re.sub(r'\\/', '_', xhtml_name)\n            xhtml_path = os.path.join(self.folder, xhtml_name)\n            with codecs.open(xhtml_path, 'w', 'utf-8') as f_out:\n                f_out.write(web)\n            print('%s\\t%s' % (md_n, md_cid))\n    \n    def volume_file(self, cid, volume_title):\n        web_title = volume_title\n        web_title = self.web_title_deal(web_title)\n        web_head = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\\n<head>\\n<title>%s</title>\\n</head>\\n<body>\\n' % web_title\n        web_body = '<h1 style=\"text-align: center; \">%s</h1>\\n' % volume_title\n        web_tail = '</body>\\n</html>'\n        web = web_head +web_body +web_tail\n        #xhtml\n        vid = cid -1\n        xhtml_name = str(vid)  +'_' +volume_title +'.xhtml'\n        if os.name == 'nt':\n            xhtml_name = re.sub(r'\\\\|\\/|\\:|\\*|\\?|\\\"|\\<|\\>|\\|', '_', xhtml_name)\n        else:\n            xhtml_name = re.sub(r'\\/', '_', xhtml_name)\n        xhtml_path = os.path.join(self.folder, xhtml_name)\n        if not self.volume.__contains__(volume_title):\n            self.volume.add(volume_title)\n            with codecs.open(xhtml_path, 'w', 'utf-8') as vf_out:\n                vf_out.write(web)\n            print('volume\\t%s' % vid)\n    \n    def web_title_deal(self, web_title):\n        web_title = re.sub(r'\"', '&quot;', web_title)\n        web_title = re.sub(r'&', '&amp;', web_title)\n        web_title = re.sub(r'<', '&lt;', web_title)\n        web_title = re.sub(r'>', '&gt;', web_title)\n        return web_title\n\nif __name__=='__main__':\n    path = os.path.abspath('.')\n    mark2html(path)\n```"
		},
		{
			"post_id": 26,
			"title": "无题",
			"date_published": 1467817098.82,
			"body": "![2.png](data/res/1467822483592-2.png)\n他先前走着，如果现在回头的话——那人一定也会回头，他强烈地拥有这种想法。没有根据，却充满自信。\n于是，在完全走过铁道的时候，他缓缓转过身看向那位女性。她也慢慢转了过来，二人目光交错。\n就在心与记忆即将沸腾的瞬间，小田急线的特快列车挡住了二人的视野。\n\n电车通过之后，他想。她应该还在那里吧。\n\n不过都无所谓。如果他就是那个人的话，这已经算是个奇迹了。他想……\n等这列电车开过之后就向前走，他在心里作出决定。\n![1.png](data/res/1467822470666-1.png)"
		},
		{
			"post_id": 25,
			"title": "2016-07-06",
			"date_published": 1467815889.708,
			"body": "这个月忙着考试，感觉我这个网站好像已经死了！！\n\n***\nPS1：\n刚刚发了一下，发现还有一个节点在。\nPS2:\n之前发布的时候都是用whonix连接tor，估计是受到编程随想的影响。\n但是不得不说，速度有一些堪忧呀！当然也没有那么慢，毕竟是自建的ss服务器，主要是开关虚拟机有一点麻烦，而且发布要实好几次才能成功**（当时特殊时期，现在基本不会了）**。\n于是计划不戴tor了，裸身上阵。\n如果我这都被XXX了，那我也无话可说了！\nPS3:\n感觉自己的网站好丑，打算优化一下。\n计划加上标签。\n当然这还不在日程上，毕竟还有其他事情要做。"
		},
		{
			"post_id": 24,
			"title": "2015.6.4",
			"date_published": 1465049590.9,
			"body": "今天答辩终于完了，忙了好一会。\n\n刚刚在论坛里看到有人发有关6.4的纪念帖子。\n>[紀念89學運27週年，為64死難者默哀](http://127.0.0.1:43110/gfwtalk.bit/?Topic:3_1Fv1xGsCxVzHNkCyCMLkJsXaqHSqc5VS45/+89+27+64)\n>向為了民主訴求而犧牲默哀，為了無辜慘遭屠殺的市民默哀！\n\n以前看了6.4的纪录片，确实有一些感想，现在回想起还是有一些感想，但更多的是一些静默，一种漠不关心。\n我不知道是什么改变了。\n现在我只是默默的看看，翻墙出去瞄瞄，看看都有什么动向，但并没有什么鸟用。\n\n也许政治这种东西是肉食者关心的事，我们屁民就好好待着吧！\n\nPS:[中国互联网维护日互联网可用性测试](http://127.0.0.1:43110/gfwtalk.bit/?Topic:3_1958F7oCppj78MP966AfojMQwHg2WUupzq/)\n\n---\n>杭州电信：\n>    无界：找不到服务器\n>    自由门：找不到服务器，使用 DNSCrypt 后可以找到服务器，但是访问不了网页\n>    赛风：正常连接，速度尚可\n >   蓝灯：连接上但不稳定，速度极低常常无法访问，提示连接被重置\n >   某商业 Shadowsocks 服务：连接正常\n>\n>杭州移动：\n>\n>    无界：正常连接\n>    自由门：正常连接\n>    赛风：无法连接\n>    蓝灯： 正常连接\n>    某商业 Shadowsocks 服务：连接正常\n>    Freenet Opennet：性能明显减低，连接数减少\n>    Tor -meek-amazon：正常连接\n>    Tor -meek-azure：正常连接\n>\n>看起来移动下挂掉的情况不是很严重，电信还是挺严重的，不过至少还能用。\n"
		},
		{
			"post_id": 23,
			"title": "2015.06.02",
			"date_published": 1464879787.1,
			"body": "老周曾经说过，忙完秋收忙秋种，学习学习再学习。\n\n但是上了大学以来就没怎么努力学习，绩点也有一点惨不忍睹。\n马上6级了，没有复习，八成过不了"
		},
		{
			"post_id": 22,
			"title": "六一儿童节快乐",
			"date_published": 1464879733.3,
			"body": "虽然已经6月2号了"
		},
		{
			"post_id": 21,
			"title": "太热了",
			"date_published": 1464624101.2,
			"body": "刚刚看了下温度计29度，没有空调的夏天真的很难过！"
		},
		{
			"post_id": 20,
			"title": "关于上网白名单的讨论，附新疆断网旧文一篇",
			"date_published": 1464511679.2,
			"body": "<a href=\"http://127.0.0.1:43110/1Nse6WcodQ5Mj6ZwvZvuyCVvQESwuxbCUy/?Topic:2_12kgNNnBaR3s7bN761BCQtkzSC7EbrK2Jd/\"target=\"_blank\">呃。。。。有人担心开启白名单模式了</a>\n\n##附： 互联网上你不知道的新疆断网<a href=\"http://www.kaixin001.com/repaste/6750921_3102996832.html\"target=\"_blank\">（转贴）</a>\n互联网上你不知道的新疆断网（转贴）\n\n大牛编前语：下面这篇文章是新疆断网时一位热心网友写的并且帖在新疆家园天网上，记录了新疆从断网到2009年12月25日期间的互联网历史。在这个中华人民共和国成立61年的大喜日子里，本着历史不能出现断档的原则，我把他转贴出来，作者不详，感谢天网，谢绝本省跨省的一切形式的追捕。\n\n---\n互联网上你不知道的新疆断网\n\n现在新疆全境依然是断网断短信断国际长途三断中。这种中断跟内地的GFW不尽相同，对于普通用户基本上算是物理隔离，几乎所有**ip和**端口均无法访问。能够访问的只有疆内的站，以及极少数ip和主机在外地的国家级官方网站（如网站备案查询、各种考试报名网站）。也就是说，国内公众网的GFW是默认放行，部分ip和域名拦截+关键词拦截；而新疆是默认断开，个别ip和域名放行。\n\n上网方式方面，7月6日凌晨4点chinanet全部封锁，随后是移动、联通的公众网络。教育网（CERNET）估计是用的电信出口，也是同步被封。科技网（金桥）多撑了两天，于7月9*被封锁。手机上网方面，NET方式均同时被封锁，WAP方式则推迟到七月十几*左右才封锁，其中电信的WAP方式甚至到了 8月中才封锁完毕，期间一直可以登录WAPQQ。3G上网卡方面，基本是同时封锁，有传闻是如果用天翼3G拨号时获得的IP是120.X.X.X就可以顺利上网，不过这个传闻未验证。普通用户能够翻出GFW终极版的上网方式只有拨号一种了，也就是10年前我们大家常用的56k窄带拨号。具体方式后文详述。\n\n短信方面，点对点短信均无法发送，手机端直接报错，应该是点对点短信网关根本就没启动。仅有部分公众SP业务开通，如天气预报、手机报、 10000/10086/10010这类的运营商官方信息，用户可以接到。需要注意的是外地手机漫游至新疆，一样无法发短信，所以不要妄想买个外地短信卡就能在新疆聊天了。我自己曾经发短信如飞的手指，现在也快不知道怎么按了。可以说这几个月新疆的SP和CP类公司遭到了重创，倒闭关门或撤回内地的不计其数。\n\n国际长途方面，大概是7月7*开始就非常难以拨入，8月以后就基本无法拨入了。呼出方面，除了当局指定的仅有几个机关单位和几个大的电信营业厅可以打（像不像80年代），其他全部的直播、IP、网络国际长途电话均无法呼出。批发国际ip卡的商家亏到出屎，新疆大批做中亚外贸的商家也都难以开展业务。我的表妹在马来西亚上学，快半年了没一个电话，她老妈被逼的请了假去内地上网，终于在QQ上得以一见。\n\n断网后的这些封锁，除国内长途封锁时间较短，断网后几天即解除外，其他的直到现在（12月25*）也没有一点儿放松的迹象。基本每个月都有传闻说这个月/下个月要开网，什么时候开网也是疆内各论坛最常讨论的一个话题，已经不能用*经来形容了。可惜这些期望，最后都是以失望告终。\n\n断网持续48小时：这是7月6*最初的传闻，据说来自于运营商。可惜随着7月7*汉族大规模游行警告告吹。\n\n断网一周：参照石首，这个传闻很快告吹。\n\n断网半个月/一个月：说西藏314时断网就这么久，新疆也会参照。可惜据我本人考证，西藏当时似乎没有断网，或断网时间很短。在Google上搜“西藏 断网”，貌似没有什么有价值的信息，希望有了解情况的JR说说。\n\n10月国庆后开网：这个传闻是说都60大寿了，安定和谐的天朝肯定会给我们这2000w*民开网。告吹。\n\n11月1*开部分门户网站，春节放开全部：这个传闻是最有模有样的，据说是自治区级领导安排，大部分运营商都内部通报了。可惜现在马上新年了，仍然没有一个门户网站能够访问。这个传闻应该为真，据说最后是自治区政法委书记符强同志强力压了下来。\n\n最新的传闻是说1月份要开放部分门户，全部放开到2010年5月份左右：这个消息同样来自运营商内部，然则已经没有多少人关心了。对通网的传闻新疆的网民早已麻木，对这类消息都是无奈的一笑了之。\n\n二、翻墙\n\n下面来说说大家最关心的，那些能上网的新疆人，是怎么从铁桶一般的墙里翻出来的。\n\n首先，内地公网使用的翻墙方法完全不适用于新疆，因为封锁的原理不同。新疆现在就是一个大局域网，断网根本就不通，你提供哪些国外的翻墙网站或者翻墙软件，自然只会404 not found或者service unveilabe。现在已知的是如下几种方法：\n\n这个是应用最广泛的拨号方式，你不需要有特权，也不需要上面有人，只要你有一个破旧的56k窄带猫，连上电话线就可以。虽然速度很慢，开个新浪都要等2分钟，但是，那是**啊（请自行脑补加入流泪233表情）！当然，你不能拨本地的窄带接入号，那一样只能访问疆内网。\n\n流行的拨号号码有这些：\n\n022-16300、0891-16300：8月起，这两个号码造福了很多网民。稳定，但不容易拨入，平均拨号5次能够连接上，10月份被封。\n\n010-95700：10月到12月间，这是最流行的拨号接入号，拨通率非常高。如果你看到有人家里固话电话费突然升到好几百元甚至上千元，那不用问，肯定是天天95700呢。这个号码12月初被封。\n\n0756-96169：95700不在的日子，我们又找到了96169。可惜没能坚持1个月就被封了。\n\n友情提醒：拨长途窄带ISP号码前请加拨11808，这样一小时话费能够控制在5元以内，否则请按长途标准资费（大约1分钟7毛钱）换算自己的电话费。\n\n2、企业内网：\n\n大家知道很多大型企业的内部网络都是全国连通的，电信运营商自不必说，其他包括铁路、石油、银行等都有自己的全国性网络，由于各企业的业务需要，这些内网是不可能断掉的。这就给通过内网翻墙带来了可能性。只要在其他省份找到一台同在内网且可以上网的机器，于其上开个代理，便可以顺利上网。一时间，“内地同事 ”成了抢手货，疆内软件网站上ccproxy、wingate这些代理软件也都上升到下载榜前列。\n\n这种翻墙方式网速尚可，实现难度也不算太高，也算是比较流行的翻墙方式。当局下强硬命令要求各单位逐机清理的7-5视频，大多也是这种方式流传出去的。\n\n3、海事卫星等卫星通讯方式：\n\n卫星通讯方式的特性决定了它不可能像有线网络一样被地区性中断，有条件的单位和个人，可以通过这种方式翻墙。不过这种方式网速不怎样，而且资费极贵，所以基本只有少数单位在使用。\n\n有次翻墙出来上QQ遇到一个疆内的朋友，问他怎么上网的，便给我炫耀其单位特权申请了卫星专线4条，办公室电脑都能上网了云云。\n\n4、特权\n\n在中国，任何事情的限制范围，都可以加一条：领导例外——就连断网都断得这么有中国特色。除了通讯管理局、公安厅、安全厅这些跟互联网直接有关的单位可以上网外，据我所知还有各通讯运营商处级以上领导，当局机关的部分部门，疆内官方大站的运营部门（亚心网、天脉、亚心网、亚心网、**），均可以顺利上网。当然，各单位都有相应的管制手段，如限制部分端口，限制QQ登录等，但还是让普通百姓羡慕不已。\n\n当然，普通企业和个人“原则上”也可以申请开通，但除非你能通过电信运营商-通讯管理局-公安/安全/自治区当局多个部门层层审批通过才行，至今通过的公司寥寥无几，个人更是不要奢望了。\n\n5、异地上网方式\n\n之所以新疆能被这么彻底的断网，跟新疆的地理位置也有很大关系。如果内地的某个省份出事被断网，网民很容易就能到别的省去，而且内地错综复杂的线路也不是能说断就断的。新疆的骨干网出疆端口只有两条，很容易就能封锁。如果你想出去外省，哪怕是到最近的甘肃，那一千多公里的路程至少也得1整夜的火车。\n\n但即便这样，仍然有难以忍耐的网民跑去外省上网。BBC就报道过一个新疆人坐飞机去深圳上网的事情。而新疆本地QQ上流传的一条信息则是：“柳园一出车站就有好几家网吧，我去过了。网吧对面就是旅馆，下火车出站就可以上网了。那个网吧全是新疆这边的，本地人基本没与几个，到了晚上8点全是新疆的网民。包夜 8 块钱。10点到早上9点。就这些了。过年时候我也要去 我的电话136699xxxxx要去一起呵呵！”——柳园是甘肃省离新疆最近的一个小火车站。\n\n还有一则未经证实的消息，是说有人利用手机基站信号的边界重叠，去新疆东南边的若羌县用青海省的3G信号上网。之所以说未经证实，是因为那里是**人口占90%以上的民族聚居地，就算能3G上网，但小命不一定能保全啊……\n\n以上基本就是新疆境内翻墙的方法，发出来给对网络习以为常的里层JR们看看，新疆人民上个网是多么难。有的人QQ密码忘了，有的人苦心经营的网站废了，连菜地都快半年没收成了……去内地出差回来的同事说：“你知不知道当我输入三达布溜淘宝点康木一回车，看到那熟悉的黄色页面出现时，才真正的体会到了什么叫这一刻我内牛满面……\n\n三、现状\n\n断网前新疆 90%以上的互联网流量都是流向内地的，但现在这个大局域网已然成了私服和山寨网站的乐土。各种网游私服层出不穷；利用亚心网和***搭建的新疆QQ也有N个版本了；山寨开心网不下10个，虽然用的都是同一套源代码连界面配色都一模一样；hao123类的疆内网站导航已知的就有几十个；甚至连百度和 Google都被山寨了（参见附图），虽然做的有够烂……最近最火的，则是一个叫zn11的网站，点击率超级高。这个网站是干什么的呢，说出来外面的各位可能会哑然失笑——就是用teleport把新浪、腾讯、网易等几个大站的首页和2、3级页面离线下载，然后挂到站上。虽然可能几天才更新一次，大量链接点不开，视频也无法收看，但还是有很多人上班第一件事就是打开这个zn11。也许是让自己觉得离真正的互联网近一点吧……\n\n这是一个最好的时代，这是一个最坏的时代。对于私服和山寨网站运营者来说，显然是前者。但对于电信运营商来说，很有可能是后者。虽然广播报纸上电信移动联通三家的3G广告还是铺天盖地，但是谁都会对“WCDMA，上网速度可达7.2兆”的广告一笑了之，网都没了推广什么3G啊。损失最重的是主要收入来自于宽带业务的新疆电信，7月份全疆宽带费用全免，一个月直接损失5000多万，关联损失8000多万。8月份以后宽带费用采用用多少交多少不用不交，最多收取原费用8折的方法，但已然抑制不住每月飙升的宽带拆机量了。\n\n上不了网的不方便，很多人已经渐渐习惯了。官方大站上不少“没有互联网的*子，我过得更好了”这类的傻逼帖子，甚至我朋友里也出现了斯德哥尔摩综合症患者。但我想大家更为担忧的，应该是TG在观察过新疆百姓对三断的反应之后，会不会把这个损招用在其他省份，甚至全国。\n"
		},
		{
			"post_id": 19,
			"title": "呵呵哒",
			"date_published": 1464508551.1,
			"body": "毕竟西湖\\*\\*月中,风光不与\\*\\*时同。"
		},
		{
			"post_id": 18,
			"title": "5月35日",
			"date_published": 1464443869.467,
			"body": "在5月35日来临前夕，让我们用更伟大的墙欢迎它的到来!\n\n![greatfirewall.png](data/res/1464444511190-greatfirewall.png)\n"
		},
		{
			"post_id": 17,
			"title": "考完试了",
			"date_published": 1464422468.545,
			"body": "如题\n今天早上考完试了。\n估计刚刚过的样子。\n\n这学期果然堕落了"
		},
		{
			"post_id": 16,
			"title": "搬迁计划进度",
			"date_published": 1464191644.441,
			"body": "#Step1:安装host系统（debian）\n##数据备份\n###Windows部分\n1.将windows自己用的软件的列表整理了出来以供备用\n一部分软件列表（都是好用的东东）\n```\nGreenshot\nEverything\nEvernote\nWox\nBitTorrent Sync\nppsspp\nWinHTTrack\nSigil\nPotPlayer\nEverything\nCCleaner\n……\n ```\n2.将硬盘上的东东copy到移动硬盘上\n###Linux部分\n1.备份软件\n`sudo dpkg --get-selections > /tmp/package.selections`\n2.打包home文件夹\n`tar -cvp -f /tmp/home.tar /home/user`\n3.备份一些其他东西\n`tar -cvpj -f /tmp/etc.tar.bz2 /etc`\n##还原\n1.安装debian\n分区方案：\n双硬盘，系统装在120G SSD上\n```\nsdb                     8:16   0 111.8G  0 disk  \n├─sdb1                  8:17   0   512M  0 part  /boot/efi\n├─sdb2                  8:18   0   244M  0 part  /boot\n└─sdb3                  8:19   0 111.1G  0 part  \n  └─sdb3_crypt        254:0    0 111.1G  0 crypt \n    ├─catcat--vg-var  254:4    0   2.8G  0 lvm   /var\n    ├─catcat--vg-tmp  254:5    0   380M  0 lvm   /tmp\n    ├─catcat--vg-root 254:6    0  18.6G  0 lvm   /\n    ├─catcat--vg-swap 254:7    0   7.5G  0 lvm   [SWAP]\n    └─catcat--vg-home 254:8    0  81.8G  0 lvm   /home\n```\nLVM+加密\n2.还原软件\n上一篇博客贴的方法有问题\n命令应为\n`dpkg --set-selections < package.selections && apt-get dselect-upgrade`\n详见https://debian-handbook.info/browse/stable/sect.apt-get.html\n3.还原home\n4.一些收尾工作\n#Step2:虚拟机\n（待续……）\n"
		},
		{
			"post_id": 15,
			"title": "虚拟机搬迁计划正式启动",
			"date_published": 1464167300.391,
			"body": "为了方便备份与重装\n准备将Windows系统重装到虚拟机，host计划使用Debian\n\n##附：[重装也不怕 教你怎样备份Ubuntu](http://www.linuxidc.com/Linux/2010-08/28181.htm)\n当系统出现问题需要重装或者新版本的出现需要重新安装Ubuntu的时候\n\n你可能深有体会？重新安装系统后，需要一个一个安装你所用到的软件，那有没有更简便的方法呢？\n\n---\n答案是肯定的，一般来说我们在重装前要备份安装软件的列表，软件源，用户文件，下面让我们来看看怎样实现的！\n\n1.备份已安装软件包列表\n\nsudo dpkg –get-selections > /home/user/package.selections\n\n2.备份Home下的用户文件夹\n\n如果你已经将Home放在额外的分区，这一步就不必了，复制所有用户文件夹下的所有内容到另外的分区，注意要包含隐藏文件（Ctrl+Hide）\n\n3.备份软件源列表，将/etc/apt/文件夹下的sources.list拷贝出来保存即可\n\n新系统安装后的恢复：\n\n1.复制备份的Sources.list文件到新系统的/etc/apt/目录，覆盖原文件，并替换（Ctrl+H）文档中的intrepid为jaunty。然后更新软件源（sudo apt-get update）。\n\n2.重新下载安装之前系统中的软件（如果你安装的软件数量比较多，可能会花费较长时间）\n\nsudo dpkg –set-selections /home/package.selections && apt-get dselect-upgrade\n\n3.最后将备份的主文件夹（/home/用户名）粘贴并覆盖现有主文件夹\n\n好了，用这个方法我们可以基本在不丢失现有系统和软件设置的情况下使用全新的Ubuntu系统了！"
		},
		{
			"post_id": 14,
			"title": "BT sync 密钥备份",
			"date_published": 1464166407,
			"body": "电脑硬盘中同步的所有BT sync密钥\n\n---\n##编程随想\nBRSSYZTSAC6UGYTUOJ22L4GCO7QESPPBD    政治\nBNZ6DOA6W577O6GUNH7C3MY6DWC6FTDQB    心理学\nBSH7FXJFVWJTKWGSX5GTWX7PHZZ2D2M7Q    历史\nB2FRYA6AXCDW6CF4YJVFWKH2HAXOFICOX    经济\nB3WNBTAAFFAODFR6FQ3E3L5BBSJAFNBSJ    管理\nBZR4TTYHT25QWUIE6YNMAKWUGBHKSGLC6    社会学\nBMBB5YLBIJJAE5H6TP27OS7YCEUKCYHZK    文艺\nB6WWVBXPMZDI5IL4KED6AAHA5FO4UNKQF    哲学\nBMWWZALG4P56LREF47EE2WSWHZEM4E6BL    军事\nBUPSDXFA3TP7KCMLHALRHLIX2FEJEUJFE    IT\nBTLZ4A4UD3PEWKPLLWEOKH3W7OQJKFPLG    翻墙软件（包含常见的翻墙软件，时常更新）\nB7P64IMWOCXWEYOXIMBX6HN5MHEULFS4V    “博客离线浏览”以及“博客电子书制作脚本”\n##电子书\n###苏菇莨百度盘书（[知乎链接](https://www.zhihu.com/question/44354572/answer/98331745)）\n第一部分：B3EWLQK7C6O6RK3CU5CG3PZUYDUJLMYCP\n第二部分：B25G263OSC7ZAXPLCZESSL3DFJOHK6UQ3\n###色酷全书live\nBBZOUOJQOMCLAG3UUVP5RWZD6G2QGQU5P\n\n"
		},
		{
			"post_id": 13,
			"title": "文件上传功能测试",
			"date_published": 1464094709.861,
			"body": "[Mahuri.svg](data/res/1464095051568-Mahuri.svg)\n测试\n\n技术支持\n[ZeroBlog 扩展: 更方便的文件上传](http://127.0.0.1:43110/1KUiLpPjyCCVst5sGuhzLAsJmBDyvYQm8g/?Post:8:ZeroBlog+%E6%89%A9%E5%B1%95:+%E6%9B%B4%E6%96%B9%E4%BE%BF%E7%9A%84%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0)"
		},
		{
			"post_id": 8,
			"title": "15-05-24",
			"date_published": 1464088993.336,
			"body": "马上要考试，有一点小忙。\n\n还在看《异世界女神传》，看到#28924，发展真是混乱啊！\n\n马上要考试，努力背书吧！\n"
		},
		{
			"post_id": 7,
			"title": "转载",
			"date_published": 1464000723.2,
			"body": "如果天空是黑暗的，那就摸黑生存；\n如果发出声音是危险的，那就保持沉默；\n如果自觉无力发光的，那就蜷伏于墙角。\n但不要习惯了黑暗就为黑暗辩护；\n不要为自己的苟且而得意；\n不要嘲讽那些比自己更勇敢热情的人们。\n我们可以卑微如尘土，不可扭曲如蛆虫。"
		},
		{
			"post_id": 6,
			"title": "一些有用的东西（转载）",
			"date_published": 1464000193.659,
			"body": "##ZeroNet\n[ZeroNet结构解析/全动态网站/无限制网站](http://127.0.0.1:43110/1LcQTyUYkq3n458rkHVQxhHyvUdmSYFURd/?Post:14#Comments)\n[成功引入标签系统和翻页系统](http://127.0.0.1:43110/eroz.bit/?Post:10#Comments)\n[ZeroNet 在远程服务器](http://ryc111.com/2016/05/03/zeronet-on-vps/)\n[一个粗略的博客样式修改指南](http://127.0.0.1:43110/1HotJMoS9Z1v8UwDCmv3VDG3q88c5na8cZ/?Post:7)\n<a href=\"http://127.0.0.1:43110/mosen.bit/?Post:8:ZeroNet+%E5%9C%A8%E6%96%87%E7%AB%A0%E4%B8%AD%E8%AE%BE%E7%BD%AE%E9%93%BE%E6%8E%A5%E6%96%B0%E7%AA%97%E5%8F%A3%E6%89%93%E5%BC%80%E7%9A%84%E6%96%B9%E6%B3%95\"target=\"_blank\">ZeroNet 在文章中设置链接新窗口打开的方法</a>\n[ZeroBlog 新手指导](http://127.0.0.1:43110/mosen.bit/?Post:5:ZeroBlog+%E6%96%B0%E6%89%8B%E6%8C%87%E5%AF%BC)\n[通用的自动快照脚本来创建一系列ZeroNet网站快照，实现真正不可能关闭的网站！](http://127.0.0.1:43110/gfwtalk.bit/?Topic:30_13Z7XxTa7JuFat3KzzMWu3onwM6biLuurJ/+ZeroNet+5+14+UPDATE)\n##markdown用法\n[用Markdown 优雅的在 ZeroNet 上写文章](http://127.0.0.1:43110/gfwtalk.bit/?Topic:6_1Q7UWk3im88kJkzmhzfD5q44rQDXLatxYq)\n[Zeronet Markdown Cheatsheet](http://127.0.0.1:43110/cryptonbits.bit/?Post:6:Zeronet+Markdown+Cheatsheet)\n[Links](http://127.0.0.1:43110/148or6yKMeNV4qdmKyCtpaNiFAeU1cccCw/?Post:10:%D0%BF%D0%BE%D0%B4%D1%81%D0%BA%D0%B0%D0%B7%D0%BA%D0%B0+%D0%B4%D0%BB%D1%8F+Markdown)\n##shadowsocks\n[Centos 7安装配置Shadowsocks](https://www.ifshow.com/centos-7-installation-and-configuration-shadowsocks/)\n[CentOS 7 Shadowsocks优化](https://www.ifshow.com/centos-7-shadowsocks-optimization/)\n##公网代理列表\n[ZeroProxies.bit](http://127.0.0.1:43110/zeroproxies.bit/)\n当前有效的开放代理\nhttps://bit.no.com:43110/\nhttp://proxy.zeroexpose.com/\nhttp://zero.pags.to:43110/\n##其他\n[BitTorrent Sync 公共资源索引](/mydf.bit/?Post:72#Comments)\n##Python\n[Python 2.7教程](http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000)\n[Python爬虫学习系列教程](http://cuiqingcai.com/1052.html)\n[Scrapy](http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html)"
		},
		{
			"post_id": 5,
			"title": "每日一记\t",
			"date_published": 1463998679.174,
			"body": "今天开了一下班会，会上先着重强调了一下什么反恐的问题，还要签什么承诺书，承诺不散播恐怖视频、自行删除自己电脑中的恐怖视频什么的巴拉巴拉一大堆。反恐也许这是有些道理，但是对于所谓的恐怖视频的定义我是在是不敢苟同。\n之后说了一些干货。大学决定一个人一生的重要时期，现在大学也过去了将近两年，也就是在暑假之前就应该对自己的以后的人生路该如何走有一些认识。\n这也许是有一些老生长谈，我的转述也遗失颇多，但是这的确对我现在来说触动极大。\n\n自己最近十分迷茫，不知道干什么，感觉好像人生失去了前进的动力。这再和“宅宅综合征”结合起来一起来，实在是让人更加迷茫不止所措。\n只有努力尽今日，然后试着找找未来的方向吧！\n\nps：什么升级考试也该准备了。"
		},
		{
			"post_id": 4,
			"title": "关于内心的阴暗面",
			"date_published": 1463931670.213,
			"body": "每个人内心都有一些阴影的角落，暗藏着一些不为人知(换而言之不想为他人所知）的事情。\n记得《楼下的房客》里有这样的一句话大意是疯狂是由于内心阴暗面的累计。那房东也依靠挑拨众人心中的阴暗，最终看到了一场自相残杀的好戏。\n\n我也有一些不想为他人所知的事情，虽然可以写在日记本中，锁在柜子里，但却有矛盾的想让他人知道，这也许是人矛盾性的一个例证。\n为了不让这些东西憋死，我决定偷偷将其发布到零网上。\n大概就是这样。"
		},
		{
			"post_id": 3,
			"title": "每日一记",
			"date_published": 1463931060.254,
			"body": "今天外出参加了一些活动。然后感觉自己真的有一些封闭。\n反正觉得自己好像越来越宅了，这好像不是一个好现象。\n\n最近在看《异世界女神转》，果如网友评论的一样，到了第五卷以后就好看多了。\n\n最后是学习上的一些事情，好好努力吧！\n感觉好颓废。\n16-05-22"
		},
		{
			"post_id": 1,
			"title": "First bloger",
			"date_published": 1433033779.604,
			"body": "test"
		}
	]
}