{
	"next_topic_id": 1,
	"topic": [],
	"topic_vote": {},
	"next_comment_id": 4,
	"comment": {
		"1567598049_1JcHMiQxXHX9q3ZK2LF9rZjvg3MUtTpcK": [
			{
				"comment_id": 1,
				"body": "> [realiodine](#comment_1_1BdN2jVk4GFXPnGjj4z9GUrFiWDupZpRWG): Question is, how many people are in the Riot room as opposed to Discord or Slack? I don't like fragmentation in a community...\n\nProbably the way to fix that is to use some similar idea that the \"ego masturbation software\", generally called as \"social networks\", use: use the adapter design pattern to make different P2P-applications to speak the same set of protocol(s). Each P2P-application would have its own plugin that acts as an adapter between the P2P-application and a protocol implementation. The protocol implementation may be some well polished server that is implemented at some very portable compiled language, preferably in C++, because C++ compilers are always ported to all hardware and operating system combinations. The C++ application should have a C ABI, because different programming languages support C ABI.  May be some C++ application that links with the Tor and has all of the dependencies included, including the OpenSSL library. If the OpenSSL library becomes \"too old\", then it will get updated as part of the C++ project, because that way the C++ code is properly portable and also compiles on older computers that only have very old Linux/BSD distribution versions available. The issue with modern Linux distributions is that they do not have the drivers for old hardware, but if a modern OpenSSL is available only through the operating system distribution package collection, then the old Linux distros that have the drivers will not get the new OpenSSL. \n\nFrom security point of view it can make sense to run an old Pentium_2 or Pentium_3 computer, because those CPUs did not have the \"trusted zone\" technology and due to their age they have been pretty well studied.\n\nhttps://fediverse.party/\n",
				"added": 1580939548
			}
		],
		"1537907056_1b9urVTLtAAZAidg6hchWqhrVtYXUaoj7": [
			{
				"comment_id": 2,
				"body": "This very same problem has been solved at the \nhttps://www.fossil-scm.org/\nThey use some sort of custom hashing to make it fast. The idea is that the hash that is being used for that purpose does not need to be a secure hash, just probabilistically unique enough to make the linked lists that will be sequentially searched short enough. That allows speed optimizations. For example, there is no need to hash the whole file, may be if just the first 2MiB is hashed, the distribution of the hashes is uniform enough. Collisions are not a problem, because in buckets, where the collisions occur, it is possible to use multiple hashes per file and still use only a single hash in those  buckets, where there are no collisions. An idea:\n\n    if the_given_bucket_with_hashes_of_at_most_512KiB_of_each_file_in_that_bucket_has_collisions(){\n        for_each_of_the_files_in_this_bucket_add_additional_hash_of_at_most_first_1MiB_of_the_file();\n    \n        if the_given_bucket_with_hashes_of_at_most_1MiB_of_each_file_in_that_bucket_has_collisions(){\n            for_each_of_the_files_in_this_bucket_add_additional_hash_of_at_most_first_2MiB_of_the_file();\n    \n            if the_given_bucket_with_hashes_of_at_most_2MiB_of_each_file_in_that_bucket_has_collisions(){\n                for_each_of_the_files_in_this_bucket_add_additional_hash_of_at_most_first_4MiB_of_the_file();\n    \n                if the_given_bucket_with_hashes_of_at_most_4MiB_of_each_file_in_that_bucket_has_collisions(){\n                    //so forth\n                }\n            }\n        }\n    }\n\nThe thing to note is that the maximum hashable file part size does not grow linearly, because if it did, two 1GiB size files that only differ by a few bytes somewhere at their 2/3 would take a very long time to store. The heuristic might also use some more clever way for choosing the hashable data. May be \n\n    hashable_bytes=get_first_N_bytes_from_the_start_of_the_file()+get_last_M_files_from_the_back_of_the_file()\n\nThe Fossil developers have worked pretty hard and long on that question, so it probably helps to study, how they have solved that problem, but it might still be possible to learn, what they did and then combine one's own ideas to develop a special hashing algorithm that is speed-optimized in this particular use case. Probably the study of compression algorithms might come handy. \n\nAnother idea is that the hashing algorithm should be such that it is OK to do the proper placement of files to the buckets in the background, because calculating the hashes still takes a lot of time, but moving files within the same partition is not that time-consuming. May be the fastest temporary hash of the temporary set of buckets might be just file size and first 10B and last 10B of the file. If collisions  occur at that temporary set of buckets, one of the colliding files will be placed to the permanent set of buckets at the \"frontline job\" in stead of waiting for it to be placed there in the background job. Or may be the very first hash of the permanent set of buckets is exactly that lightweight and the 2. hashes are calculated to all files in all buckets in the background, unless there is a need to calculate it for some file in the fore-ground. \n\nA ZeroNet independent workaround, a more general approach, is to implement that feature as a Linux FUSE driver and then just \"mount\" that file system to the data folder. That way the ZeroNet code does not need to be changed at all. May be some Linux copy-on-write (COW) file system already has some similar feature built-in. May be there could be a daemon that looks for duplicates and then renames one of the duplicates and then \"copies\" it back from another duplicate and the COW-filesystem then handles the space saving. After the \"copying\" is complete, the renamed instance can be deleted. The side effect of it will be that the moment some of the big files will be written to, the whole system hangs, because the COW(copy-on-write)-filesystem will start to copy the files to create the instances that will be written to. So some kind of timing analysis is needed. That can get complex. The Fossil developers do not have such a problem, because artifacts that are once loaded to the repository, usually stay there and they do not have the kind of soft-real-time requirements that a ZeroNet node has. After about 5min surfing I found\n\nhttp://aufs.sourceforge.net/\nhttps://sourceforge.net/p/lessfs/wiki/Home/\n\nA citation from the \"lessfs\" home page: \"Lessfs is an userspace (fuse) inline data de-duplicating filesystem for Linux that includes support for lzo or QuickLZ compression and encryption.\"\n\n\n\n\n",
				"added": 1582488692
			}
		],
		"1565033981_16hDLv7wiUVh3cuz6W1C2RWkesxVnqYkKW": [
			{
				"comment_id": 3,
				"body": "> [kaffie](#comment_6_1NWh3WAty57FH8at1WtrZigMrdhrDkuPzh): ZeroVerse is long dead. Can't sign up anymore because the person running it abandoned it.\n\nIt seems to be running on 2020_02_23\nhttp://127.0.0.1:43110/zeroverse.bit",
				"added": 1582490978
			}
		]
	},
	"comment_vote": {}
}