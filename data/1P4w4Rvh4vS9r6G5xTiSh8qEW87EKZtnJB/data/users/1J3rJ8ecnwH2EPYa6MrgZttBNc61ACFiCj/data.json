{
    "issue_comments": [
        {
            "issue_id": 0,
            "issue_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "Thanks for reporting fixed in Rev3158 (cloning also copies the site's size limit)",
            "date_added": 1512400124976,
            "id": 0
        },
        {
            "issue_id": 0,
            "issue_json": "data/users/1AWwhg4EiWAVttfQboJZ4wJfX3WawfJT3h",
            "body": "This is how publishing works right now:\n -  Picks 5 peers (already connected are preferred)\n - Try to push the new content.json for these clients\n -  After the peers received a new update, they will try to download the changed files from clients they already connected (these can be different from the actual publisher)\n\nSo the changed file's won't be pushed to client so it's not that trivial to create a counter like that.\n\nWhat could be possible: On publish save the changed file list and then watch for file requests how many of those files have been sent.",
            "date_added": 1512657245279,
            "id": 1
        },
        {
            "issue_id": 1,
            "issue_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "Thanks for reporting, fixed in Rev3167",
            "date_added": 1513122887862,
            "id": 2
        },
        {
            "issue_id": 0,
            "issue_json": "data/users/1NoM7QuFsNPWJBqVCezV9oxQP7NofoWXir",
            "body": "Thanks for reporting, fixed in Rev3671",
            "date_added": 1540035411236,
            "id": 3
        },
        {
            "issue_id": 0,
            "issue_json": "data/users/1PpvHgysHyZk6AEaG38SzH2FXgcr74vJt5",
            "body": "Yes, thanks for reporting, it's a bug. I will fix it in next rev, but as a workaround setting \"schema_changed\" to 10 to json table should work.",
            "date_added": 1541786773451,
            "id": 4
        },
        {
            "issue_id": 1,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "It needs more details: Using the provided idea it could be possible to change a value of a file to a specified text, but then how you going to display it?",
            "date_added": 1543008847752,
            "id": 5
        },
        {
            "issue_id": 0,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "It's a known error. There is a need for new distribution method for Linux that is planned with the change to Python3",
            "date_added": 1543195949796,
            "id": 6
        },
        {
            "issue_id": 4,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "Hm, I also tested it using the binary in the ZeroBundle and it does matches for me, so I think the problem is in different place",
            "date_added": 1548076200636,
            "id": 7
        },
        {
            "issue_id": 4,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "I have tried to reproduce, but it worked for me:\n - Created 2 sites: Merger, Merged\n - Added dbschema.json to Merger with \"maps\": {\".*content.json\": { \"to_json_table\": [ \"title\"] }}\n - Executed dbrebuild from sidebar\n - Both Merger and Merged site root content.json Title got added to the database",
            "date_added": 1548376257510,
            "id": 8
        },
        {
            "issue_id": 5,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "The download notification has to done on site level, but we could create a new list (same as blocked sites/users) to filters.json and the site could use this list to check if the content is blocked.",
            "date_added": 1549924559415,
            "id": 9
        },
        {
            "issue_id": 5,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "Yeah, and probably fileblockAdd, fileblockRemove, fileblockList command that lists all blocked files on the site with the reason and the source. Probably it could also contain post/comment ids, not just files.",
            "date_added": 1550019129304,
            "id": 10
        },
        {
            "issue_id": 6,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "Then where would be the dbschema.json specified?\nAlso it would take away the possibility to have different data structure for different merger sites. Eg. right now you can add voting feature to a custom zerome merger site, if we fix the database, then it would not be possible.\n",
            "date_added": 1550263053051,
            "id": 11
        },
        {
            "issue_id": 8,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "I would prefer the setAuthIndex -like solution as it would make it possible to recover it from the master seed",
            "date_added": 1552159917955,
            "id": 12
        },
        {
            "issue_id": 10,
            "issue_json": "data/users/1DfrA2M9Qra6stqT6tzwNMYNweswY54KAC",
            "body": "Yes, file_done should only get fired after it's got inserted to the database. I also found similar bug some days ago (not sure if it's related), but I wasn't able to reproduce it since then. I will try to do a test site with bigger data file.",
            "date_added": 1567354978669,
            "id": 13
        }
    ],
    "next_issue_comment_id": 14,
    "pull_request_comments": [
        {
            "pull_request_id": 0,
            "pull_request_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "I recommend to use ajax instead of fileGet if you want to read big files:\n - It support streaming (don't have to read who file to memory)\n - Does not blocks websocket\n - Support ranged requests (Since Rev3136)\n - Big file support\n - Don't have to base64 encode/decode the file",
            "date_added": 1512662245586,
            "id": 0
        },
        {
            "pull_request_id": 0,
            "pull_request_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "I will publish some examples soon. (currently working on it)",
            "date_added": 1512663493747,
            "id": 1
        },
        {
            "pull_request_id": 0,
            "pull_request_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "Here it is:\nhttps://zeronet.readthedocs.io/en/latest/site_development/zeroframe_api_reference/#wrappergetajaxkey\n\nAlso changed the ZeroFrame monkey patch to use this way:\nhttps://github.com/HelloZeroNet/ZeroHello/blob/adb792fa5a2463059ca2fcd6f21988bcd2a1d1e3/template-new/js/ZeroFrame.js#L109-L118",
            "date_added": 1512669178509,
            "id": 2
        },
        {
            "pull_request_id": 0,
            "pull_request_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "Thanks for reporting, fixed the monkey patching code: https://github.com/HelloZeroNet/ZeroHello/blob/61749945c002e23d9eca292e88232a9342d5cb88/template-new/js/ZeroFrame.js#L109-L118\n\nInstead of separate command to write a partial file I would add a seek argument to the fileWrite API command.It should work on bigger files (<100MB), but not on huge ones.",
            "date_added": 1512868452679,
            "id": 3
        },
        {
            "pull_request_id": 0,
            "pull_request_json": "data/users/1Cy3ntkN2GN9MH6EaW6eHpi4YoRS2nK5Di",
            "body": "It was a typo, fixed in latest version",
            "date_added": 1514292829767,
            "id": 4
        }
    ],
    "next_pull_request_comment_id": 5,
    "issue_actions": [
        {
            "issue_id": 0,
            "issue_json": "data/users/1NoM7QuFsNPWJBqVCezV9oxQP7NofoWXir",
            "action": "changeStatus",
            "param": "close",
            "date_added": 1540035411418,
            "id": 0
        }
    ],
    "next_issue_action_id": 1
}