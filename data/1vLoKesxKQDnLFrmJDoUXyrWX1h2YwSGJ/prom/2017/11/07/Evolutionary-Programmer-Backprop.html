<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Evolutionary Programmer: Backpropogation - LiberIT Blog</title>
  <link rel="stylesheet" href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/css/main.css">
  <!--   Syntax highlighting is only needed for posts -->
  
  <link rel="stylesheet" href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/css/syntax-highlighter.css">
  
  <link href="//fonts.googleapis.com/css?family=Space+Mono|Work+Sans:700" rel="stylesheet">

</head>
<body>

<input type="checkbox" id="hamburger" name="hamburger">
<label id="label-ham" for="hamburger"></label>
<div class="menu-container">
  <div class="menu"></div>
  <div class="links">
      <a href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/"><i class="fa fa-home"></i>&nbsp;Home</a>
      <a href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/about/"><i class="fa fa-user"></i>&nbsp;About</a>
      <a href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/hme_c/"><i class="fa fa-user"></i>&nbsp;Magic
        Spells</a>
      <a href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/archive/"><i class="fa fa-file-zip-o"></i>&nbsp;Archive</a>
      <a href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/contact/"><i class="fa fa-address-book-o"></i>&nbsp;Contact</a>
      <a href="https://github.com/sharu725/krishna"><i class="fa fa-code-fork"></i>&nbsp;Project</a>
  </div>
</div>

<div class="content">
<header>
     <a href="/1vLoKesxKQDnLFrmJDoUXyrWX1h2YwSGJ/"><h1 class="site-title">LiberIT Blog</h1></a>
</header>
  <div class="post-title">
    
    <div class="title">
        <h2>Evolutionary Programmer: Backpropogation</h2>
        
        <span>
            <time datetime="">
                   November 07, 2017
            </time>
        </span>
        
    </div>
</div>


<h3 id="intro">Intro</h3>
<p>Backpropogation is a very common thing in neural networks, and has lead to great
success in that realm.</p>

<p>However the line between a neural-network and
a traditional computer program is a rather blurry one.</p>

<h3 id="neural-networks">Neural Networks</h3>
<p>Neural networks consit of neurons, or independent-clauses, which hold a value and
an operator, just like normal programs.  They then pass the return to the next
“layer” or possibly pass it back to a previous layer if they are recurrent. The
result of a node can be used by multiple other independent-clauses.</p>

<h3 id="backpropogation">Backpropogation</h3>
<p>Backpropogation works by finding the derivative of an independent clause, and
either increasing or decreasing the value of the independent clause, by
multiplying it by either a positive or negative step.</p>

<p>Backpropogation has been used to a limited extent in Genetic Programming as
demonstrated in <a href="http://ieeexplore.ieee.org/abstract/document/6808504/">Semantic Backpropagation for Designing Search Operators in Genetic Programming</a>. Though
there it is limited to sub-components of programs, rather than the more usual
backpropogation in neural networks.</p>

<h3 id="automatic-differentiation">Automatic Differentiation</h3>
<p>With <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic 
Differentiation</a> it is possible to do
backpropogation on any program. While traditional irreversible programs
would require a reverse-mode automatic differentiation,
 which is absurdly complicated to implement. Reversible programs don’t have that
problem, since you can simply invert them, then do the rather simple
forward-mode differentiation, and then invert it back.</p>

<p>Though there may be a flaw to this thinking, in that reversible programs
typically have as many outputs as they have inputs.  However some of the outputs
may simply be clones of the inputs (if the inputs are constants), in which case
there really would be fewer outputs, and it would make sense to invert the
program.</p>

<h3 id="implementation">Implementation</h3>

<p>In order to implement it, would need.</p>

<ol>
  <li>A language that only support time-reversible operationse, like
<a href="https://www.sciencedirect.com/science/article/pii/S1571066110000204">Janus</a>. This is
pretty much done, as Pyash supports very few functions so far, and all the
ones it supports are reversible. So can say done.</li>
  <li>An invert function to invert programs.</li>
  <li>An automatic differentiation (autodiff) implementation for the language.</li>
  <li>A mutation operator that uses autodiff backprop to improve a program.</li>
</ol>

<p>Once that is done, will have one of the fanciest tools in the AI toolkit, in our
Evolutionary Programmer.</p>

<p>Though first I’ll be adding OpenCL support, so all of this evolutionary and
automatic differentiation stuff can run in parallel on GPU.</p>

<h3 id="neural-network-programs">Neural Network Programs</h3>

<p>Something interesting to think about, is that it may be possible to evolve
Neuro-Network size programs, with possibly hundreds of nodes.  Rather than the
few dozen that most GP algorithms are limited to nowadays.</p>

<p>In that regard can look at <a href="http://blog.otoro.net/2016/05/07/backprop-neat/">Backprop NEAT</a> for inspiration.</p>

  
<!--    <footer>
      Made with <span style="color: palevioletred">❤</span> by <a href="http://webjeda.com">webjeda</a>
</footer>-->
</div>
  <!-- Calling Fontawesome at the end to avoid delay in loading --> 
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<!-- Google Analytics Tracking code -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
