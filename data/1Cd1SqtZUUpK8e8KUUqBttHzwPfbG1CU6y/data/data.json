{
	"title": "IFS Blog",
	"description": "\n[Intergalactic File Server](/12MVkvYGcRW6u2NYbpfwVad1oQeyG4s9Er)",
	"links": "",
	"next_post_id": 4,
	"demo": false,
	"modified": 1495228350,
	"post": [
		{
			"post_id": 3,
			"title": "How to make LOCAL copying=\"UPLOAD\" faster?",
			"date_published": 1493313237.468,
			"body": "\"Uploading\" files in Zeronet is the equivalent of moving a file to the appropriate folder.\n\nOddly, this procedure of copying a file to the zeronet data folder is taking extremely long.\n\nIn theory, \"uploading\" a file should take roughly the same time as to \"Transfer a file locally from an internal storage place to the Zeronet Data-Cache folder\", which should be very fast, as it doesnt need to be \"uploaded\" anywhere outside of a users datafolder.\n\nAfter it has been added it is only when someone requests that file, that it actually gets \"uploaded\".\n\nTherefore anyone has an idea how to make \"copying a file to ones data folder\" to make it available on zeronet faster as its currently done in IFS?"
		},
		{
			"post_id": 2,
			"title": "How to create a long-term persistent Zeronet?",
			"date_published": 1488385421.939,
			"body": "One of the areas zeronet could be improved is that each item is currently unique by default.\nThat means (hash-)identical items cannot be identified and commonly shared, even among merger-sites.\n\nThat leads to a fragile state of optional files in case not everyone shares all of the items of a big site default.\n\nAn example that illustrates the above would be a shared picture (or other informational item), which the original author might decide to deleted and therefore disappears the very moment that decision to delete the item spreads via \"sign&publish\".\nDespite other seeders/peers may still have that item cached, there is no chance to preserve that exact item _as the unique item hash it was shared_ (and referenced) by others within zeronet.\n\nSo even if someone got a copy of that original item archived out of zeronet and is re-adding it again, it then has to be re-discovered by any other peers due to its non-known new hash, starting its peer count with 0 again even though its identical to the item before.\n\nFurthering this problem is the non-retrievable context that might have been built around the original item like the description, comments or votings, which after deletion would all be meaningless since the missing reference context file is gone.\n\nFor all the reasons above, an option to reference specifically marked files by its unique hash value across any zeronet sites would be tremendously useful long-term.\n\nAnyone having ideas how to tackle this within zeronet (I know IPFS tackles this very problem, but is yet another technology), please add your thoughts to the comments."
		}
	]
}